{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f77be5",
   "metadata": {},
   "source": [
    "# Language models with Keras\n",
    "\n",
    "Keras does not afford language models out of the box.\n",
    "Instead, we will build them ourselves using neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359101c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['un ordianteur peut vous aider',\n",
       " 'il veut vous aider',\n",
       " 'il veut un ordinateur',\n",
       " 'il peut nager']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppose we have already segmented sentences\n",
    "sentences = [\n",
    "    \"un ordianteur peut vous aider\",\n",
    "    \"il veut vous aider\",\n",
    "    \"il veut un ordinateur\",\n",
    "    \"il peut nager\"\n",
    "]\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd02ac",
   "metadata": {},
   "source": [
    "## I. Simple FeedForward 3-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ceea97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> <s> un ordianteur peut vous aider </s> </s>',\n",
       " '<s> <s> il veut vous aider </s> </s>',\n",
       " '<s> <s> il veut un ordinateur </s> </s>',\n",
       " '<s> <s> il peut nager </s> </s>']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 3\n",
    "\n",
    "# Padding the sentences\n",
    "# we can use NLTK, but I will showcase how to do it just using Keras\n",
    "# in case you didn't install NLTK\n",
    "sentencesPad = []\n",
    "for sentence in sentences:\n",
    "    new_sentence = sentence\n",
    "    for i in range(N-1):\n",
    "        new_sentence = \"<s> \" + new_sentence + \" </s>\"\n",
    "    sentencesPad.append(new_sentence)\n",
    "\n",
    "sentencesPad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7003c5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2, 4, 9, 5, 6, 7, 2, 2],\n",
       " [2, 2, 3, 8, 6, 7, 2, 2],\n",
       " [2, 2, 3, 8, 4, 10, 2, 2],\n",
       " [2, 2, 3, 5, 11, 2, 2]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# We can use NLTK vocab to simplify this\n",
    "tokenizer = Tokenizer(oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(sentencesPad)\n",
    "# 10 tokens + UNK\n",
    "total_words = len(tokenizer.word_index)\n",
    "\n",
    "sentencesOrdinal = []\n",
    "for line in sentencesPad:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    sentencesOrdinal.append(token_list)\n",
    "\n",
    "sentencesOrdinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af680542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 1],\n",
       "  [1, 2],\n",
       "  [2, 3],\n",
       "  [3, 4],\n",
       "  [4, 5],\n",
       "  [5, 6],\n",
       "  [0, 1],\n",
       "  [1, 2],\n",
       "  [2, 3],\n",
       "  [3, 4],\n",
       "  [4, 5],\n",
       "  [0, 1],\n",
       "  [1, 2],\n",
       "  [2, 3],\n",
       "  [3, 4],\n",
       "  [4, 5],\n",
       "  [0, 1],\n",
       "  [1, 2],\n",
       "  [2, 3],\n",
       "  [3, 4]],\n",
       " [2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 2, 3, 4, 5, 6, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create ngrams \n",
    "# we can use NLTK, but I will showcase how to do it just using Keras\n",
    "# in case you didn't install NLTK\n",
    "\n",
    "# the inputs are two consecutive words\n",
    "# So, we will have a two dimentinal array\n",
    "X = []\n",
    "# the outputs are the third word\n",
    "# So, we will have a one dimentional array\n",
    "Y = []\n",
    "\n",
    "for sentence in sentencesOrdinal:\n",
    "    slen = len(sentence)\n",
    "    for i in range(slen-N):\n",
    "        Xi = []\n",
    "        for j in range(N-1):\n",
    "            Xi.append(i+j)\n",
    "        X.append(Xi)\n",
    "        Y.append(i+N-1)\n",
    "\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9537800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "Ybin = to_categorical(Y, num_classes=total_words)\n",
    "Xbin = np.array(to_categorical(X, num_classes=total_words))\n",
    "input_len = Xbin.shape[1] * Xbin.shape[2]\n",
    "Xbin = Xbin.reshape(Xbin.shape[0], input_len)\n",
    "\n",
    "Xbin[0,:], Ybin[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f90bf",
   "metadata": {},
   "source": [
    "### I.1. Without embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1531f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                230       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                121       \n",
      "=================================================================\n",
      "Total params: 351\n",
      "Trainable params: 351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "ff_model = Sequential()\n",
    "ff_model.add(Dense(10, input_dim=input_len, activation='relu'))\n",
    "ff_model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "ff_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8a5d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 2.4283\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.4233\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4182\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4132\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4082\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4032\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3982\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3933\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3883\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3834\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3785\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3736\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3687\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3638\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3590\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3541\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3493\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3447\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3400\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.3354\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3308\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3264\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3221\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3178\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3134\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3090\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3046\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2954\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2907\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.2861\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2814\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.2766\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2719\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2673\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2626\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2579\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2532\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2486\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2439\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2392\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.2345\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2298\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2250\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2202\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2155\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2108\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2060\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2012\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1965\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1917\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1870\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1823\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1776\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1728\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1681\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1634\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1587\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1540\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1493\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1447\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1400\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1354\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1307\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1260\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1214\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1167\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1120\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.1074\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1027\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0980\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0934\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0887\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0840\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.0793\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0746\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0700\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0653\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0606\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0559\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0513\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0466\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0419\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.0372\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0325\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0278\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.0231\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.0184\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0137\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0090\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0042\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9995\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9948\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.9901\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9854\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9806\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9759\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9711\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.9664\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb43d717700>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "ff_model.fit(Xbin, Ybin, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39743c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'un'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function which takes N-1 words separated by space\n",
    "# and returns the Nth most probable word\n",
    "def estimate(words):\n",
    "    Xp = tokenizer.texts_to_sequences([words])[0]\n",
    "    Xp = to_categorical(Xp, num_classes=total_words)\n",
    "    Xp = np.array([(np.array(Xp)).flatten()])\n",
    "    prob = ff_model.predict(Xp)\n",
    "    i = prob.argmax()\n",
    "    return tokenizer.sequences_to_texts([[i]])[0]\n",
    "    \n",
    "estimate('peut aider')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3a936",
   "metadata": {},
   "source": [
    "### I.2. With embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "219340c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2, 10)             110       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                231       \n",
      "=================================================================\n",
      "Total params: 341\n",
      "Trainable params: 341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "ff_model_emb = Sequential()\n",
    "ff_model_emb.add(Embedding(total_words, 10, input_length=N-1))\n",
    "ff_model_emb.add(Flatten())\n",
    "ff_model_emb.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "ff_model_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66abc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 2.4116\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4072\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.4029\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3985\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3942\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3899\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3856\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3813\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3770\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3727\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.3684\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3641\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3597\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3554\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3511\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3468\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3424\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3381\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3337\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3294\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3250\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3206\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3162\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3117\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3073\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3028\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2983\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.2937\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2891\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2845\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.2799\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2752\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2705\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2658\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2610\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.2561\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2513\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2463\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2414\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.2364\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2313\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.2262\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2211\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2158\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2106\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2053\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1999\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1945\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1890\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1835\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1779\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1722\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1665\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1607\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1549\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1490\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1430\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1370\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1309\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1247\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1185\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1122\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1059\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0994\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0930\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0864\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0798\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0731\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.0663\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0595\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0526\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0457\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0386\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0315\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0244\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0171\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0098\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0024\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9950\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9875\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9799\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9722\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9645\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.9567\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9488\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9409\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9329\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.9248\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9167\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.9085\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9002\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8919\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8835\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8750\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8664\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8578\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8492\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8404\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8316\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb43c02a820>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_model_emb.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#Here we use X instead of Xbin\n",
    "ff_model_emb.fit(np.array(X), Ybin, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f62e3c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'un'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function which takes N-1 words separated by space\n",
    "# and returns the Nth most probable word\n",
    "def estimate(words):\n",
    "    Xp = tokenizer.texts_to_sequences([words])[0]\n",
    "    Xp = np.array([Xp])\n",
    "    prob = ff_model_emb.predict(Xp)\n",
    "    i = prob.argmax()\n",
    "    return tokenizer.sequences_to_texts([[i]])[0]\n",
    "    \n",
    "estimate('peut aider')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b3a98",
   "metadata": {},
   "source": [
    "## II. LSTM model\n",
    "\n",
    "https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/\n",
    "\n",
    "In recurrent models, we add just one padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6ec9da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> un ordianteur peut vous aider </s>',\n",
       " '<s> il veut vous aider </s>',\n",
       " '<s> il veut un ordinateur </s>',\n",
       " '<s> il peut nager </s>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding the sentences\n",
    "# we can use NLTK, but I will showcase how to do it just using Keras\n",
    "# in case you didn't install NLTK\n",
    "sentencesPad2 = []\n",
    "for sentence in sentences:\n",
    "    new_sentence = \"<s> \" + sentence + \" </s>\"\n",
    "    sentencesPad2.append(new_sentence)\n",
    "\n",
    "sentencesPad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21cf71f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4],\n",
       " [2, 4, 9],\n",
       " [2, 4, 9, 5],\n",
       " [2, 4, 9, 5, 6],\n",
       " [2, 4, 9, 5, 6, 7],\n",
       " [2, 4, 9, 5, 6, 7, 2],\n",
       " [2, 3],\n",
       " [2, 3, 8],\n",
       " [2, 3, 8, 6],\n",
       " [2, 3, 8, 6, 7],\n",
       " [2, 3, 8, 6, 7, 2],\n",
       " [2, 3],\n",
       " [2, 3, 8],\n",
       " [2, 3, 8, 4],\n",
       " [2, 3, 8, 4, 10],\n",
       " [2, 3, 8, 4, 10, 2],\n",
       " [2, 3],\n",
       " [2, 3, 5],\n",
       " [2, 3, 5, 11],\n",
       " [2, 3, 5, 11, 2]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# We can use NLTK vocab to simplify this\n",
    "tokenizer2 = Tokenizer(oov_token='<UNK>')\n",
    "tokenizer2.fit_on_texts(sentencesPad2)\n",
    "# 10 tokens + UNK + padding code\n",
    "total_words2 = len(tokenizer2.word_index) + 1\n",
    "\n",
    "# each sentence is encoded as lists of 2 to its length\n",
    "sequences = []\n",
    "for line in sentencesPad2:\n",
    "    encoded = tokenizer2.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a729936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  2,  4],\n",
       "       [ 0,  0,  0,  0,  2,  4,  9],\n",
       "       [ 0,  0,  0,  2,  4,  9,  5],\n",
       "       [ 0,  0,  2,  4,  9,  5,  6],\n",
       "       [ 0,  2,  4,  9,  5,  6,  7],\n",
       "       [ 2,  4,  9,  5,  6,  7,  2],\n",
       "       [ 0,  0,  0,  0,  0,  2,  3],\n",
       "       [ 0,  0,  0,  0,  2,  3,  8],\n",
       "       [ 0,  0,  0,  2,  3,  8,  6],\n",
       "       [ 0,  0,  2,  3,  8,  6,  7],\n",
       "       [ 0,  2,  3,  8,  6,  7,  2],\n",
       "       [ 0,  0,  0,  0,  0,  2,  3],\n",
       "       [ 0,  0,  0,  0,  2,  3,  8],\n",
       "       [ 0,  0,  0,  2,  3,  8,  4],\n",
       "       [ 0,  0,  2,  3,  8,  4, 10],\n",
       "       [ 0,  2,  3,  8,  4, 10,  2],\n",
       "       [ 0,  0,  0,  0,  0,  2,  3],\n",
       "       [ 0,  0,  0,  0,  2,  3,  5],\n",
       "       [ 0,  0,  0,  2,  3,  5, 11],\n",
       "       [ 0,  0,  2,  3,  5, 11,  2]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#get the maximum length of all sentences in term of words number\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "#add \n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c2333ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 2], dtype=int32),\n",
       " array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into input and output elements\n",
    "# The last word is the destination \n",
    "# The other words are the context\n",
    "X2, Y2 = sequences[:,:-1], sequences[:,-1]\n",
    "#We will encode only the output to one-hot encoding\n",
    "#The input will not be encoded since we use an embedding layer \n",
    "#which will hadle the encoding part\n",
    "Y2 = to_categorical(Y2, num_classes=total_words2)\n",
    "\n",
    "X2[0,:], Y2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d045ba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 6, 10)             120       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                612       \n",
      "=================================================================\n",
      "Total params: 12,932\n",
      "Trainable params: 12,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "# define model\n",
    "model_lstm_emb = Sequential()\n",
    "model_lstm_emb.add(Embedding(total_words2, 10, input_length=max_length-1))\n",
    "model_lstm_emb.add(LSTM(50))\n",
    "model_lstm_emb.add(Dense(total_words2, activation='softmax'))\n",
    "\n",
    "model_lstm_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "248d5377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 2s - loss: 2.4837 - accuracy: 0.1500\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 2.4813 - accuracy: 0.2000\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 2.4790 - accuracy: 0.3000\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 2.4767 - accuracy: 0.3000\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 2.4743 - accuracy: 0.2500\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 2.4719 - accuracy: 0.4000\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 2.4694 - accuracy: 0.4000\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 2.4668 - accuracy: 0.4000\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 2.4641 - accuracy: 0.4000\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 2.4613 - accuracy: 0.4000\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 2.4583 - accuracy: 0.3500\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 2.4552 - accuracy: 0.3500\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 2.4520 - accuracy: 0.3500\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 2.4485 - accuracy: 0.3500\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 2.4449 - accuracy: 0.3500\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 2.4410 - accuracy: 0.3500\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 2.4370 - accuracy: 0.3500\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 2.4326 - accuracy: 0.3500\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 2.4280 - accuracy: 0.3500\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 2.4230 - accuracy: 0.3500\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 2.4177 - accuracy: 0.3500\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 2.4121 - accuracy: 0.3500\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 2.4061 - accuracy: 0.2000\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 2.3996 - accuracy: 0.2000\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 2.3927 - accuracy: 0.2000\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 2.3853 - accuracy: 0.2000\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 2.3775 - accuracy: 0.2000\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 2.3690 - accuracy: 0.2000\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 2.3600 - accuracy: 0.2000\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 2.3504 - accuracy: 0.2000\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 2.3402 - accuracy: 0.2000\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 2.3293 - accuracy: 0.2000\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 2.3179 - accuracy: 0.2000\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 2.3058 - accuracy: 0.2000\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 2.2932 - accuracy: 0.2000\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 2.2802 - accuracy: 0.2000\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 2.2668 - accuracy: 0.2000\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 2.2532 - accuracy: 0.2000\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 2.2397 - accuracy: 0.2000\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 2.2265 - accuracy: 0.2000\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 2.2138 - accuracy: 0.2000\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 2.2019 - accuracy: 0.2000\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 2.1909 - accuracy: 0.2000\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 2.1810 - accuracy: 0.2000\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 2.1718 - accuracy: 0.2000\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 2.1631 - accuracy: 0.2000\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 2.1542 - accuracy: 0.2000\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 2.1448 - accuracy: 0.2000\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 2.1345 - accuracy: 0.2000\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 2.1232 - accuracy: 0.3500\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 2.1109 - accuracy: 0.3500\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 2.0979 - accuracy: 0.3500\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 2.0844 - accuracy: 0.3500\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 2.0705 - accuracy: 0.3500\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 2.0563 - accuracy: 0.3500\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 2.0417 - accuracy: 0.3500\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 2.0267 - accuracy: 0.3500\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 2.0110 - accuracy: 0.3500\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 1.9947 - accuracy: 0.3500\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 1.9775 - accuracy: 0.3500\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 1.9596 - accuracy: 0.3500\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 1.9409 - accuracy: 0.3500\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 1.9218 - accuracy: 0.3500\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 1.9023 - accuracy: 0.3500\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 1.8826 - accuracy: 0.3500\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 1.8628 - accuracy: 0.3500\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 1.8431 - accuracy: 0.3500\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 1.8234 - accuracy: 0.3500\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 1.8037 - accuracy: 0.3500\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 1.7842 - accuracy: 0.3500\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 1.7649 - accuracy: 0.3500\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 1.7459 - accuracy: 0.3500\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 1.7275 - accuracy: 0.3500\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 1.7097 - accuracy: 0.3500\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 1.6923 - accuracy: 0.3500\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 1.6753 - accuracy: 0.3500\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 1.6586 - accuracy: 0.3500\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 1.6419 - accuracy: 0.3500\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 1.6254 - accuracy: 0.3500\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 1.6091 - accuracy: 0.3500\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 1.5931 - accuracy: 0.3500\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 1.5775 - accuracy: 0.3500\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 1.5623 - accuracy: 0.4000\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 1.5473 - accuracy: 0.4000\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 1.5324 - accuracy: 0.4500\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 1.5177 - accuracy: 0.4500\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 1.5032 - accuracy: 0.4500\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 1.4890 - accuracy: 0.4500\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 1.4750 - accuracy: 0.4500\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 1.4611 - accuracy: 0.4500\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 1.4471 - accuracy: 0.5000\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 1.4332 - accuracy: 0.5000\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 1.4193 - accuracy: 0.5000\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 1.4055 - accuracy: 0.5000\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 1.3916 - accuracy: 0.5000\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 1.3776 - accuracy: 0.5000\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 1.3635 - accuracy: 0.5000\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 1.3494 - accuracy: 0.5500\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 1.3353 - accuracy: 0.5500\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 1.3210 - accuracy: 0.5500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb4355d5400>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model_lstm_emb.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_lstm_emb.fit(X2, Y2, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7f7e03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 6) for input KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None, 7).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'vous'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function which takes many words separated by space\n",
    "# and returns the next most probable word\n",
    "def estimate_lstm(words):\n",
    "    Xp = tokenizer2.texts_to_sequences([words])[0]\n",
    "    Xp = pad_sequences([Xp], maxlen=max_length, padding='pre')\n",
    "    prob = model_lstm_emb.predict(Xp)\n",
    "    i = prob.argmax()\n",
    "    return tokenizer.sequences_to_texts([[i]])[0]\n",
    "    \n",
    "estimate_lstm('<s> il peut aider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615202e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
