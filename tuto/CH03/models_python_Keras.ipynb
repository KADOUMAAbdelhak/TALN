{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f77be5",
   "metadata": {},
   "source": [
    "# Language models with Keras\n",
    "\n",
    "Keras does not afford language models out of the box.\n",
    "Instead, we will build them ourselves using neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359101c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['un ordianteur peut vous aider',\n",
       " 'il veut vous aider',\n",
       " 'il veut un ordinateur',\n",
       " 'il peut nager']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppose we have already segmented sentences\n",
    "sentences = [\n",
    "    \"un ordianteur peut vous aider\",\n",
    "    \"il veut vous aider\",\n",
    "    \"il veut un ordinateur\",\n",
    "    \"il peut nager\"\n",
    "]\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd02ac",
   "metadata": {},
   "source": [
    "## I. Simple FeedForward 3-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8ceea97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> <s> un ordianteur peut vous aider </s> </s>',\n",
       " '<s> <s> il veut vous aider </s> </s>',\n",
       " '<s> <s> il veut un ordinateur </s> </s>',\n",
       " '<s> <s> il peut nager </s> </s>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 3\n",
    "\n",
    "# Padding the sentences\n",
    "# we can use NLTK, but I will showcase how to do it just using Keras\n",
    "# in case you didn't install NLTK\n",
    "sentencesPad = []\n",
    "for sentence in sentences:\n",
    "    new_sentence = sentence\n",
    "    for i in range(N-1):\n",
    "        new_sentence = \"<s> \" + new_sentence + \" </s>\"\n",
    "    sentencesPad.append(new_sentence)\n",
    "\n",
    "sentencesPad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7003c5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2, 4, 9, 5, 6, 7, 2, 2],\n",
       " [2, 2, 3, 8, 6, 7, 2, 2],\n",
       " [2, 2, 3, 8, 4, 10, 2, 2],\n",
       " [2, 2, 3, 5, 11, 2, 2]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# We can use NLTK vocab to simplify this\n",
    "tokenizer = Tokenizer(oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(sentencesPad)\n",
    "# 10 tokens + UNK\n",
    "total_words = len(tokenizer.word_index)\n",
    "\n",
    "sentencesOrdinal = []\n",
    "for line in sentencesPad:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    sentencesOrdinal.append(token_list)\n",
    "\n",
    "sentencesOrdinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af680542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 1],\n",
       "  [1, 2],\n",
       "  [2, 3],\n",
       "  [3, 4],\n",
       "  [4, 5],\n",
       "  [5, 6],\n",
       "  [0, 1],\n",
       "  [1, 2],\n",
       "  [2, 3],\n",
       "  [3, 4],\n",
       "  [4, 5],\n",
       "  [0, 1],\n",
       "  [1, 2],\n",
       "  [2, 3],\n",
       "  [3, 4],\n",
       "  [4, 5],\n",
       "  [0, 1],\n",
       "  [1, 2],\n",
       "  [2, 3],\n",
       "  [3, 4]],\n",
       " [2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 6, 2, 3, 4, 5, 6, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create ngrams \n",
    "# we can use NLTK, but I will showcase how to do it just using Keras\n",
    "# in case you didn't install NLTK\n",
    "\n",
    "# the inputs are two consecutive words\n",
    "# So, we will have a two dimentinal array\n",
    "X = []\n",
    "# the outputs are the third word\n",
    "# So, we will have a one dimentional array\n",
    "Y = []\n",
    "\n",
    "for sentence in sentencesOrdinal:\n",
    "    slen = len(sentence)\n",
    "    for i in range(slen-N):\n",
    "        Xi = []\n",
    "        for j in range(N-1):\n",
    "            Xi.append(i+j)\n",
    "        X.append(Xi)\n",
    "        Y.append(i+N-1)\n",
    "\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9537800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "Ybin = to_categorical(Y, num_classes=total_words)\n",
    "Xbin = np.array(to_categorical(X, num_classes=total_words))\n",
    "input_len = Xbin.shape[1] * Xbin.shape[2]\n",
    "Xbin = Xbin.reshape(Xbin.shape[0], input_len)\n",
    "\n",
    "Xbin[0,:], Ybin[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f90bf",
   "metadata": {},
   "source": [
    "### I.1. Without embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d1531f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 2.5003\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4934\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4866\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4798\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4730\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4662\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4595\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4527\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4460\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4393\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4326\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4259\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4192\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4125\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4059\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3992\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3926\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3860\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3794\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3728\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3662\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3596\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3531\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3465\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3400\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3335\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3269\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3204\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3139\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3074\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3010\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2945\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2882\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2819\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2756\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2693\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2630\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2567\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2504\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2441\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2378\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2315\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2252\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2189\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2126\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2063\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1937\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1874\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1810\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1746\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.1683\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1619\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1555\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1491\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1427\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1363\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1297\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1230\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1163\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1096\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1028\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0961\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.0893\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0826\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0758\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0690\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0622\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0554\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0486\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0418\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0350\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0282\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0214\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0146\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0078\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0009\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9941\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9873\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9804\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9736\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9667\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9599\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9530\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9461\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9392\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9324\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9255\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9186\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9117\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9047\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.897 - 0s 11ms/step - loss: 1.8978\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8909\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8838\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8767\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8695\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8623\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8551\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8479\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1df2439730>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "ff_model = Sequential()\n",
    "ff_model.add(Dense(10, input_dim=input_len, activation='relu'))\n",
    "ff_model.add(Dense(total_words, activation='softmax'))\n",
    "ff_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "ff_model.fit(Xbin, Ybin, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39743c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vous'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function which takes N-1 words separated by space\n",
    "# and returns the Nth most probable word\n",
    "def estimate(words):\n",
    "    Xp = tokenizer.texts_to_sequences([words])[0]\n",
    "    Xp = to_categorical(Xp, num_classes=total_words)\n",
    "    Xp = np.array([(np.array(Xp)).flatten()])\n",
    "    prob = ff_model.predict(Xp)\n",
    "    i = prob.argmax()\n",
    "    return tokenizer.sequences_to_texts([[i]])[0]\n",
    "    \n",
    "estimate('peut aider')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3a936",
   "metadata": {},
   "source": [
    "### I.2. With embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "219340c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 477ms/step - loss: 2.4217\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4173\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4129\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4085\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4041\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3997\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3953\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3909\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3865\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3821\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3777\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3733\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3689\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3645\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3600\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3556\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3511\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3467\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3422\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3377\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3332\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3286\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3241\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3195\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3149\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.3103\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3056\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3009\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2962\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2915\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2867\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2819\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2770\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2721\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2672\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2622\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2572\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2521\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2470\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2419\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2367\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.2315\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2262\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.2208\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2154\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.2100\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2045\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.1989\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1933\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1876\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.1819\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1761\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1703\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1643\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1584\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1523\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1463\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1401\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.1339\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1276\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1213\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1149\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1084\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1018\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.0952\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0886\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0818\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0750\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0682\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0612\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0543\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0472\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0401\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0329\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0256\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0183\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0109\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0034\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9959\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9883\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9807\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9729\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9652\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.9573\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9494\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9414\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9334\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9253\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9171\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9089\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9006\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.8922\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8838\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8753\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8668\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8582\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8496\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8408\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8321\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1df240f4f0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "ff_model_emb = Sequential()\n",
    "ff_model_emb.add(Embedding(total_words, 10, input_length=N-1))\n",
    "ff_model_emb.add(Flatten())\n",
    "ff_model_emb.add(Dense(total_words, activation='softmax'))\n",
    "ff_model_emb.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#Here we use X instead of Xbin\n",
    "ff_model_emb.fit(np.array(X), Ybin, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f62e3c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vous'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function which takes N-1 words separated by space\n",
    "# and returns the Nth most probable word\n",
    "def estimate(words):\n",
    "    Xp = tokenizer.texts_to_sequences([words])[0]\n",
    "    Xp = np.array([Xp])\n",
    "    prob = ff_model_emb.predict(Xp)\n",
    "    i = prob.argmax()\n",
    "    return tokenizer.sequences_to_texts([[i]])[0]\n",
    "    \n",
    "estimate('peut aider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2da11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
