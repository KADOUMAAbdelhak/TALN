{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091f8c92",
   "metadata": {},
   "source": [
    "# Sequences tagging using flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1a2af",
   "metadata": {},
   "source": [
    "## I. Data preparation\n",
    "\n",
    "### I.1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4482da9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token: 1 I,\n",
       " Token: 2 am,\n",
       " Token: 3 going,\n",
       " Token: 4 to,\n",
       " Token: 5 visit,\n",
       " Token: 6 Dr.,\n",
       " Token: 7 Watson,\n",
       " Token: 8 .,\n",
       " Token: 9 He,\n",
       " Token: 10 is,\n",
       " Token: 11 as,\n",
       " Token: 12 U.K,\n",
       " Token: 13 .]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# make a sentence\n",
    "#sentence = Sentence('Karim bought a Lenovo computer with over 70000 DZD when he was in Algiers, Algeria.')\n",
    "\n",
    "sentence = Sentence(\"I am going to visit Dr. Watson. He is as U.K.\")\n",
    "sentence.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e527982c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token: 1 Karim,\n",
       " Token: 2 bought,\n",
       " Token: 3 a,\n",
       " Token: 4 Lenovo,\n",
       " Token: 5 computer]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# introduce a tokenized sentence\n",
    "sentence2 = Sentence(['Karim', 'bought', 'a', 'Lenovo', 'computer'])\n",
    "\n",
    "sentence2.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa1aa6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token: 1 私,\n",
       " Token: 2 は,\n",
       " Token: 3 ESI,\n",
       " Token: 4 の,\n",
       " Token: 5 先生,\n",
       " Token: 6 です,\n",
       " Token: 7 。,\n",
       " Token: 8 毎日,\n",
       " Token: 9 、,\n",
       " Token: 10 そこ,\n",
       " Token: 11 に,\n",
       " Token: 12 行き,\n",
       " Token: 13 ます,\n",
       " Token: 14 。]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use another tokenizer\n",
    "from flair.tokenization import JapaneseTokenizer\n",
    "\n",
    "# init japanese tokenizer\n",
    "ja_tokenizer = JapaneseTokenizer(\"janome\")\n",
    "\n",
    "# make sentence (and tokenize)\n",
    "sentence3 = Sentence('私はESIの先生です。毎日、そこに行きます。', use_tokenizer=ja_tokenizer)\n",
    "\n",
    "sentence3.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fcafc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token: 1 أنا,\n",
       " Token: 2 ذاهب,\n",
       " Token: 3 إلى,\n",
       " Token: 4 السوق,\n",
       " Token: 5 .,\n",
       " Token: 6 هل,\n",
       " Token: 7 تريد,\n",
       " Token: 8 أن,\n",
       " Token: 9 أحضر,\n",
       " Token: 10 لك,\n",
       " Token: 11 شيء,\n",
       " Token: 12 ما,\n",
       " Token: 13 ؟,\n",
       " Token: 14 هكذا,\n",
       " Token: 15 إذن,\n",
       " Token: 16 !,\n",
       " Token: 17 نلتقي,\n",
       " Token: 18 بعد,\n",
       " Token: 19 أن,\n",
       " Token: 20 أعود,\n",
       " Token: 21 .]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Sentence, Token, Tokenizer\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "class ArTokenizer(Tokenizer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ArTokenizer, self).__init__()\n",
    "        self.punct = re.compile(r'^(.*)([،:.,,؟!])$')\n",
    "\n",
    "    def tokenize(self, text: str) -> List[Token]:\n",
    "        words = text.split()\n",
    "        tokens: List[Token] = []\n",
    "        for word in words:\n",
    "            m = self.punct.match(word)\n",
    "            if m:\n",
    "                tokens.append(Token(m.group(1)))\n",
    "                tokens.append(Token(m.group(2)))\n",
    "            else:\n",
    "                tokens.append(Token(word))\n",
    "        return tokens\n",
    "        \n",
    "        \n",
    "\n",
    "# init arabic tokenizer\n",
    "ar_tokenizer = ArTokenizer()\n",
    "\n",
    "ar_sentence = Sentence(\"أنا ذاهب إلى السوق. هل تريد أن أحضر لك شيء ما؟ هكذا إذن! نلتقي بعد أن أعود.\", use_tokenizer=ar_tokenizer)\n",
    "\n",
    "ar_sentence.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5030f22a",
   "metadata": {},
   "source": [
    "### I.2. Corpus preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb63264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 20:03:25,688 Reading data from /home/kariminf/.flair/datasets/ud_english\n",
      "2021-09-29 20:03:25,689 Train: /home/kariminf/.flair/datasets/ud_english/en_ewt-ud-train.conllu\n",
      "2021-09-29 20:03:25,692 Dev: /home/kariminf/.flair/datasets/ud_english/en_ewt-ud-dev.conllu\n",
      "2021-09-29 20:03:25,694 Test: /home/kariminf/.flair/datasets/ud_english/en_ewt-ud-test.conllu\n",
      "______ Access a sentence ______\n",
      "Sentence: \"What if Google Morphed Into GoogleOS ?\"   [− Tokens: 7  − Token-Labels: \"What <what/PRON/WP/root/Int> if <if/SCONJ/IN/mark> Google <Google/PROPN/NNP/nsubj/Sing> Morphed <morph/VERB/VBD/advcl/Ind/Past/Fin> Into <into/ADP/IN/case> GoogleOS <GoogleOS/PROPN/NNP/obl/Sing> ? <?/PUNCT/./punct>\"]\n",
      "______ PoS-tagged sentence ______\n",
      "What <WP> if <IN> Google <NNP> Morphed <VBD> Into <IN> GoogleOS <NNP> ? <.>\n"
     ]
    }
   ],
   "source": [
    "#existing corpora in flair\n",
    "import flair.datasets\n",
    "ud_en_corpus = flair.datasets.UD_ENGLISH()\n",
    "\n",
    "print('______ Access a sentence ______')\n",
    "print(ud_en_corpus.test[0])\n",
    "\n",
    "print('______ PoS-tagged sentence ______')\n",
    "print(ud_en_corpus.test[0].to_tagged_string('pos'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02af0b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 20:31:16,457 Reading data from .\n",
      "2021-09-29 20:31:16,459 Train: flair_train.txt\n",
      "2021-09-29 20:31:16,459 Dev: flair_dev.txt\n",
      "2021-09-29 20:31:16,460 Test: flair_test.txt\n"
     ]
    }
   ],
   "source": [
    "#creating your own corpus\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'pos', 2: 'ner'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '.'\n",
    "\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='flair_train.txt',\n",
    "                              test_file='flair_test.txt',\n",
    "                              dev_file='flair_dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7f03dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'un <DET> ordianteur <NOUN> peut <VERB> vous <PRON> aider <VERB>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train[0].to_tagged_string('pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c2f07",
   "metadata": {},
   "source": [
    "## II. Part of Speech (PoS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a5f90",
   "metadata": {},
   "source": [
    "### II.1. Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8a64551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 15:15:36,712 --------------------------------------------------------------------------------\n",
      "2021-09-29 15:15:36,713 The model key 'pos' now maps to 'https://huggingface.co/flair/pos-english' on the HuggingFace ModelHub\n",
      "2021-09-29 15:15:36,715  - The most current version of the model is automatically downloaded from there.\n",
      "2021-09-29 15:15:36,716  - (you can alternatively manually download the original model at https://nlp.informatik.hu-berlin.de/resources/models/pos/en-pos-ontonotes-v0.5.pt)\n",
      "2021-09-29 15:15:36,717 --------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6136d700c446258a9f7a7d6dc0c976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 15:16:24,644 loading file /home/kariminf/.flair/models/pos-english/a9a73f6cd878edce8a0fa518db76f441f1cc49c2525b2b4557af278ec2f0659e.121306ea62993d04cd1978398b68396931a39eb47754c8a06a87f325ea70ac63\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load the PoS tagger\n",
    "pos_tagger = SequenceTagger.load('pos')\n",
    "\n",
    "# run PoS over sentence\n",
    "pos_tagger.predict(sentence)\n",
    "\n",
    "del pos_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86619443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karim NNP 0.9999685287475586\n",
      "bought VBD 0.9999796152114868\n",
      "a DT 0.9999998807907104\n",
      "Lenovo NNP 0.9999986886978149\n",
      "computer NN 0.9999860525131226\n",
      "with IN 1.0\n",
      "over IN 0.9845556616783142\n",
      "70000 CD 0.9999727010726929\n",
      "DZD NNP 0.7187032103538513\n",
      "when WRB 1.0\n",
      "he PRP 0.9999998807907104\n",
      "was VBD 0.9999997615814209\n",
      "in IN 0.9999995231628418\n",
      "Algiers NNP 0.9992132186889648\n",
      ", , 1.0\n",
      "Algeria NNP 0.9999998807907104\n",
      ". . 0.999991774559021\n"
     ]
    }
   ],
   "source": [
    "for entity in sentence.get_spans('pos'):\n",
    "    print(entity.text, entity.tag, entity.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94a2c1",
   "metadata": {},
   "source": [
    "### II.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b12ad9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 20:36:28,022 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 4891.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 20:36:28,028 Corpus contains the labels: pos (#25), ner (#24)\n",
      "2021-09-29 20:36:28,029 Created (for label 'pos') Dictionary with 4 tags: DET, NOUN, VERB, PRON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<flair.data.Dictionary at 0x7f4afd902bb0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. get the corpus\n",
    "# we will use the corpus we created earlier \n",
    "\n",
    "# 2. what label do we want to predict?\n",
    "pos_label_type = 'pos'\n",
    "\n",
    "# 3. make the label dictionary from the corpus\n",
    "pos_label_dict = corpus.make_label_dictionary(label_type=pos_label_type)\n",
    "\n",
    "pos_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3252041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 20:37:02,667 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpr18kkm75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 9807872/160000128 [00:36<10:45, 232527.37B/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e1a0a05047d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m pos_embedding_types = [\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mWordEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# comment in this line to use character embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/flair/embeddings/token.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embeddings, field)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# GLOVE embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"glove\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"en-glove\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{hu_path}/glove.gensim.vectors.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{hu_path}/glove.gensim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/flair/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"http\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# File, and it exists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/flair/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m                 if (\n\u001b[1;32m    509\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings\n",
    "\n",
    "# 4. initialize embeddings\n",
    "pos_embedding_types = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    # FlairEmbeddings('news-forward'),\n",
    "    # FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "pos_embeddings = StackedEmbeddings(embeddings=pos_embedding_types)\n",
    "\n",
    "pos_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "pos_tagger = SequenceTagger(hidden_size=10,\n",
    "                        embeddings=pos_embeddings,\n",
    "                        tag_dictionary=pos_label_dict,\n",
    "                        tag_type=pos_label_type,\n",
    "                        use_crf=True)\n",
    "\n",
    "pos_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "pos_model_path = '/home/kariminf/Data/tutoriel/flair_pos.tagger.fr'\n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(pos_tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train(pos_model_path,\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=10, \n",
    "              max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f172b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model you trained\n",
    "pos_load_model = SequenceTagger.load(pos_model_path)\n",
    "\n",
    "# create example sentence\n",
    "sentence_fr = Sentence('il peut aider')\n",
    "\n",
    "# predict tags and print\n",
    "pos_load_model.predict(sentence_fr)\n",
    "\n",
    "sentence_fr.to_tagged_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1507e",
   "metadata": {},
   "source": [
    "## III. Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba367b6",
   "metadata": {},
   "source": [
    "### III.1. Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7c8b18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 14:34:11,093 --------------------------------------------------------------------------------\n",
      "2021-09-29 14:34:11,094 The model key 'ner' now maps to 'https://huggingface.co/flair/ner-english' on the HuggingFace ModelHub\n",
      "2021-09-29 14:34:11,096  - The most current version of the model is automatically downloaded from there.\n",
      "2021-09-29 14:34:11,099  - (you can alternatively manually download the original model at https://nlp.informatik.hu-berlin.de/resources/models/ner/en-ner-conll03-v0.4.pt)\n",
      "2021-09-29 14:34:11,100 --------------------------------------------------------------------------------\n",
      "2021-09-29 14:34:11,990 loading file /home/kariminf/.flair/models/ner-english/4f4cdab26f24cb98b732b389e6cebc646c36f54cfd6e0b7d3b90b25656e4262f.8baa8ae8795f4df80b28e7f7b61d788ecbb057d1dc85aacb316f1bd02837a4a4\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "\n",
    "\n",
    "# load the NER tagger\n",
    "ner_tagger = SequenceTagger.load('ner')\n",
    "\n",
    "# run NER over sentence\n",
    "ner_tagger.predict(sentence)\n",
    "\n",
    "del ner_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a9973dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karim PER 0.9950203895568848\n",
      "Lenovo ORG 0.9930753707885742\n",
      "DZD MISC 0.661797046661377\n",
      "Algiers LOC 0.999290943145752\n",
      "Algeria LOC 0.9998273253440857\n"
     ]
    }
   ],
   "source": [
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity.text, entity.tag, entity.score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cfd3d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 15:57:25,556 --------------------------------------------------------------------------------\n",
      "2021-09-29 15:57:25,574 The model key 'ar-ner' now maps to 'https://huggingface.co/megantosh/flair-arabic-multi-ner' on the HuggingFace ModelHub\n",
      "2021-09-29 15:57:25,576  - The most current version of the model is automatically downloaded from there.\n",
      "2021-09-29 15:57:25,579 --------------------------------------------------------------------------------\n",
      "2021-09-29 15:57:26,589 loading file /home/kariminf/.flair/models/flair-arabic-multi-ner/c7af7ddef4fdcc681fcbe1f37719348afd2862b12aa1cfd4f3b93bd2d77282c7.242d030cb106124f7f9f6a88fb9af8e390f581d42eeca013367a86d585ee6dd6\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "# The model is 500MB, it is so heavy\n",
    "ar_ner_tagger = SequenceTagger.load('ar-ner')\n",
    "\n",
    "\n",
    "# predict NER tags\n",
    "ar_ner_tagger.predict(ar_sentence)\n",
    "\n",
    "del ar_ner_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "023c7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sentence with predicted tags\n",
    "for entity in ar_sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671805b",
   "metadata": {},
   "source": [
    "### III.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65b76f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 21:13:47,312 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [00:00<00:00, 4854.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-29 21:13:47,318 Corpus contains the labels: pos (#25), ner (#24)\n",
      "2021-09-29 21:13:47,319 Created (for label 'ner') Dictionary with 4 tags: O, B-PER, I-PER, B-LOC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<flair.data.Dictionary at 0x7f4afcfa0070>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. get the corpus\n",
    "# we will use the corpus we created earlier \n",
    "\n",
    "# 2. what label do we want to predict?\n",
    "ner_label_type = 'ner'\n",
    "\n",
    "# 3. make the label dictionary from the corpus\n",
    "ner_label_dict = corpus.make_label_dictionary(label_type=ner_label_type)\n",
    "\n",
    "ner_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768aaddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings\n",
    "\n",
    "# 4. initialize embeddings\n",
    "ner_embedding_types = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward')\n",
    "]\n",
    "\n",
    "ner_embeddings = StackedEmbeddings(embeddings=ner_embedding_types)\n",
    "\n",
    "ner_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "ner_tagger = SequenceTagger(hidden_size=10,\n",
    "                        embeddings=ner_embeddings,\n",
    "                        tag_dictionary=ner_label_dict,\n",
    "                        tag_type=ner_label_type,\n",
    "                        use_crf=True)\n",
    "\n",
    "ner_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b96dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "ner_model_path = '/home/kariminf/Data/tutoriel/flair_ner.tagger.fr'\n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(ner_tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train(ner_model_path,\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=10, \n",
    "              max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1896c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model you trained\n",
    "pos_load_model = SequenceTagger.load(pos_model_path)\n",
    "\n",
    "# create example sentence\n",
    "sentence_fr = Sentence('il peut aider')\n",
    "\n",
    "# predict tags and print\n",
    "pos_load_model.predict(sentence_fr)\n",
    "\n",
    "sentence_fr.to_tagged_string()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
