{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ad3a23",
   "metadata": {},
   "source": [
    "# Word representation using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ac6416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['this is the first document',\n",
       "       'this document is the second document',\n",
       "       'this is the third one not the first nor the third',\n",
       "       'is this the first document or is is another document'],\n",
       "      dtype='<U52')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([\n",
    "    'this is the first document',\n",
    "    'this document is the second document',\n",
    "    'this is the third one not the first nor the third',\n",
    "    'is this the first document or is is another document'\n",
    "])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa495c2a",
   "metadata": {},
   "source": [
    "## I. Vectorization\n",
    "\n",
    "### I.1. Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6815937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['another' 'document' 'first' 'is' 'nor' 'not' 'one' 'or' 'second' 'the'\n",
      " 'third' 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 0, 0, 3, 2, 1],\n",
       "       [1, 2, 1, 3, 0, 0, 0, 1, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer()\n",
    "TF = tf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(tf_vectorizer.get_feature_names_out())\n",
    "\n",
    "TF.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a1053",
   "metadata": {},
   "source": [
    "### I.2. Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1172857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['another' 'document' 'first' 'is' 'nor' 'not' 'one' 'or' 'second' 'the'\n",
      " 'third' 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.49967281, 0.49967281, 0.40851526, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.40851526,\n",
       "        0.        , 0.40851526],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.53864762, 0.28108867,\n",
       "        0.        , 0.28108867],\n",
       "       [0.        , 0.        , 0.19789669, 0.16179351, 0.3100434 ,\n",
       "        0.3100434 , 0.3100434 , 0.        , 0.        , 0.48538052,\n",
       "        0.6200868 , 0.16179351],\n",
       "       [0.37708861, 0.48138155, 0.24069077, 0.59034144, 0.        ,\n",
       "        0.        , 0.        , 0.37708861, 0.        , 0.19678048,\n",
       "        0.        , 0.19678048]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "TFIDF = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "TFIDF.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961d73f",
   "metadata": {},
   "source": [
    "### I.3. Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfdd139a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1223226  0.68092366 0.15434875 0.04240499]\n",
      "0.9999999999999998\n",
      "[5.91216815 3.40420027 1.6491539  0.85905745]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "\n",
    "svd.fit(TF)\n",
    "\n",
    "print(svd.explained_variance_ratio_)\n",
    "\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "\n",
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b39db47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0601922 , -0.30909615,  0.15622694,  0.7972834 ],\n",
       "       [ 2.26591799, -0.94003037,  1.39040475, -0.22075592],\n",
       "       [ 3.37398661,  2.75685491, -0.05652745, -0.11300441],\n",
       "       [ 3.76711697, -1.73468527, -0.87113775, -0.20202899]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.transform(TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc9e40",
   "metadata": {},
   "source": [
    "## II. Parameters\n",
    "\n",
    "### II.1. Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c5e36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also' 'another' 'are' 'be' 'can' 'contains' 'document' 'documents'\n",
      " 'have' 'is' 'it' 'list' 'long' 'of' 'or' 'other' 'sentences' 'short'\n",
      " 'some' 'these' 'this' 'very']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 2, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 2, 1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read many documents\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "files_list = [\n",
    "    'docs/doc1.txt',\n",
    "    'docs/doc2.txt',\n",
    "    'docs/doc3.txt'\n",
    "]\n",
    "\n",
    "tf_vectorizer_docs = CountVectorizer(input='filename')\n",
    "TF_docs = tf_vectorizer_docs.fit_transform(files_list)\n",
    "\n",
    "print(tf_vectorizer_docs.get_feature_names_out())\n",
    "\n",
    "TF_docs.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a0c0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also' 'another' 'are' 'contains' 'document' 'is' 'it' 'other'\n",
      " 'sentences' 'short' 'some' 'these' 'this' 'very']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read a single file\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer_file = CountVectorizer(input='file')\n",
    "\n",
    "f1 = open('docs/doc1.txt', 'r')\n",
    "f2 = open('docs/doc2.txt', 'r')\n",
    "\n",
    "TF_file = tf_vectorizer_file.fit_transform([f1, f2])\n",
    "\n",
    "print(tf_vectorizer_file.get_feature_names_out())\n",
    "\n",
    "TF_file.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca54de04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be' 'can' 'document' 'documents' 'each' 'have' 'in' 'is' 'line' 'list'\n",
      " 'long' 'of' 'or' 'sentences' 'short' 'these']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read a file where each  line is a document\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer_fileln = CountVectorizer()\n",
    "\n",
    "f = open('docs/docln.txt', 'r')\n",
    "\n",
    "TF_fileln = tf_vectorizer_fileln.fit_transform(f)\n",
    "\n",
    "print(tf_vectorizer_fileln.get_feature_names_out())\n",
    "\n",
    "TF_fileln.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06112a4e",
   "metadata": {},
   "source": [
    "### II.2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f13b68d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABDELKADER' 'AM' 'AM ' 'BUY' 'GOING' 'HAS' 'HOPE' 'I' 'I ' 'STILL' 'TEA'\n",
      " 'TIRED' 'TO' 'VERY']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data_sharp = [\n",
    "    'I#AM #VERY#TIRED',\n",
    "    'I#AM#GOING#TO#BUY#TEA',\n",
    "    'I #HOPE#ABDELKADER#STILL#HAS#TEA'\n",
    "]\n",
    "\n",
    "def sharp_tokenizer(text):\n",
    "    return text.split('#')\n",
    "\n",
    "# tokenizer apply only if analyzer=='word'\n",
    "# we can desactivate automatic lowercasing\n",
    "token_vectorizer = CountVectorizer(tokenizer=sharp_tokenizer, lowercase=False)\n",
    "\n",
    "\n",
    "TF_token = token_vectorizer.fit_transform(data_sharp)\n",
    "\n",
    "print(token_vectorizer.get_feature_names_out())\n",
    "\n",
    "TF_token.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0b7f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABDELKADER' 'AM' 'BUY' 'GOING' 'HAS' 'HOPE' 'I' 'STILL' 'TEA' 'TIRED'\n",
      " 'TO' 'VERY']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def sharp_preprocessor(text):\n",
    "    return text.replace(' ', '')\n",
    "\n",
    "# tokenizer apply only if analyzer=='word'\n",
    "# we can desactivate automatic lowercasing\n",
    "token_vectorizer = CountVectorizer(preprocessor=sharp_preprocessor, tokenizer=sharp_tokenizer)\n",
    "\n",
    "\n",
    "TF_token = token_vectorizer.fit_transform(data_sharp)\n",
    "\n",
    "print(token_vectorizer.get_feature_names_out())\n",
    "\n",
    "TF_token.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed79dbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document' 'second']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [2, 1],\n",
       "       [0, 0],\n",
       "       [2, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predefined stop-words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer_sw = CountVectorizer(stop_words='english')\n",
    "\n",
    "TF_sw = tf_vectorizer_sw.fit_transform(data)\n",
    "\n",
    "print(tf_vectorizer_sw.get_feature_names_out())\n",
    "\n",
    "TF_sw.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e2ed1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['another' 'document' 'first' 'one' 'second' 'third']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 0],\n",
       "       [0, 2, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 0, 2],\n",
       "       [1, 2, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given stop-words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sw_list = ['the', 'this', 'or', 'nor', 'not', 'is']\n",
    "\n",
    "tf_vectorizer_sw = CountVectorizer(stop_words=sw_list)\n",
    "\n",
    "TF_sw = tf_vectorizer_sw.fit_transform(data)\n",
    "\n",
    "print(tf_vectorizer_sw.get_feature_names_out())\n",
    "\n",
    "TF_sw.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf35cd",
   "metadata": {},
   "source": [
    "## III. Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f775592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.79056942, 0.61558701, 0.84327404],\n",
       "       [0.79056942, 1.        , 0.40555355, 0.75      ],\n",
       "       [0.61558701, 0.40555355, 1.        , 0.43259046],\n",
       "       [0.84327404, 0.75      , 0.43259046, 1.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# if we pass a matrix (document X term), we will get a similarity (document X document)\n",
    "Sim = cosine_similarity(TF)\n",
    "\n",
    "# The diagonale is always 1, because each document is similar to itself\n",
    "# This representation is somehow a complete graph\n",
    "Sim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
