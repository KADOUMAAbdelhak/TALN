{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191e523e",
   "metadata": {},
   "source": [
    "# Encodage du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b20c43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['This is the first document.', '1'],\n",
       "       ['This document is the second document.', '0'],\n",
       "       ['this is the third one not the first nor the third.', '1'],\n",
       "       ['Is this the first document or is is another document?', '0']],\n",
       "      dtype='<U53')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# supposant nous avons 4 textes (les lignes)\n",
    "# pour chaque texte (colonne 0), on accorde une classe (colonne 1)\n",
    "data = np.array([\n",
    "    [\"This is the first document.\", 1],\n",
    "    [\"This document is the second document.\", 0],\n",
    "    [\"this is the third one not the first nor the third.\", 1],\n",
    "    [\"Is this the first document or is is another document?\", 0]\n",
    "])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1d9105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['another', 'document', 'first', 'is', 'nor', 'not', 'one', 'or', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 0, 0, 3, 2, 1],\n",
       "       [1, 2, 1, 3, 0, 0, 0, 1, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF : term frequency\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X = count_vectorizer.fit_transform(data[:, 0]) #appliquer sur toutes les lignes, colonne 0\n",
    "\n",
    "# Afficher les différents terms\n",
    "print(count_vectorizer.get_feature_names())\n",
    "\n",
    "# Afficher la matrice (document X term)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "442af356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.79056942, 0.61558701, 0.84327404],\n",
       "       [0.79056942, 1.        , 0.40555355, 0.75      ],\n",
       "       [0.61558701, 0.40555355, 1.        , 0.43259046],\n",
       "       [0.84327404, 0.75      , 0.43259046, 1.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# la fonction accepte deux vecteurs X et Y (pour calculer la similarité entre les deux)\n",
    "# Si on fourni X comme une matrice, sans fournir Y\n",
    "# la fonction va calculer la cosinus entre chaque ligne\n",
    "# Donc, on aura une matrice (document X document)\n",
    "Sim = cosine_similarity(X)\n",
    "\n",
    "# Bien sûr, le diagonale aura 1 puisque c'est la similarité du texte avec lui-même\n",
    "Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b247894a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56235762, 0.48653074, 0.36343276, 0.50646612])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pour calculer la moyenne des similarité de chaque document avec les autres\n",
    "# On fait la somme sur les lignes ou les colonne (la matrice est symétrique : égale à son transposé)\n",
    "# On dimunie la somme par 1 (la similarité du document avec lui-même)\n",
    "# ensuite, on divise sur le nombre des lignes (documents)\n",
    "Moy = (Sim.sum(axis=0) - 1)/Sim.shape[0]\n",
    "\n",
    "Moy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decf6ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False,  True])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supposant, on veut garder seulement les documents avec une similarité moyenne >= 0.5\n",
    "Garder = Moy >= 0.5\n",
    "\n",
    "Garder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b41f58e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['This is the first document.',\n",
       "       'Is this the first document or is is another document?'],\n",
       "      dtype='<U53')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extraire les documents à garder\n",
    "# Sélectionner les lignes avec True et la colonne 0\n",
    "Documents_pertinents = data[Garder, 0]\n",
    "\n",
    "Documents_pertinents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0362fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['another', 'document', 'first', 'one', 'second', 'third']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.70710678, 0.70710678, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.78722298, 0.        , 0.        , 0.61666846,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.27448674, 0.43003652, 0.        ,\n",
       "        0.86007303],\n",
       "       [0.57381765, 0.73252075, 0.36626037, 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ici, je vais fournir une liste des mots vides\n",
    "mots_vides = {\"not\", \"is\", \"are\", \"this\", \"that\", \"or\", \"and\", \"the\", \"nor\"}\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=mots_vides)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data[:, 0]) #appliquer sur toutes les lignes, colonne 0\n",
    "\n",
    "# Afficher les différents terms\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Afficher la matrice (document X term)\n",
    "X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba256f",
   "metadata": {},
   "source": [
    "## Word2Vec en iutilisant Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45473b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'the', 'first', 'document.'],\n",
       " ['This', 'document', 'is', 'the', 'second', 'document.'],\n",
       " ['this',\n",
       "  'is',\n",
       "  'the',\n",
       "  'third',\n",
       "  'one',\n",
       "  'not',\n",
       "  'the',\n",
       "  'first',\n",
       "  'nor',\n",
       "  'the',\n",
       "  'third.'],\n",
       " ['Is',\n",
       "  'this',\n",
       "  'the',\n",
       "  'first',\n",
       "  'document',\n",
       "  'or',\n",
       "  'is',\n",
       "  'is',\n",
       "  'another',\n",
       "  'document?']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ici, un tableau de phrases, où chaque phrase contient des mots\n",
    "phrases = [[mot for mot in phrase.split()] for phrase in data[:, 0]]\n",
    "\n",
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbb7c7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaire = {'the': 0, 'is': 1, 'first': 2, 'This': 3, 'this': 4, 'document.': 5, 'document': 6, 'second': 7, 'document?': 8, 'another': 9, 'one': 10, 'not': 11, 'nor': 12, 'third.': 13, 'Is': 14, 'or': 15, 'third': 16}\n",
      "Le embedding de 'first' = [-0.46529216 -0.35571396]\n",
      "Le embedding de 'document' = [-0.22693975  0.3276365 ]\n",
      "cos('first', 'document') = 1.0469160676002502\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "#un modèle avec un vecteur de 2, une fenêtre de 3 et 20 itérations\n",
    "model_w2v= Word2Vec(sentences=phrases, vector_size=2, window=3, epochs=20, min_count=1)\n",
    "word_vectors = model_w2v.wv\n",
    "\n",
    "print (\"vocabulaire = \" + str(word_vectors.key_to_index))\n",
    "print(\"Le embedding de 'first' = \" + str(word_vectors[\"first\"]))\n",
    "print(\"Le embedding de 'document' = \" + str(word_vectors.get_vector(\"document\")))\n",
    "print(\"cos('first', 'document') = \" + str(word_vectors.distance(\"first\",\"document\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af9af064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour sauvegarder le word2vec : il ne faut pas entrainer à chaque fois\n",
    "model_w2v.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cc2961e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('first', 0.980423629283905),\n",
       " ('this', 0.978571891784668),\n",
       " ('Is', 0.9327181577682495),\n",
       " ('second', 0.8803404569625854),\n",
       " ('or', 0.7467302083969116)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# récupérer un modèle sauvegardé\n",
    "model2 = Word2Vec.load(\"word2vec.model\")\n",
    "\n",
    "#tester le modèle 2 avec les mots les plus proches à un autre\n",
    "# les 5 mots les plus similaires\n",
    "mot = \"another\"\n",
    "sim_mot = []\n",
    "if mot in model2.wv: # tester si le mot existe dans la vocabulaire\n",
    "    sim_mot = model2.wv.most_similar(mot, topn=5)\n",
    "sim_mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a7cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour plus d'info sur Gensim \n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "#Il y a aussi , doc2vec \n",
    "# https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
