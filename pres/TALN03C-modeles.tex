% !TEX TS-program = pdflatex
% !TeX program = pdflatex
% !TEX encoding = UTF-8
% !TEX spellcheck = fr

\documentclass[xcolor=table]{beamer}

\input{options}

\title[TALN : 03- Modèles de langage]%
{Traitement automatique du langage naturel\\Chapitre 03 : Modèles de langage} 

\changegraphpath{../img/modeles/}

\begin{document}
	
\begin{frame}
\frametitle{Traitement automatique du langage naturel}
\framesubtitle{Modèles de langage : Introduction}

\begin{itemize}
	\item \optword{Probabilité d'une phrase} : $\color{red} P(S) = P(w_1, w_2, ..., w_n) $
	\begin{itemize}
		\item Traduction automatique : \\
		\expword{My tall brother \textrightarrow\ P(Mon grand frère) \textgreater\ P(Mon haut frère)}
		\item Correction des fautes grammaticales : \\
		\expword{P(Un objet qu'on puisse emporter) \textgreater\ P(Un objet qu'ont puisse emporter)}
		\item Reconnaissance de paroles : \\
		\expword{P(Jeudi matin) \textgreater\ P(Je dis matin)}
	\end{itemize}
	\item \optword{Probabilité d'occurrence d'un mot} : $\color{red} P(w_i | w_1, \ldots, w_{i-1}) $
	\begin{itemize}
		\item Auto-complétion : \\
		\expword{P(traitement automatique de l'information) \textgreater\ P(traitement automatique de l'eau)}
		\item Génération automatique de textes
	\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Traitement automatique du langage naturel}
\framesubtitle{Modèles de langage : Plan}

\begin{multicols}{2}
%	\small
\tableofcontents
\end{multicols}
\end{frame}

%===================================================================================
\section{Modèle N-gramme}
%===================================================================================

\begin{frame}
\frametitle{Modèles de langage}
\framesubtitle{Modèle N-gramme}

\begin{block}{Formule des probabilités composées}
	$ P(w_1 \ldots w_m) =  P(w_1) P(w_2 | w_1) P(w_3 | w_1, w_2) \ldots P(w_n | w_1, \ldots, w_{m-1})$
\end{block}


\begin{exampleblock}{Exemple de probabilité d'une phrase}
	$ P(\text{\textit{je travaille à l'ESI}}) =  P(je) P(travaille | je) P(\text{\textit{à}} | \text{\textit{je travaille}}) \ldots P(ESI | \text{\textit{je travaille à l'}})$
	
	$P(ESI | \text{\textit{je travaille à l'}}) = \frac{C(\text{\textit{je travaille à l'ESI}})}{C( \text{\textit{je travaille à l'}})}$ 
	
	Où 
	
	$C$ est la fonction qui compte le nombre d'occurrences d'une expression dans un corpus d'entraînement
\end{exampleblock}

\begin{itemize}
	\item Plusieurs phrases possibles
	\item Il faut un grand corpus pour estimer cette probabilité (infinité de phrases possibles)
\end{itemize}

\end{frame}

\subsection{Formulation}

\begin{frame}
\frametitle{Modèles de langage : N-gramme}
\framesubtitle{Formulation : Propriété de Markov}

\begin{block}{Propriété de Markov}
	Un état futur ne dépend que de l'état présent. 
	\[%
	P(w_i | w_1,\ldots, w_{i-1}) \approx P(w_i | w_{i-1})
	\]
	Cas général avec $n-1$ états passés 
	\[%
	P(w_i | w_1,\ldots, w_{i-1}) \approx P(w_i | w_{i-n+1}, \ldots, w_{i-1})
	\]
\end{block}

\begin{block}{Estimation de probabilité en utilisant les N-grammes (n grammes)}
	\[
	P(w_1,\ldots, w_{m}) \approx \prod_{i=1}^m P(w_i | w_{i-k+1}, \ldots, w_{i-1})
	\]
\end{block}

\end{frame}

\begin{frame}
\frametitle{Modèles de langage : N-gramme}
\framesubtitle{Formulation : Quelques modèles}

\begin{itemize}
	\item \optword{Modèle uni-gramme} : $P(w_i | w_1,\ldots, w_{i-1}) \approx P(w_i)$
	\item \optword{Modèle bi-gramme} : $P(w_i | w_1,\ldots, w_{i-1}) \approx P(w_i | w_{i-1})$
	\item \optword{Modèle tri-gramme} :  $P(w_i | w_1,\ldots, w_{i-1}) \approx P(w_i | w_{i-2}, w_{i-1})$
	\item Google Books Ngram Viewer
	\begin{itemize}
		\item \url{https://books.google.com/ngrams}
		\item Modèles pré-traités à partir des livres 
		\item Téléchargement gratuit : \url{https://storage.googleapis.com/books/ngrams/books/datasetsv3.html}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Modèles de langage : N-gramme}
\framesubtitle{Formulation : Google Books Ngram Viewer (Humour)}

\hgraphpage{humour-ngram.png}

\end{frame}

\begin{frame}
\frametitle{Modèles de langage : N-gramme}
\framesubtitle{Formulation : Estimation}

\begin{itemize}
	\item En utilisant un corpus d'entraînement avec suffisamment de données
	\item On marque le début et la fin des phrases avec \keyword{\textless s\textgreater} et \keyword{\textless/s\textgreater} (une fois pour les bi-grammes, 2 fois pour les tri-grammes, etc.)
	\item \keyword{Estimateur du maximum de vraisemblance}
\end{itemize}

\begin{block}{Estimation des probabilités en utilisant le maximum de vraisemblance}
	{\small \[%
	P(w_i | w_{i-n+1},\ldots, w_{i-1}) = \frac{C(w_{i-n+1} \ldots w_{i-1} w_i)}{\sum_i C(w_{i-n+1} \ldots w_{i-1} w_i)}
	= \frac{C(w_{i-n+1} \ldots w_{i-1} w_i)}{C(w_{i-n+1} \ldots w_{i-1})}
	\]}
	Où $C$ est le nombre d'occurrences des N-grammes dans le corpus
	\[%
	\text{Bi-grammes : } P(w_i | w_{i-1}) = \frac{C(w_{i-1} w_i)}{C(w_{i-1})}
	\]
\end{block}

\end{frame}

\begin{frame}
\frametitle{Modèles de langage : N-gramme}
\framesubtitle{Formulation : Exemple}

\begin{exampleblock}{Exemple d'un corpus d'entraînement}
	\begin{itemize}
		\item \textless s\textgreater un ordianteur peut vous aider \textless/s\textgreater
		\item \textless s\textgreater il veut vous aider \textless/s\textgreater
		\item \textless s\textgreater il veut un ordinateur \textless/s\textgreater
		\item \textless s\textgreater il peut nager \textless/s\textgreater
	\end{itemize}
\end{exampleblock}

\begin{itemize}
	\item $P(peut | il) = \frac{C(il\ peut)}{C(il)} = \frac{1}{3}$
	\item $P(\text{\textit{\textless s\textgreater il peut vous aider \textless/s\textgreater}}) = 
	\underbrace{P(il|\text{\textit{\textless s\textgreater}})}_{\frac{3}{4}}
	\underbrace{P(peut|il)}_{\frac{1}{3}} 
	\underbrace{P(vous|peut)}_{\frac{1}{2}} 
	\underbrace{P(aider|vous)}_{\frac{2}{2}}
	\underbrace{P(\text{\textit{\textless/s\textgreater}}|aider)}_{\frac{2}{2}} = 
%	\frac{3}{4} \frac{1}{3} \frac{1}{2} \frac{2}{2} \frac{2}{2} = \frac{1}{8}
	\frac{1}{8}
	$
\end{itemize}


\end{frame}

\subsection{Lissage (Smoothing)}

\begin{frame}
\frametitle{Modèles de langage : N-gramme}
\framesubtitle{Lissage (Smoothing)}

\begin{itemize}
	
	\item Si on utilise des petits N-grammes $ \Longrightarrow $ Perte de l'information
	\begin{itemize}
		\item Les langues permettent des dépendances à long terme
		\item \expword{\underline{L'ordinateur} que j'ai utilisé hier à l'ESI pendant la  séance du cours \underline{a planté}}
	\end{itemize}

	\item Si on utilise des grands N-grammes $ \Longrightarrow $ Complexité élevée du modèle
	\begin{itemize}
		\item Il faut un corpus plus grand
		\item Représentation des N-grammes : $V^N$ où $V$ est la taille du vocabulaire et $N$ est le nombre de grammes
	\end{itemize}

	\item Problème des N-grammes absents dans le corpus d'entraînement
	\begin{itemize}
		\item $P(\text{\textit{\textless s\textgreater il veut nager \textless/s\textgreater}}) = 
		P(il|\text{\textit{\textless s\textgreater}}) P(veut|il) P(nager|veut)  P(\text{\textit{\textless/s\textgreater}}|nager) = 
		\frac{3}{4} \frac{2}{3} \frac{0}{1} \frac{1}{1} = 0
		$
	\end{itemize}
	\item L'intuition est d'emprunter une petite portion des probabilités des N-grammes existants pour former une probabilité aux N-grammes absents 
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Modèles de langage : N-gramme}
\framesubtitle{Lissage (Smoothing) : Lidstone}

\begin{block}{Lissage de Lidstone (Bi-grammes comme exemple)}
	\[%
	P(w_i | w_{i-1}) = \frac{C(w_{i-1} w_i) + \alpha}{C(w_{i-1}) + \alpha V}
	\]
	Où $V$ est la taille du vocabulaire du modèle
	
	$\alpha = 1$ : \keyword{Lissage de Laplace} 
	
	$\alpha = 0.5$ : \keyword{Loi de Jeffreys-Perks}
\end{block}

\begin{exampleblock}{Exemple : lissage de Laplace}
	\begin{itemize}
		\item Le corpus contient 8 mots différents
		\item Il y a  $8^2 = 64$ bi-grammes possibles
		\item $P(\text{\textit{\textless s\textgreater il veut nager \textless/s\textgreater}}) = 
		P(il|\text{\textit{\textless s\textgreater}}) P(veut|il) P(nager|veut)  P(\text{\textit{\textless/s\textgreater}}|nager) = 
		\frac{3 + 1}{4 + 64} \frac{2 + 1}{3 + 64} \frac{0 + 1}{1 + 64} \frac{1 + 1}{1 + 64} $
	\end{itemize}
\end{exampleblock}

\end{frame}

\begin{frame}
\frametitle{Modèles de langage : N-gramme}
\framesubtitle{Lissage (Smoothing) : Interpolation}

\begin{block}{Interpolation (Tri-grammes comme exemple)}
	\[%
	P_{I}(w_i | w_{i-2} w_{i-1}) = 
	\lambda_3 P(w_i | w_{i-2} w_{i-1}) 
	+ \lambda_2 P(w_i | w_{i-1}) 
	+ \lambda_1 P(w_i) 
	\]
	
	Où $\sum_j \lambda_j = 1$
	
	$\lambda_3$, $\lambda_2$ et $\lambda_1$ sont estimés en utilisant un autre corpus de réglage
\end{block}

%\begin{exampleblock}{Exemple}
%	\begin{itemize}
%		\item Le corpus contient 8 mots différents
%		\item Il y a  $8^2 = 64$ bi-grammes possibles
%		\item $P(\text{\textit{\textless s\textgreater il veut nager \textless/s\textgreater}}) = 
%		P(il|\text{\textit{\textless s\textgreater}}) P(veut|il) P(nager|veut)  P(\text{\textit{\textless/s\textgreater}}|nager) = 
%		\frac{3 + 1}{4 + 64} \frac{2 + 1}{3 + 64} \frac{0 + 1}{1 + 64} \frac{1 + 1}{1 + 64} $
%	\end{itemize}
%\end{exampleblock}

\end{frame}

\begin{frame}
\frametitle{Modèles de langage : N-gramme}
\framesubtitle{Lissage (Smoothing) : Back-off de Katz}

\begin{block}{Back-off de Katz (Tri-grammes comme exemple)}
	\[%
	P_{BO}(w_i | w_{i-2} w_{i-1}) = 
	\begin{cases}
	P^*(w_i | w_{i-2} w_{i-1}) & \text{si } C(w_{i-2} w_{i-1} w_i) > 0 \\
	\alpha(w_{i-2} w_{i-1}) P_{BO}(w_i | w_{i-1}) & \text{sinon}
	\end{cases}
	\]
	
	Où : 
	
	$P^*$ est la probabilité réduite (la réduction sera distribuée sur les probabilités des N-grammes de l'ordre inférieur)
	
	$\alpha$ est une fonction qui distribue la réduction selon le contexte
\end{block}
%
%\begin{exampleblock}{Exemple}
%	\begin{itemize}
%		\item Le corpus contient 8 mots différents
%		\item Il y a  $8^2 = 64$ bi-grammes possibles
%		\item $P(\text{\textit{\textless s\textgreater il veut nager \textless/s\textgreater}}) = 
%		P(il|\text{\textit{\textless s\textgreater}}) P(veut|il) P(nager|veut)  P(\text{\textit{\textless/s\textgreater}}|nager) = 
%		\frac{3 + 1}{4 + 64} \frac{2 + 1}{3 + 64} \frac{0 + 1}{1 + 64} \frac{1 + 1}{1 + 64} $
%	\end{itemize}
%\end{exampleblock}
%
\end{frame}


%===================================================================================
\section{Modèles neuronaux}
%===================================================================================

%\begin{frame}
%\frametitle{Modèles de langage}
%\framesubtitle{Modèles neuronaux}
%
%\begin{itemize}
%	\item 
%\end{itemize}
%
%\end{frame}

\subsection{Réseau de neurones à propagation avant}

\begin{frame}
\frametitle{Modèles de langage : Modèles neuronaux}
\framesubtitle{Réseau de neurones à propagation avant : Cas d'étude}

\begin{itemize}
	\item On va suivre le modèle de \cite{2003-bengio-al}
	\item On choisit le nombre des n-grammes $n$ ; donc, le nombre des mots en entrée c'est $n-1$
	\item Le vecteur $m$ est la concaténation des embeddings des mots précédents
	\item On choisit la taille de vecteur de représentation (embedding) $d$
	\item Les mots sont encodés sous forme One-Hot (un vecteur avec une taille $V$ du vocabulaire)
\end{itemize}

\begin{block}{Modèle neuronal à propagation avant}
	$
	P(h_i|h_{i-n+1},\ldots, h_{i-1}) = 
	Softmax \left(
	(b + m A) 
	+ 
	W\ Tanh(u + m T)
	\right)
	$
	
	Où $b \in \mathbb{R}^{V},\, A \in \mathbb{R}^{(n-1) \times d \times V},\, u \in \mathbb{R}^{H},\, T \in \mathbb{R}^{(n-1) \times d \times H},\, W \in \mathbb{R}^{V \times H}$
\end{block}

\end{frame}

\begin{frame}
\frametitle{Modèles de langage : Modèles neuronaux}
\framesubtitle{Réseau de neurones à propagation avant : Modèle}

\hgraphpage{fw-model.pdf}

\end{frame}

\subsection{Réseau de neurones récurrents}

\begin{frame}
\frametitle{Modèles de langage : Modèles neuronaux}
\framesubtitle{Réseau de neurones récurrents : Cas d'étude}

\begin{itemize}
	\item On va suivre le modèle de \cite{2010-mokolov-al}
	\item Dans un instant $t$, on calcule l'état $s_t$ en se basant sur l'état précédent $s_{t-1}$ et le mot actuel $m_t$
	\item L'état $s_t$ est utilisé pour estimer les probabilités $y_t$ de chaque mot du vocabulaire
\end{itemize}

\begin{block}{Modèle neuronal récurrent}
	$x_t = s_{t-1} \bullet m_t$
	
	$s_t = \sigma(x_t W)$
	
	$y_t = softmax(s_t U)$
	
	Où $m_t \in \mathbb{R}^{V},\, s_t \in \mathbb{R}^{H},\, W \in \mathbb{R}^{(H+V)\times H},\, U \in \mathbb{R}^{H\times V}$
\end{block}

\end{frame}

\begin{frame}
\frametitle{Modèles de langage : Modèles neuronaux}
\framesubtitle{Réseau de neurones récurrents : Modèle}

\hgraphpage{rnn-model.pdf}

\end{frame}

\subsection{Quelques améliorations}

\begin{frame}
\frametitle{Modèles de langage : Modèles neuronaux}
\framesubtitle{Quelques améliorations}

\begin{itemize}
	\item Taille limitée du contexte dans les réseaux à propagation avant
	\begin{itemize}
		\item Utiliser les réseaux récurrents
	\end{itemize}
	\item Problème de disparition du gradient dans les réseaux récurrents
	\begin{itemize}
		\item Utiliser des réseaux plus avancés : \keyword{LSTM} et \keyword{GRU}
		\item Limiter la taille du contexte 
	\end{itemize}
	\item Mots hors vocabulaire
	\begin{itemize}
		\item Limiter le vocabulaire  et marquer le reste des mots par \optword{\textlangle UNK\textrangle}
	\end{itemize}
\end{itemize}

\end{frame}

%===================================================================================
\section{Évaluation}
%===================================================================================

\subsection{Approches}

\begin{frame}
\frametitle{Modèles de langage : Évaluation}
\framesubtitle{Approches}

\begin{itemize}
	\item \optword{Évaluation extrinsèque}
	\begin{itemize}
		\item Évaluer le modèle par rapport à une autre tâche : son effet
		\item Exemple, \expword{La qualité de traduction automatique en utilisant ce modèle} 
		\item Évaluation très couteuse
	\end{itemize}
	\item \optword{Évaluation intrinsèque}
	\begin{itemize}
		\item Évaluer le modèle par rapport à sa représentation du langage
		\item Exemple, \expword{Comparer deux modèles en se basant sur leurs capacités de représenter un dataset de test} 
		\item Ne garantit pas une bonne performance du modèle pour une tâche donnée
	\end{itemize}
\end{itemize}

\end{frame}

\subsection{Perplexité}

\begin{frame}
\frametitle{Modèles de langage : Évaluation}
\framesubtitle{Perplexité}

\begin{itemize}
	\item Mesurer la qualité de prédiction d'un modèle sur un corpus de test
	\item Utiliser la probabilité estimée sur un corpus de test de taille $N$
	\item Le modèle avec une perplexité minimale est le meilleur
	\item Il faut inclure la fin d'une phrase et le début de la suivante dans l'entraînement du modèle (puisque la perplexité traite tout le corpus comme une seule chaîne)
\end{itemize}

\begin{block}{Perplexité}
	\begin{center}
		$PP(w) = \sqrt[N]{\frac{1}{P(w_1 w_2 \ldots w_N)}}$
		
		$PP(w) = \sqrt[N]{\prod\limits_{i=1}^{N}\frac{1}{P(w_i | w_1 \ldots w_{i-1})}}$
	\end{center}
\end{block}

\end{frame}


\begin{frame}
\frametitle{Modèles de langage}
\framesubtitle{Un peu d'humour}

\begin{center}
	\vgraphpage{modele-humour.png}
\end{center}

\end{frame}

\insertbibliography{TALN03}{*}

\end{document}

