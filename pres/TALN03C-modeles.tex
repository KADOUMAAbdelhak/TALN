% !TEX TS-program = pdflatex
% !TeX program = pdflatex
% !TEX encoding = UTF-8
% !TEX spellcheck = fr

\documentclass[xcolor=table]{beamer}

\input{options}

\title[TALN : 03- Modèles de langue]%
{Traitement automatique du langage naturel\\Chapitre 03 : Modèles de langue} 

\changegraphpath{../img/modeles/}

\begin{document}
	
\begin{frame}
\frametitle{Traitement automatique du langage naturel}
\framesubtitle{Modèles de langue : Introduction}

\begin{itemize}
	\item Probabilité d'une phrase ($ P(S) = P(w_1, w_2, ..., w_n) $)
	\begin{itemize}
		\item Traduction automatique : \\
		\expword{My tall brother \textrightarrow P(Mon grand frère) \textgreater P(Mon haut frère)}
		\item Correction des fautes grammaticales : \\
		\expword{P(Un objet qu'on peut emporter) \textgreater P(Un objet qu'ont peut emporter)}
		\item Reconnaissance de paroles : \\
		\expword{P(Jeudi matin) \textgreater P(Je dit matin)}
	\end{itemize}
	\item Probabilité d'occurrence d'un mot ($ P(w_n | w_1, \ldots, w_{n-1}) $)
	\begin{itemize}
		\item Auto-complétion : \\
		\expword{P(traitement automatique de l'information) \textgreater P(traitement automatique de l'eau)}
		\item Génération automatique de textes
	\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Traitement automatique du langage naturel}
\framesubtitle{Modèles de langue : Plan}

\begin{multicols}{2}
%	\small
\tableofcontents
\end{multicols}
\end{frame}

%===================================================================================
\section{Modèle N-gramme}
%===================================================================================

\begin{frame}
\frametitle{Modèles de langue}
\framesubtitle{Modèle N-gramme}

\begin{block}{Formule des probabilités composées}
	$ P(x_1 \ldots x_n) =  P(x_1) P(x_2 | x_1) P(x_3 | x_1, x_2) \ldots P(x_n | x_1, \ldots, x_{n-1})$
\end{block}


\begin{exampleblock}{Exemple de probabilité d'une phrase}
	$ P(\text{\textit{je travaille à l'ESI}}) =  P(je) P(travaille | je) P(\text{\textit{à}} | \text{\textit{je travaille}}) \ldots P(ESI | \text{\textit{je travaille à l'}})$
	
	$P(ESI | \text{\textit{je travaille à l'}}) = \frac{N(\text{\textit{je travaille à l'ESI}})}{N( \text{\textit{je travaille à l'}})}$ 
	
	Où 
	
	$N$ est le nombre d'occurrence d'une phrase dans un corpus d'entraînement
\end{exampleblock}

\begin{itemize}
	\item Plusieurs phrases possibles
	\item Il faut faut un grand corpus pour estimer cette probabilité (infinité de phrases possibles)
\end{itemize}

\end{frame}

\subsection{Formulation}

\begin{frame}
\frametitle{Modèles de langue : N-gramme}
\framesubtitle{Formulation : Propriété de Markov}

\begin{block}{Propriété de Markov}
	Un état futur ne dépend que de l'état présent. 
	\[%
	P(x_n | x_1,\ldots, x_{n-1}) \approx P(x_n | x_{n-1})
	\]
	Cas général avec $k$ états passés 
	\[%
	P(x_1 \ldots x_n) = P(x_n | x_{n-k}, \ldots, x_{n-1})
	\]
\end{block}

\begin{block}{Estimation de probabilité en utilisant les N-grammes (k grammes)}
	\[
	P(x_n | x_1,\ldots, x_{n-1}) \approx \prod_i P(x_i | x_{i-k}, \ldots, x_{i-1})
	\]
\end{block}

\end{frame}

\begin{frame}
\frametitle{Modèles de langue : N-gramme}
\framesubtitle{Formulation : Quelques modèles}

\begin{itemize}
	\item \optword{Modèle uni-gramme} : $P(w_n | w_1,\ldots, w_{n-1}) \approx P(w_n)$
	\item \optword{Modèle bi-gramme} : $P(w_n | w_1,\ldots, w_{n-1}) \approx P(w_n | w_{n-1})$
	\item \optword{Modèle tri-gramme} :  $P(w_n | w_1,\ldots, w_{n-1}) \approx P(w_n | w_{n-2}, w_{n-1})$
	\item Google Books Ngram Viewer
	\begin{itemize}
		\item \url{https://books.google.com/ngrams}
		\item Modèles pré-traités à partir des livres 
		\item Téléchargement gratuit : \url{https://storage.googleapis.com/books/ngrams/books/datasetsv3.html}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Modèles de langue : N-gramme}
\framesubtitle{Formulation : Google Books Ngram Viewer (Humour)}

\hgraphpage{humour-ngram.png}

\end{frame}

\begin{frame}
\frametitle{Modèles de langue : N-gramme}
\framesubtitle{Formulation : Estimation}

\begin{itemize}
	\item en utilisant un corpus d'entraînement avec suffisamment de données
	\item on marque le début et la fin des phrases avec \keyword{\textless s\textgreater} et \keyword{\textless/s\textgreater} (une fois pour les bi-grammes, 2 fois pour les tri-grammes, etc.)
	\item \keyword{l'estimateur du maximum de vraisemblance}
\end{itemize}

\begin{block}{Estimation des probabilités en utilisant le maximum de vraisemblance}
	\[%
	P(w_n | w_{n-k},\ldots, w_{n-1}) = \frac{N(w_{n-k} \ldots w_{n-1} w_n)}{\sum_i N(w_{n-k} \ldots w_{n-1} w_i)}
	= \frac{N(w_{n-k} \ldots w_{n-1} w_n)}{N(w_{n-k} \ldots w_{n-1})}
	\]
	Où $N$ est le nombre d'occurrences des N-grammes dans le corpus
	\[%
	\text{Bi-grammes : } P(w_n | w_{n-1}) = \frac{N(w_{n-1} w_n)}{N(w_{n-1})}
	\]
\end{block}

\end{frame}

\begin{frame}
\frametitle{Modèles de langue : N-gramme}
\framesubtitle{Formulation : Exemple}

\begin{exampleblock}{Exemple d'un corpus d'entraînement}
	\begin{itemize}
		\item \textless s\textgreater un ordianteur peut vous aider \textless/s\textgreater
		\item \textless s\textgreater il veut vous aider \textless/s\textgreater
		\item \textless s\textgreater il veut un ordinateur \textless/s\textgreater
		\item \textless s\textgreater il peut nager \textless/s\textgreater
	\end{itemize}
\end{exampleblock}

\begin{itemize}
	\item $P(peut | il) = \frac{N(il\ peut)}{N(il)} = \frac{1}{3}$
	\item $P(\text{\textit{\textless s\textgreater il peut vous aider \textless/s\textgreater}}) = 
	P(il|\text{\textit{\textless s\textgreater}}) P(peut|il) P(vous|peut) P(aider|vous) P(\text{\textit{\textless/s\textgreater}}|aider) = 
	\frac{3}{4} \frac{1}{3} \frac{1}{2} \frac{2}{2} \frac{2}{2} = \frac{1}{8}
	$
\end{itemize}


\end{frame}

\subsection{Lissage (Smoothing)}

\begin{frame}
\frametitle{Modèles de langue : N-gramme}
\framesubtitle{Lissage (Smoothing)}

\begin{itemize}
	
	\item Si on utilise des petits N-grammes $ \Longrightarrow $ Perte de l'information
	\begin{itemize}
		\item Les langues permettent des dépendances à long terme
		\item \expword{\underline{L'ordinateur} que j'ai utilisé hier à l'ESI pendant la  séance du cours \underline{a planté}}
	\end{itemize}

	\item Si on utilise des grands N-grammes $ \Longrightarrow $ Complexité élevée du modèle
	\begin{itemize}
		\item Il faut un corpus plus grand
		\item Représentation des N-grammes : $V^N$ où $V$ est la taille du vocabulaire et $N$ est le nombre de grammes
	\end{itemize}

	\item Problème des N-grammes absents dans le corpus d'entraînement
	\begin{itemize}
		\item $P(\text{\textit{\textless s\textgreater il veut nager \textless/s\textgreater}}) = 
		P(il|\text{\textit{\textless s\textgreater}}) P(veut|il) P(nager|veut)  P(\text{\textit{\textless/s\textgreater}}|nager) = 
		\frac{3}{4} \frac{2}{3} \frac{0}{1} \frac{1}{1} = 0
		$
	\end{itemize}
	\item L'intuition est d'emprunter une petite portion des probabilités des N-grammes existants pour former une probabilité aux N-grammes absents 
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Modèles de langue : N-gramme}
\framesubtitle{Lissage (Smoothing) : Lidstone}

\begin{block}{Lissage de Lidstone (Bi-grammes comme exemple)}
	\[%
	P(w_n | w_{n-1}) = \frac{N(w_{n-1} w_n) + \alpha}{N(w_{n-1}) + \alpha V}
	\]
	Où $V$ est la taille du vocabulaire du modèle
	
	$\alpha = 1$ : \keyword{Lissage de Laplace} 
	
	$\alpha = 0.5$ : \keyword{Loi de Jeffreys-Perks}
\end{block}

\begin{exampleblock}{Exemple : lissage de Laplace}
	\begin{itemize}
		\item Le corpus contient 8 mots différents
		\item Il y a  $8^2 = 64$ bi-grammes possibles
		\item $P(\text{\textit{\textless s\textgreater il veut nager \textless/s\textgreater}}) = 
		P(il|\text{\textit{\textless s\textgreater}}) P(veut|il) P(nager|veut)  P(\text{\textit{\textless/s\textgreater}}|nager) = 
		\frac{3 + 1}{4 + 64} \frac{2 + 1}{3 + 64} \frac{0 + 1}{1 + 64} \frac{1 + 1}{1 + 64} $
	\end{itemize}
\end{exampleblock}

\end{frame}

\begin{frame}
\frametitle{Modèles de langue : N-gramme}
\framesubtitle{Lissage (Smoothing) : Interpolation}

\begin{block}{Interpolation (Tri-grammes comme exemple)}
	\[%
	P_{I}(w_n | w_{n-2} w_{n-1}) = 
	\lambda_3 P(w_n | w_{n-2} w_{n-1}) 
	+ \lambda_2 P(w_n | w_{n-1}) 
	+ \lambda_1 P(w_n) 
	\]
	
	Où $\sum_i \lambda_i = 1$
	
	$\lambda_3$, $\lambda_2$ et $\lambda_1$ sont estimés en utilisant un autre corpus de réglage
\end{block}

%\begin{exampleblock}{Exemple}
%	\begin{itemize}
%		\item Le corpus contient 8 mots différents
%		\item Il y a  $8^2 = 64$ bi-grammes possibles
%		\item $P(\text{\textit{\textless s\textgreater il veut nager \textless/s\textgreater}}) = 
%		P(il|\text{\textit{\textless s\textgreater}}) P(veut|il) P(nager|veut)  P(\text{\textit{\textless/s\textgreater}}|nager) = 
%		\frac{3 + 1}{4 + 64} \frac{2 + 1}{3 + 64} \frac{0 + 1}{1 + 64} \frac{1 + 1}{1 + 64} $
%	\end{itemize}
%\end{exampleblock}

\end{frame}

\begin{frame}
\frametitle{Modèles de langue : N-gramme}
\framesubtitle{Lissage (Smoothing) : Back-off de Katz}

\begin{block}{Back-off de Katz (Tri-grammes comme exemple)}
	\[%
	P_{BO}(w_n | w_{n-2} w_{n-1}) = 
	\begin{cases}
	P^*(w_n | w_{n-2} w_{n-1}) & \text{si } N(w_{n-2} w_{n-1} w_n) > 0 \\
	\alpha(w_{n-2} w_{n-1}) P_{BO}(w_n | w_{n-1}) & \text{sinon}
	\end{cases}
	\]
	
	Où : 
	
	$P^*$ est la probabilité réduite (la réduction sera distribuée sur les probabilités des N-grammes de l'ordre inférieur)
	
	$\alpha$ est une fonction qui distribue la réduction selon le contexte
\end{block}
%
%\begin{exampleblock}{Exemple}
%	\begin{itemize}
%		\item Le corpus contient 8 mots différents
%		\item Il y a  $8^2 = 64$ bi-grammes possibles
%		\item $P(\text{\textit{\textless s\textgreater il veut nager \textless/s\textgreater}}) = 
%		P(il|\text{\textit{\textless s\textgreater}}) P(veut|il) P(nager|veut)  P(\text{\textit{\textless/s\textgreater}}|nager) = 
%		\frac{3 + 1}{4 + 64} \frac{2 + 1}{3 + 64} \frac{0 + 1}{1 + 64} \frac{1 + 1}{1 + 64} $
%	\end{itemize}
%\end{exampleblock}
%
\end{frame}


%===================================================================================
\section{Modèles neuronaux}
%===================================================================================

%\begin{frame}
%\frametitle{Modèles de langue}
%\framesubtitle{Modèles neuronaux}
%
%\begin{itemize}
%	\item 
%\end{itemize}
%
%\end{frame}

\subsection{Réseau de neurones à propagation avant}

\begin{frame}
\frametitle{Modèles de langue : Modèles neuronaux}
\framesubtitle{Réseau de neurones à propagation avant : Cas d'étude}

\begin{itemize}
	\item On va suivre le modèle de \cite{2003-bengio-al}
	\item On choisit le nombre des n-grammes $n$ ; donc, le nombre des mots en entrée c'est $n-1$
	\item Le vecteur $m_j$ est le embedding de position $j$
	\item On choisit la taille de vecteur de représentation (embedding) $d$
	\item Les mots sont encodés sous forme One-Hot (un vecteur avec une taille $V$ du vocabulaire)
\end{itemize}

\begin{block}{Modèle neuronal à propagation avant}
	$
	P(.|h_1,\ldots, h_{n-1}) = 
	Softmax \left(
	(b + \sum\limits_{j=1}^{n-1} m_j A_j) 
	+ 
	W\ Tanh(u + \sum\limits_{j=1}^{n-1} m_j T_j)
	\right)
	$
	
	Où $b \in \mathbb{R}^{V},\, A \in \mathbb{R}^{(n-1) \times d \times V},\, u \in \mathbb{R}^{H},\, T \in \mathbb{R}^{(n-1) \times d \times H},\, W \in \mathbb{R}^{V \times H}$
\end{block}

\end{frame}

\begin{frame}
\frametitle{Modèles de langue : Modèles neuronaux}
\framesubtitle{Réseau de neurones à propagation avant : Modèle}

\hgraphpage{fw-model.pdf}

\end{frame}

\subsection{Réseau de neurones récurrents}

\begin{frame}
\frametitle{Modèles de langue : Modèles neuronaux}
\framesubtitle{Réseau de neurones récurrents : Cas d'étude}

\begin{itemize}
	\item On va suivre le modèle de \cite{2010-mokolov-al}
	\item Dans un instant $t$, on calcule l'état $s_t$ en se basant sur l'état précédent $s_{t-1}$ et le mot actuel $m_t$
	\item L'état $s_t$ est utilisé pour estimer les probabilités $y_t$ de chaque mot du vocabulaire
\end{itemize}

\begin{block}{Modèle neuronal récurrent}
	$x_t = s_{t-1} \bullet m_t$
	
	$s_t = \sigma(x_t W)$
	
	$y_t = softmax(s_t U)$
	
	Où $m_t \in \mathbb{R}^{V},\, s_t \in \mathbb{R}^{H},\, W \in \mathbb{R}^{(H+V)\times H},\, U \in \mathbb{R}^{H\times V}$
\end{block}

\end{frame}

\begin{frame}
\frametitle{Modèles de langue : Modèles neuronaux}
\framesubtitle{Réseau de neurones récurrents : Modèle}

\hgraphpage{rnn-model.pdf}

\end{frame}

\subsection{Quelques améliorations}

\begin{frame}
\frametitle{Modèles de langue : Modèles neuronaux}
\framesubtitle{Quelques améliorations}

\begin{itemize}
	\item Taille limitée du contexte dans les réseaux à propagation avant
	\begin{itemize}
		\item Utiliser les réseaux récurrents
	\end{itemize}
	\item Problème de disparition du gradient dans les réseaux récurrents
	\begin{itemize}
		\item Utiliser des réseaux plus avancés : \keyword{LSTM} et \keyword{GRU}
		\item Limiter la taille du contexte 
	\end{itemize}
	\item Mots hors vocabulaire
	\begin{itemize}
		\item Limiter le vocabulaire  et marquer le reste des mots par \optword{\textlangle UNK\textrangle}
	\end{itemize}
\end{itemize}

\end{frame}

%===================================================================================
\section{Évaluation}
%===================================================================================

\subsection{Approches}

\begin{frame}
\frametitle{Modèles de langue : Évaluation}
\framesubtitle{Approches}

\begin{itemize}
	\item \optword{Évaluation extrinsèque}
	\begin{itemize}
		\item Évaluer le modèle par rapport à une autre tâche : son effet
		\item Exemple, \expword{La qualité de traduction automatique en utilisant ce modèle} 
		\item Évaluation très couteuse
	\end{itemize}
	\item \optword{Évaluation intrinsèque}
	\begin{itemize}
		\item Évaluer le modèle par rapport à sa représentation du langage
		\item Exemple, \expword{Comparer deux modèles en se basant sur leurs capacités de représenter un dataset de test} 
		\item Ne garantit pas une bonne performance du modèle pour une tâche donnée
	\end{itemize}
\end{itemize}

\end{frame}

\subsection{Perplexité}

\begin{frame}
\frametitle{Modèles de langue : Évaluation}
\framesubtitle{Perplexité}

\begin{itemize}
	\item Mesurer la qualité de prédiction d'un modèle sur un corpus de test
	\item Utiliser la probabilité estimée sur un corpus de test de taille $N$
	\item Le modèle avec une perplexité minimale est le meilleur
	\item Il faut inclure la fin d'une phrase et le début de la suivante dans l'entraînement du modèle (puisque la perplexité traite tout le corpus comme une seule chaîne)
\end{itemize}

\begin{block}{Perplexité}
	\begin{center}
		$PP(w) = \sqrt[N]{\frac{1}{P(w_1 w_2 \ldots w_N)}}$
	\end{center}
\end{block}

\end{frame}


\begin{frame}
\frametitle{Modèles de langue}
\framesubtitle{Un peu d'humour}

\begin{center}
	\vgraphpage{modele-humour.png}
\end{center}

\end{frame}

\insertbibliography{TALN03}{*}

\end{document}

