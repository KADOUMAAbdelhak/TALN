% !TEX TS-program = pdflatex
% !TeX program = pdflatex
% !TEX encoding = UTF-8
% !TEX spellcheck = fr

\documentclass[xcolor=table]{beamer}

\input{options}

\title[TALN : 06- Sens et Désambigüisation]%
{Traitement automatique du langage naturel\\Chapitre 06 : Sens des mots et désambigüisation lexicale} 

\changegraphpath{../img/sens/}

\begin{document}
	
\begin{frame}
\frametitle{Traitement automatique du langage naturel}
\framesubtitle{Sens des mots et désambigüisation lexicale : Introduction}

%\begin{exampleblock}{Exemple d'une phrase en français}
%	\begin{center}
%		\Large\bfseries
%		L'élève a écrit une solution avec un stylo. 
%		
%		L'élève a écrit une solution avec une explication.
%	\end{center}
%\end{exampleblock}

\begin{itemize}
	\item 
\end{itemize}

\end{frame}

%\begin{frame}
%\frametitle{Traitement automatique du langage naturel}
%\framesubtitle{Sens des mots et désambigüisation lexicale : Un peu d'humour}
%
%\begin{center}
%	\vgraphpage{humour-parse.jpg}
%\end{center}
%
%\end{frame}

\begin{frame}
\frametitle{Traitement automatique du langage naturel}
\framesubtitle{Sens des mots et désambigüisation lexicale : Plan}

\begin{multicols}{2}
%	\small
\tableofcontents
\end{multicols}
\end{frame}

%===================================================================================
\section{Représentation vectorielle des mots}
%===================================================================================

\begin{frame}
\frametitle{Sens des mots et désambigüisation lexicale}
\framesubtitle{Représentation vectorielle des mots}

\begin{itemize}
	\item Un mot peut être représenté en utilisant l'encodage \keyword{One-Hot} 
	\begin{itemize}
		\item Ne représente pas les relations sémantiques entre les mots : similarité (Ex. \expword{chat, chien}) et proximité (Ex. \expword{café, tasse})
		\item Comment représenter un document/phrase en utilisant cette représentation ?
	\end{itemize}

	\item \optword{Terme-document}
	\begin{itemize}
		\item On représente un terme par les documents le contiennent (ou l'inverse)
		\item Ex. \expword{Term frequency - innverse document frequency (Tf-Idf)}
	\end{itemize}

	\item \optword{Document-terme}
	\begin{itemize}
		\item On représente un terme par d'autres termes
	\end{itemize}

	\item \optword{Terme-concept-document}
	\begin{itemize}
		\item On représente les termes et les documents par un vecteur de concepts
		\item Ex. \expword{Analyse sémantique latente (LSA)}
		\item Bienvenu dans \keyword{le plongement lexical (word embedding)}
	\end{itemize}

\end{itemize}

\end{frame}


\subsection{Tf-Idf}

\begin{frame}
\frametitle{Sens et désambigüisation : Représentation vectorielle}
\framesubtitle{Tf-idf}

\begin{itemize}
	\item Le sens d'un document/phrase peut être représenté par ces mots 
	\item La fréquence d'un mot dans un document/phrase s'appelle \keywords{term frequency (TF)}
	\item Il y a des mots qui se répètent beaucoup, mais qui n'ont pas de sens ajouté (Ex. \expword{
	Prépositions})
	\item Les mots qui apparient dans plusieurs documents sont moins importants
\end{itemize}

\begin{block}{Calcul de Tf-Idf}
	\[
	Tf_d(t) =  |\{t_i \in d / t_i = t\}|
	\hskip2cm 
	Df_D(t) = |\{d \in D / t \in d\}|
	\]
	\[Idf_D(t) = \log_{10} \left( \frac{|\{d \in D\}|}{Df_D(t)} \right)\]
	\[Tf\text{-}Idf_{d, D}(t) = Tf_d(t) * Idf_D(t)\]
\end{block}

\end{frame}


\begin{frame}
\frametitle{Sens et désambigüisation : Représentation vectorielle}
\framesubtitle{Tf-idf : Exemple d'une représentation avec Tf}

\begin{exampleblock}{Exemple d'une collection de phrases}
	\begin{itemize}
		\item S1 : un ordinateur peut vous aider
		\item S2 : il peut vous aider et il veut vous aider
		\item S3 : il veut un ordinateur et un ordinateur pour vous
	\end{itemize}
\end{exampleblock}

\begin{center}
	\begin{tabular}{llllllllll}
	\hline\hline
	& un & ordinateur & peut & vous & aider & il & et & veut & pour \\
	\hline
	S1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0\\
	S2 & 0 & 0 & 1 & 2 & 2 & 2 & 1 & 1 & 0\\
	S3 & 2 & 2 & 0 & 1 & 0 & 1 & 1 & 1 & 1\\
	\hline\hline
\end{tabular}
\end{center}

\end{frame}


\begin{frame}
\frametitle{Sens et désambigüisation : Représentation vectorielle}
\framesubtitle{Tf-idf : Similarité cosinus}

\begin{minipage}{.68\textwidth}
\begin{itemize}
	\item $a$ et $b$ sont deux documents représentés par deux vecteurs $\overrightarrow{a}$ et $\overrightarrow{b}$ respectivement
	\item La représentation peut être les mots du vocabulaire (Tf ou Tf-Idf) ou d'autres caractéristiques du document
	\item La longueur des vecteurs est de $n$
\end{itemize}
\end{minipage}
\begin{minipage}{.3\textwidth}
\hgraphpage{exp-cos.pdf}
\end{minipage}

\begin{block}{Calcul de similarité cosinus entre deux vecteurs}
	\[
	Cos(\theta) = \frac{\overrightarrow{a} \overrightarrow{b}}{||\overrightarrow{a}||\, ||\overrightarrow{b}||}
	= \frac{\sum_{i=1}^{n} a_i b_i}{\sqrt{\sum_{i=1}^{n} a_i^2} \sqrt{\sum_{i=1}^{n} b_i^2}}
	\]
\end{block}

\end{frame}

\subsection{Mot-Mot}

\begin{frame}
\frametitle{Sens et désambigüisation : Représentation vectorielle}
\framesubtitle{Mot-Mot}

\begin{itemize}
	\item On peut représenter un mot par rapport aux autres mot du vocabulaire en utilisant la co-occurrence
	\item Pour représenter les mots d'un vocabulaire $ V $, on doit utiliser une matrice $|V| \times |V|$
	\item Chaque mot est représenté par $|V|$ mots appelés \keyword{le contexte}
	\item La co-occurrence peut être calculée par rapport aux documents, aux phrases ou des fenêtres auteur du mot
	\item La fenêtre peut être 4 mots avant et 4 mots après
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Sens et désambigüisation : Représentation vectorielle}
\framesubtitle{Mot-Mot : Exemple avec un fenêtre de 1-1}

\begin{exampleblock}{Exemple d'une collection de phrases}
	\begin{itemize}
		\item S1 : un ordinateur peut vous aider
		\item S2 : il peut vous aider et il veut vous aider
		\item S3 : il veut un ordinateur et un ordinateur pour vous
	\end{itemize}
\end{exampleblock}

\begin{center}
	\scriptsize
	\begin{tabular}{llllllllll}
		\hline\hline
		& un & ordinateur & peut & vous & aider & il & et & veut & pour \\
		\hline
		un & 0 & 3 & 0 & 0 & 0 & 0 & 1 & 1 & 0\\
		ordinateur & 3 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 1\\
		peut & 0 & 1 & 0 & 2 & 0 & 1 & 0 & 0 & 0\\
		vous & 0 & 0 & 2 & 0 & 3 & 0 & 0 & 1 & 1\\
		aider & 0 & 0 & 0 & 3 & 0 & 0 & 1 & 0 & 0\\
		il & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 2 & 0\\
		et & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0\\
		veut & 1 & 0 & 0 & 1 & 0 & 2 & 0 & 0 & 0\\
		pour & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\
		\hline\hline
	\end{tabular}
\end{center}

\end{frame}

\begin{frame}
\frametitle{Sens et désambigüisation : Représentation vectorielle}
\framesubtitle{Mot-Mot : Similarité cosinus}

\begin{itemize}
	\item On peut calculer la similarité entre deux mots
	\item Une mesure de similarité est cosinus (vue précédemment)
\end{itemize}

\hgraphpage{exp-word-v1_.pdf}

\begin{minipage}{.58\textwidth}
	\begin{figure}
		\caption{Un exemple des vecteurs de co-occurrence à partir de Wikipedia et une visualisation de deux mots \cite{2019-jurafsky-martin} }
	\end{figure}
\end{minipage}
\begin{minipage}{.4\textwidth}
	\hgraphpage{exp-word-v2_.pdf}
\end{minipage}

\end{frame}

\subsection{Analyse sémantique latente (LSA)}

\begin{frame}
\frametitle{Sens et désambigüisation : Représentation vectorielle}
\framesubtitle{Analyse sémantique latente (LSA)}

\begin{itemize}
	\item La matrice terme-document $X[N, M]$ 
	\begin{itemize}
		\item dimension trop grande
		\item beaucoup de zéros (voir l'exemple de Tf-Idf)
	\end{itemize}
	\item On peut utiliser $L$ concepts pour représenter
	\begin{itemize}
		\item les termes $T[N, L]$
		\item les documents $D[M, L]$
	\end{itemize}
	\item $L \le \min(N, M)$
	\item $X$ peut être décomposée en utilisant \keyword{décomposition en valeurs singulières (SVD)}
	\begin{itemize}
		\item $X = T \times S \times D^\top$
	\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}
	\frametitle{Sens et désambigüisation : Représentation vectorielle}
	\framesubtitle{Analyse sémantique latente (LSA) : Formulation SVD}
	
	\begin{itemize}
		\item $T^\top T = \mathbb{I}_{N \times N}$ 
		\item $D^\top D = \mathbb{I}_{M \times M}$ 
		\item SVD peut être résolue en utilisant par exemple l'\keyword{algorithme de Lanczos}, la \keyword{décomposition QR}
	\end{itemize}
	
	\begin{block}{Décomposition en valeurs singulières (SVD) de la matrice terme-document}
		\scriptsize\bfseries
		\[
		\overbrace{
			\begin{bmatrix}
			x_{11} & \ldots & \ldots & \ldots & x_{1M} \\ 
			\vdots & \ddots & \ddots & \ddots &\vdots \\
			\vdots & \ddots & \ddots & \ddots &\vdots \\
			x_{N1} & \ldots & \ldots & \ldots & x_{NM} \\ 
			\end{bmatrix}
		}^{X \text{ : terme-document}}
		=
		\overbrace{
			\left[
			\begin{bmatrix}
			t_{11} \\ 
			\vdots \\
			\vdots \\
			t_{N1} \\ 
			\end{bmatrix}
			\begin{matrix}
			\ldots \\ 
			\end{matrix}
			\begin{bmatrix}
			t_{L1} \\ 
			\vdots \\
			\vdots \\
			t_{L1} \\ 
			\end{bmatrix}
			\right]
		}^{T \text{ : terme-concept}}
		\times 
		\overbrace{
			\begin{bmatrix}
			s_{11} & \ldots & 0 \\
			0 & \ddots & 0 \\
			0 & \ldots & s_{LL} \\
			\end{bmatrix}
		}^{S \text{ : concept-concept}}
		\times 
		\overbrace{
			\begin{bmatrix}
			\begin{bmatrix}
			d_{11} & \ldots & \ldots & \ldots & d_{1M} \\
			\end{bmatrix}\\
			\vdots \\
			\begin{bmatrix}
			d_{11} & \ldots & \ldots & \ldots & d_{1M} \\
			\end{bmatrix}\\
			\end{bmatrix}
		}^{D^\top \text{ : concept-document}}
		\]
		
	\end{block}
	
\end{frame}


\subsection{Évaluation des modèles}

\begin{frame}
\frametitle{Sens et désambigüisation : Représentation vectorielle}
\framesubtitle{Évaluation des modèles}
%TODO compléter l'évaluation
\begin{itemize}
	\item 
\end{itemize}
	
\end{frame}


%===================================================================================
\section{Word embedding}
%===================================================================================

\begin{frame}
\frametitle{Sens des mots et désambigüisation lexicale}
\framesubtitle{Word embedding}

\begin{itemize}
	\item king - man + woman = queen
\end{itemize}

\end{frame}


\subsection{Word2vec}

\begin{frame}
\frametitle{Sens et désambigüisation : Word embedding}
\framesubtitle{Word2vec}

\begin{itemize}
	\item Dans le modèle Mot-Mot, nous avons vu la notion du contexte 
	\begin{itemize}
		\item Un mot est représenté par les mots qui les entourent
		\item Un vecteur de fréquence de co-occurrence
		\item La longueur du vecteur est la taille du vocabulaire (trop large)
		\item La co-occurrence d'un mot avec un petit sous-ensemble de mots (beaucoup de zéros)
	\end{itemize}
	\item Word2vec est un outil fourni par Google implémentant deux méthodes de \keyword{Word embedding} \cite{2013-mikolov-al}
	\begin{itemize}
		\item \optword{CBOW} : Continuous Bag-of-Words
		\item \optword{Skip-gram} : Continuous Skip-gram
	\end{itemize}
	\item Encoder les mots sur un petit vecteur (50-1000) en se basant sur le contexte (en utilisant un encodeur-décodeur)
\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Sens et désambigüisation : Word embedding}
	\framesubtitle{Word2vec : CBOW}
\begin{minipage}{.58\textwidth}
	\begin{itemize}
		\item Le contexte ici est deux mots avant et deux après
		\item Inférer le mot $w_i$ étant donné son contexte
	\end{itemize}
	\begin{block}{Fonction du coût à minimiser}
		\[%
		J(\theta) = \frac{-1}{V} \sum_{i=1}^{V} p(w_i |w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2})
		\]
	\end{block}
\end{minipage}
\begin{minipage}{.08\textwidth}
\end{minipage}	
\begin{minipage}{.4\textwidth}
	\hgraphpage{word2vec-cbow.pdf}
\end{minipage}
	
\end{frame}

\begin{frame}
	\frametitle{Sens et désambigüisation : Word embedding}
	\framesubtitle{Word2vec : Skip-gram}
	\begin{minipage}{.58\textwidth}
		\begin{itemize}
			\item Le contexte ici est deux mots avant et deux après
			\item Inférer le contexte étant donné le mot $w_i$
		\end{itemize}
		\begin{block}{Fonction du coût à minimiser}
			\[%
			J(\theta) = \frac{-1}{V} \sum_{i=1}^{V} \sum_{j= i-2; j \ne i}^{i+2} p(w_j |w_i)
			\]
		\end{block}
	\end{minipage}
	\begin{minipage}{.08\textwidth}
	\end{minipage}
	\begin{minipage}{.4\textwidth}
		\hgraphpage{word2vec-skip.pdf}
	\end{minipage}
	
\end{frame}

\subsection{GloVe}

\begin{frame}
\frametitle{Sens et désambigüisation : Word embedding}
\framesubtitle{GloVe}

\begin{itemize}
	\item \keyword{GloVe} : Global Vectors
	\item une méthode développée par Standford \cite{2014-pennington-al}
	\item essaye d'exploiter les deux approches : matrice mot-mot (comme LSA) et apprentissage par contexte (comme CBOW)
	\item $X[V, V]$ est la matrice Mot-Mot où $X_{ij}$ est le nombre des occurrence du mot $w_j$ dans le contexte de $w_i$, et $X_i$ est le nombre d'occurrences de $w_i$ dans le corpus
	\item $P_ij= \frac{X_{ij}}{X_i}$ est la probabilité d'occurrence de $w_j$ dans le contexte de $w_i$
\end{itemize}

%\hgraphpage[.5\textwidth]{exp-glove_.pdf}

\end{frame}

\begin{frame}
\frametitle{Sens et désambigüisation : Word embedding}
\framesubtitle{GloVe : Motivation}
	
\begin{minipage}{.5\textwidth}
	\begin{itemize}
		\item On veut trouver la relation entre $w_i = ice$ et $w_j = steam$
		\item Ceci peut être représentée par un vecteur des mots $w_j$ 
		\item On calcule le ratio $\frac{P_ik}{Pjk}$
	\end{itemize}
\end{minipage}
\begin{minipage}{.48\textwidth}
	\begin{figure}
		\hgraphpage{exp-glove_.pdf}
		\caption{Exemple de probabilités et un ratio \cite{2014-pennington-al}}
	\end{figure}
\end{minipage}
	
\begin{itemize}
	\item la valeur du ratio doit suivre les relations $R(w_k, w_i)$ et $R(w_k, w_j)$
	\begin{itemize}
		\item Si $R(w_k, w_i) \wedge \neg R(w_k, w_j)$, le ratio sera grand. Ex. \expword{solid}
		\item Si $\neg R(w_k, w_i) \wedge R(w_k, w_j)$, le ratio sera petit. Ex. \expword{gas}
		\item Si $R(w_k, w_i) \wedge R(w_k, w_j)$, le ratio tend vers $1$ . Ex. \expword{water}
		\item Si $\neg R(w_k, w_i) \wedge \neg R(w_k, w_j)$, le ratio tend vers $1$ . Ex. \expword{fashion}
	\end{itemize}
	\item On veut entraîner une fonction $F$ sur les représentations de $w_i$, $w_j$ et $\tilde{w_k}$
	\vspace{-6pt}\[F(w_i, w_j, \tilde{w_k}) = \frac{P_ik}{P_jk}\]
\end{itemize}
	
\end{frame}

\begin{frame}
\frametitle{Sens et désambigüisation : Word embedding}
\framesubtitle{GloVe : Formulation (raisonnement)}
	
\begin{itemize}
	\item On peut restreindre la fonction par utilisation d'une soustraction
	\vspace{-6pt}\[F(w_i - w_j, \tilde{w_k}) = \frac{P_ik}{P_jk}\]
	
	\item On peut transformer l'argument à un scalaire comme le résultat
	\vspace{-6pt}\[F((w_i - w_j)^\top \tilde{w_k}) = \frac{P_ik}{P_jk}\]
	
	\item On veut que $w \leftrightarrow \tilde{w}$ et $X \leftrightarrow X^\top$. Donc, $F$ doit être un homomorphisme entre $(\mathbb{R}, +)$ et $(\mathbb{R}_{>0}, \times)$
	\vspace{-6pt}\[F((w_i - w_j)^\top \tilde{w_k}) = \frac{F(w_i^\top \tilde{w_k})}{F(w_j^\top \tilde{w_k})}\]
	
	\item Donc, $F(w_i^\top \tilde{w_k}) = P_ik = \frac{X_{ik}}{X_i}$
	
	\item La solution est $F = exp$. Donc $w_i^\top \tilde{w_k} = \log X_{ik} - \log X_i$
\end{itemize}
	
\end{frame}

\begin{frame}
\frametitle{Sens et désambigüisation : Word embedding}
\framesubtitle{GloVe : Formulation (version finale)}
	
\begin{itemize}
	\item la valeur $log X_i$ est indépendante de $k$. Donc, on peut entraîner un biais $b_i$ sur $w_i$. Pour respecter la symétrie, il faut entraîner un biais $\tilde{b_k}$ sur $\tilde{w_k}$ 
	\item La fonction objective $J$ utilisée est \keyword{la méthode des moindres carrés}
	\begin{itemize}
		\item le problème est que $J$ ne doit pas pondérer les co-occurrences de la même façon : les co-occurrences rares doivent avoir moins d'impact sur $J$
		\item un solution est de pondérer $J$ avec une fonction sur $X_{ij}$
	\end{itemize}
\end{itemize}

\vspace{-6pt}
\begin{block}{Formulation de la méthode GloVe}\vspace{-6pt}
	\[w_i^\top \tilde{w_j} + b_i + \tilde{b_j} = \log X_{ij} \]
	\[J(\theta) = \sum_{i=1}^{V} \sum_{j=1}^{V} f(X_{ij}) (w_i^\top \tilde{w_j} + b_i + \tilde{b_j} - \log X_{ij})^2\]
	\vspace{-12pt}\[f(x) = \begin{cases}
	\frac{x}{x_{max}} & \text{si } x < x_{max} \\
	1 & \text{ sinon}
	\end{cases}\]
\vspace{-6pt}\end{block}
	
\end{frame}

\begin{frame}
\frametitle{Sens et désambigüisation : Word embedding}
\framesubtitle{GloVe : Architecture}
	
\begin{center}
	\vgraphpage{glove.pdf}
\end{center}
	
\end{frame}

\subsection{BERT}

\begin{frame}
\frametitle{Sens des mots et désambigüisation lexicale : Word embedding}
\framesubtitle{BERT}

\begin{itemize}
	\item 
\end{itemize}

\end{frame}

\subsection{ELMo}

\begin{frame}
\frametitle{Sens des mots et désambigüisation lexicale : Word embedding}
\framesubtitle{ELMo}

\begin{itemize}
	\item 
\end{itemize}

\end{frame}

%===================================================================================
\section{Bases de donnée lexicales}
%===================================================================================

\begin{frame}
\frametitle{Sens des mots et désambigüisation lexicale}
\framesubtitle{Bases de donnée lexicales}

\begin{itemize}
	\item 	
\end{itemize}

\end{frame}

\subsection{Relations sémantiques}

\begin{frame}
\frametitle{Sens des mots et désambigüisation lexicale : BD lexicales}
\framesubtitle{Relations sémantiques}

\begin{itemize}
	\item 
\end{itemize}

\end{frame}

\subsection{Wordnet}

\begin{frame}
\frametitle{Sens des mots et désambigüisation lexicale : BD lexicales}
\framesubtitle{Wordnet}

\begin{itemize}
	\item 
\end{itemize}

\end{frame}

\subsection{Autres ressources}

\begin{frame}
\frametitle{Sens des mots et désambigüisation lexicale : BD lexicales}
\framesubtitle{Autres ressources}

\begin{itemize}
	\item BabelNet, FrameNet
\end{itemize}

\end{frame}

%===================================================================================
\section{Désambigüisation lexicale}
%===================================================================================

\begin{frame}
\frametitle{Sens des mots et désambigüisation lexicale}
\framesubtitle{Désambigüisation lexicale}

\begin{itemize}
	\item 	
\end{itemize}

\end{frame}

\subsection{Algorithme de Lesk}

\begin{frame}
\frametitle{Sens des mots et désambigüisation lexicale : Désambigüisation}
\framesubtitle{Algorithme de Lesk}

\begin{itemize}
	\item 
\end{itemize}

\end{frame}

\subsection{Contextual Embeddings}

\begin{frame}
\frametitle{Sens des mots et désambigüisation lexicale : Désambigüisation}
\framesubtitle{Contextual Embeddings}

\begin{itemize}
	\item 
\end{itemize}

\end{frame}

\insertbibliography{TALN06}{*}

\end{document}

