
@book{2019-jurafsky-martin,
	author = {Jurafsky, Dan and Martin, James H.},
	title = {Speech and Language Processing},
	year = {2019},
	note = {\url{https://web.stanford.edu/~jurafsky/slp3/}},
}

@book{2018-eisenstein,
	author = {Jacob Eisenstein},
	title = {Natural Language Processing},
	year = {2018},
	note = {\url{https://web.stanford.edu/~jurafsky/slp3/}},
}


@ARTICLE{1990-deerwester-al,
	author = {Scott Deerwester and Susan T. Dumais and George W. Furnas and Thomas K. Landauer and Richard Harshman},
	title = {Indexing by latent semantic analysis},
	journal = {JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE},
	year = {1990},
	volume = {41},
	number = {6},
	pages = {391--407}
}

@misc{2013-mikolov-al,
	title={Efficient Estimation of Word Representations in Vector Space},
	author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
	year={2013},
	note={URL: \url{https://arxiv.org/abs/1301.3781}},
	eprint={1301.3781},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{2014-pennington-al,
	title = "{G}lo{V}e: Global Vectors for Word Representation",
	author = "Pennington, Jeffrey  and
	Socher, Richard  and
	Manning, Christopher",
	booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
	month = oct,
	year = "2014",
	address = "Doha, Qatar",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/D14-1162",
	doi = "10.3115/v1/D14-1162",
	pages = "1532--1543",
}

@inproceedings{2018-peters-al,
	title = "Deep Contextualized Word Representations",
	author = "Peters, Matthew  and
	Neumann, Mark  and
	Iyyer, Mohit  and
	Gardner, Matt  and
	Clark, Christopher  and
	Lee, Kenton  and
	Zettlemoyer, Luke",
	booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
	month = jun,
	year = "2018",
	address = "New Orleans, Louisiana",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N18-1202",
	doi = "10.18653/v1/N18-1202",
	pages = "2227--2237",
	abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@misc{2015-kim-al,
	title={Character-Aware Neural Language Models},
	author={Yoon Kim and Yacine Jernite and David Sontag and Alexander M. Rush},
	year={2015},
	eprint={1508.06615},
	note = {URL :  \url{https://arxiv.org/abs/1508.06615}},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{2002-finkelstein-al,
	title = {Placing Search in Context: The Concept Revisited},
	year = {2002},
	author={Lev Finkelstein and Evgeniy Gabrilovich and Yossi Matias and Ehud Rivlin and Zach Solan and Gadi Wolfman and Eytan Ruppin},
	issue_date = {January 2002},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {20},
	number = {1},
	issn = {1046-8188},
	url = {https://doi.org/10.1145/503104.503110},
	doi = {10.1145/503104.503110},
	journal = {ACM Trans. Inf. Syst.},
	month = jan,
	pages = {116â€“131},
	numpages = {16},
	keywords = {invisible web, statistical natural language processing, Search, context, semantic processing}
}

@article{2015-hill-al,
	title = "{S}im{L}ex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation",
	author = "Hill, Felix  and
	Reichart, Roi  and
	Korhonen, Anna",
	journal = "Computational Linguistics",
	volume = "41",
	number = "4",
	month = dec,
	year = "2015",
	url = "https://www.aclweb.org/anthology/J15-4004",
	doi = "10.1162/COLI_a_00237",
	pages = "665--695",
}

@inproceedings{2013-mikolov-al2,
	title = "Linguistic Regularities in Continuous Space Word Representations",
	author = "Mikolov, Tomas  and
	Yih, Wen-tau  and
	Zweig, Geoffrey",
	booktitle = "Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2013",
	address = "Atlanta, Georgia",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N13-1090",
	pages = "746--751",
}

@article {2017-caliskan-al,
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	number = {6334},
	pages = {183--186},
	year = {2017},
	doi = {10.1126/science.aal4230},
	publisher = {American Association for the Advancement of Science},
	abstract = {AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs{\textemdash}for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.Science, this issue p. 183; see also p. 133Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/356/6334/183},
	eprint = {https://science.sciencemag.org/content/356/6334/183.full.pdf},
	journal = {Science}
}