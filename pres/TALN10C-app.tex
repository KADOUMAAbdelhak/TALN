% !TEX TS-program = pdflatex
% !TeX program = pdflatex
% !TEX encoding = UTF-8
% !TEX spellcheck = fr

\documentclass[xcolor=table]{beamer}

\input{options}

\title[TALN : 10- Quelques applications]%
{Traitement automatique du langage naturel\\Chapitre 10 : Quelques applications} 

\changegraphpath{../img/app/}

\begin{document}
	
\begin{frame}
\frametitle{Traitement automatique du langage naturel}
\framesubtitle{Quelques applications : Introduction}

	\begin{itemize}
		\item On veut présenter quelques applications utilisées dans notre vie
		\item Selon l'entrée/sortie : on peut traiter des paroles ou du texte
		\item Selon l'interactivité : le système interagit avec l'utilisateur ou non
		\item Selon la sortie : un ensemble de classes ou un autre texte généré
		\item Notre structure 
		\begin{itemize}
			\item Transformation
			\item Interaction
			\item Classification
			\item Voix
		\end{itemize}
	\end{itemize}

\end{frame}

%\begin{frame}
%\frametitle{Traitement automatique du langage naturel}
%\framesubtitle{Cohérence du discours :  Un peu d'humour}
%
%\begin{center}
%	\vgraphpage{humour1.jpg}
%\end{center}
%
%\end{frame}

\begin{frame}
\frametitle{Traitement automatique du langage naturel}
\framesubtitle{Quelques applications : Plan}

\begin{multicols}{2}
%	\small
\tableofcontents
\end{multicols}
\end{frame}

%===================================================================================
\section{Transformation}
%===================================================================================

\begin{frame}
	\frametitle{Quelques applications}
	\framesubtitle{Transformation}
	\begin{itemize}
		\item Entrée : un texte 
		\item Sortie : un texte généré à partir de l'entrée
		\item Applications 
		\begin{itemize}
			\item Traduction automatique
			\item Résumé automatique
		\end{itemize}
	\end{itemize}
\end{frame}

\subsection{Traduction automatique}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approches}
%	\begin{itemize}
%		\item \optword{Approche directe}
%		\item \optword{à base des règles} (RBMT: rule-based machine translation)
%		\begin{itemize} 
%			\item Approche par transfert
%			\item Approche Interlangue
%		\end{itemize}
%	
%		\item \optword{à base du corpus}
%		\begin{itemize}
%			\item \optword{statistique} (SMT: statistical machine translation)
%			\item \optword{neuronale} (NMT: neural machine translation)
%		\end{itemize}
%	
%	\end{itemize}
	\hgraphpage{translation-classif.pdf}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche directe}
	\begin{itemize}
		\item Algorithme
		\begin{itemize}
			\item Le texte source $S$ est traité comme une série de mots 
			\item Chaque mot $S_i$ est remplacé par un mot $T_i$ dans le texte destinataire $T$ en utilisant un dictionnaire bilingue
			\item Les mots sont ordonnés. Ex. \expword{SVO \textrightarrow VSO}, \expword{adj + N \textrightarrow N + Adj}
		\end{itemize}
		\item Préconditions 
		\begin{itemize}
			\item Les langues (source et destinataire) doivent être proches (structures grammaticales proches)
			\item Un dictionnaire bilingue bien conçu
			\item Des outils d'analyse morphologiques 
		\end{itemize}
		\item Niveau morphologique 
		\item Ex. \expword{Systèmes avant 1967 : Météo, Weidner, CULT et Systran (premières versions)}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche par transfert}
	\begin{itemize}
		\item Algorithme
		\begin{itemize}
			\item Trouver l'arbre syntaxique de $S$
			\item Chercher la traduction des mots de la langue source dans la langue destinataire (dictionnaire bilingue)
			\item Appliquer des règles pour transformer l'arbre syntaxique de la langue source vers une arbre syntaxique de la langue destinataire
			\item Générer le texte traduit $T$ à partir de cette arbre syntaxique
		\end{itemize}
		\item Préconditions 
		\begin{itemize}
			\item Grammaires des deux langues (source et destinataire)
			\item Les règles de transfert sont fixées manuellement ou apprises à partir d'un corpus
			\item Un dictionnaire bilingue bien conçu
			\item Des outils d'analyse morphologiques et syntaxique pour la langue source
			\item Un outil de génération du texte à partir de l'arbre syntaxique
		\end{itemize}
		\item Niveau syntaxique 
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche par transfert (Exemple de règles de transfert)}
	
	\begin{figure}
		\centering
		\vgraphpage[.7\textheight]{MT-tranfert-exp_.pdf}
		\caption{Un exemple de règles de transfert syntaxique \cite{06-quah}}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche par transfert (Exemple : Apertium \cite{11-forcada-al})}
	
	\begin{itemize}
		\item \url{https://www.apertium.org/}
	\end{itemize}

	\hgraphpage{apertium-arch_.pdf}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche interlangue}
	\hgraphpage{MT-Interlingua.pdf}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche interlangue (Description)}
	\begin{itemize}
		\item Algorithme
		\begin{itemize}
			\item Analyser le texte source $S$ pour avoir un arbre syntaxique
			\item Utiliser un dictionnaire entre le langage source et les concepts de l'interlangue 
			\item Transformer l'arbre syntaxique (langue source) vers l'interlingue
			\item Transformer l'interlingue vers une arbre syntaxique (langue destinataire)
			\item Utiliser un dictionnaire entre le langage destinataire et les concepts de l'interlangue 
			\item Générer le texte destinataire $T$
		\end{itemize}
		\item Préconditions 
		\begin{itemize}
			\item Outils d'analyse syntaxique et même sémantique  pour les langues
			\item Un langage de représentation universelle
			\item Dictionnaire sur les concepts de ce langage
		\end{itemize}
		\item Niveau syntaxique/sémantique
	\end{itemize}
\end{frame}


\begin{frame}[fragile]
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche interlangue (Exemple KANT \cite{98-czuba-al})}
\begin{columns}
\begin{column}{0.5\textwidth}
\fontsize{4}{5}\selectfont\bfseries
\begin{verbatim}
(*A-REMAIN  ; action rep for ’remain’
   (FORM FINITE)
   (TENSE PAST)
   (MOOD DECLARATIVE)
   (PUNCTUATION PERIOD)
   (IMPERSONAL -) ; passive + expletive subject
   (ARGUMENT-CLASS THEME+PREDICATE) ; predicate argument structure
   (Q-MODIFIER ; PP semrole (generic)
      (*K-DURING ; PP interlingua
         (POSITION FINAL) ; clue for translation
            (OBJECT ; PP object semrole
               (*O-TIME ; object rep for ’time’
                  (UNIT -)
                  (NUMBER SINGULAR)
                  (REFERENCE DEFINITE)
                  (DISTANCE NEAR)
                  (PERSON THIRD)))))
   (THEME ; object semrole
      (*O-DEFAULT-RATE ; object rep for ’default rate’
         (PERSON THIRD)
         (UNIT -)
         (NUMBER SINGULAR)
         (REFERENCE DEFINITE)))
   (PREDICATE ; adjective phrase semrole
      (*P-CLOSE ; property rep for ’closer’
         (DEGREE POSITIVE)
         (Q-MODIFIER
            (*K-TO
               (OBJECT
                  (*O-ZERO
                     (UNIT -)
                     (NUMBER SINGULAR)
                     (REFERENCE NO-REFERENCE)
                     (PERSON THIRD))))))))
\end{verbatim}
\end{column}
\begin{column}{0.5\textwidth}
	\begin{figure}
		\caption{Représentation de la phrase "\textit{The default rate remained close to zero during this time.}" avec KANT interlingua \cite{98-czuba-al}}
	\end{figure}
\end{column}
\end{columns}


\end{frame}

\begin{frame}[fragile]
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche interlangue (Exemple système KANTOO \cite{00-nyberg-al})}
	
	\hgraphpage{kantoo-arch.pdf}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche statistique}
	\[
	p(T|S) = \frac{p(T) p(S|T)}{p(S)} \propto \underbrace{p(T)}_\text{Cohérence} \underbrace{p(S|T)}_\text{Fidélité}
	\]
	\[\hat{T} = \arg\max_{T} p(T) p(S|T)\]
	\[\]
	\begin{itemize}
		\item $p(T)$ est une modèle de langue entrainé sur la langue destinataire
		\begin{itemize}
			\item Il faut un corpus pour la langue destinataire
			\item $p(T) = \prod_{j=1}^m p(t_j|t_{j-N}\ldots t_{j-1})$
		\end{itemize}
		
		\item $p(S|T)$ est le modèle de traduction 
		\begin{itemize}
			\item Il faut un corpus pour la langue destinataire
			\item Le problème de correspondance entre mots (alignement)
			\item Le problème de la différence entre tailles
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche statistique (Alignement)}
	\[p(S|T) = \sum_{A} p(S, A | T)\]
	\[A^* = \arg\max_A p(S, A | T)\]
	\[p(S, A | T) = \prod_{i=1}^{n} p(S_i, A_i | S_1^{i-1}, A_1^{i-1}, T_1^{m})\]
	\[p(S_i, A_i | S_1^{i-1}, A_1^{i-1}, T_1^{m}) = p(A_i | S_1^{i-1}, A_1^{i-1}, T_1^{m}) p(S_i | S_1^{i-1}, A_1^{i}, T_1^{m})\]
	\[ \text{Nombre des alignements possibles } = (m + 1)^n\]
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche statistique (Alignement IBM1)}
	\begin{itemize}
		\item Modèle IBM 1 (Distribution uniforme des mots)
	\end{itemize}

	\[p(A_i | S_1^{i-1}, A_1^{i-1}, T_1^{m}) = \frac{1}{m+1}\]
	\[p(S|T) = \frac{1}{(m+1)^n} \sum_{A} \prod_{i=1}^{n} p(S_i | S_1^{i-1}, A_1^{i}, T_1^{m})\]
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche statistique (Alignement IBM2)}
	\begin{itemize}
		\item Modèle IBM 2
		\item La probabilité des alignements se base seulement sur la position actuelle $i$, le mot actuel $A_i$, la taille du texte source $n$ et la taille du texte destinataire $m$
		\item Cette probabilité est entrainée en comparant l'alignement juste avec le reste des alignements
	\end{itemize}
	
	\[p(A_i | S_1^{i-1}, A_1^{i-1}, T_1^{m}) = p(A_i | i, n, m)\]
	\[p(S|T) = \sum_{A} \prod_{i=1}^{n} p(A_i | i, n, m) p(S_i | S_1^{i-1}, A_1^{i}, T_1^{m})\]
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche statistique (Alignement HMM)}
	\begin{itemize}
		\item Modèle HMM
		\item La probabilité de alignement d'un mot $A_i$ se base sur l'alignement du mot précédent $A_{i-1}$
	\end{itemize}
	
	\[p(A_i | S_1^{i-1}, A_1^{i-1}, T_1^{m}) = p(A_i | A_{i-1}, m)\]
	\[p(S|T) = \sum_{A} \prod_{i=1}^{n} p(A_i | A_{i-1}, m) p(S_i | S_1^{i-1}, A_1^{i}, T_1^{m})\]
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche par exemples}
	\begin{itemize}
		\item Comme l'approche statistique
		\item A la place des mots, on peut trouver des segments
		\item On calcule la probabilité d'un segment (ensemble de mots consécutifs) par rapport à un autre
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche par exemples (Exemple : Moses \cite{07-koehn-al})}
	\begin{itemize}
		\item \url{http://statmt.org/moses/}
		\item Table de traduction des segments $\phi(S|T)$ : segment $S$, segment $T$ équivalent et une probabilité
		\item Modèle de langage sur la langue destinataire $LM$
		\item Modèle de distorsion $ D(T, S) $ : chaque réorganisation des segment d'une phrase veut un coût 
		\item Pénalité de mots $W(T)$ : pour qu'une traduction ne soit pas longue ou courte
		\item Pour estimer $\hat{T}$ \keyword{Beam search} est utilisé
	\end{itemize}

	\[p(T|S) = \phi(S|T)^{poids_{\phi}} \times LM^{poids_{LM}} \times D(T, S)^{poids_{D}} \times W(T)^{poids_{W}}\]
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche neuronale}
	\begin{itemize}
		\item Structure
		\begin{itemize}
			\item Des réseaux de neurones de type sequence-to-sequence (seq2seq)
			\item Modèle : encodeur-décodeur (séquentiel)
			\item Type : Many to Many 
			\item \optword{Encodeur} : encoder une phrase du langage source $S$. Le résultat est une représentation du contexte sous forme d'un vecteur
			\item \optword{Décodeur} : décoder le vecteur du contexte vers une phrase du langage destinataire $T$
		\end{itemize}
	
		\item Modèle formel 
		\[ p(T|S) = p(t_1|S) p(t_2|S, t_1) p(t_3|S, t_1, t_2)\ldots p(t_m|S, t_1\ldots t_{m-1}) \]
		\[\hat{t}_j = \arg\max_{w \in V} p(w | S, t_1\ldots t_{j-1})\]
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche neuronale (Exemple : Google \cite{16-wu-al} )}
	\begin{itemize}
		\item \url{https://translate.google.com/}
	\end{itemize}
	\begin{center}
		\hgraphpage[.9\textwidth]{googlet_.pdf}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Traduction automatique : Approche neuronale (Exemple : OpenNMT \cite{17-klein-al})}
	\begin{itemize}
		\item \url{https://opennmt.net/}
	\end{itemize}
	\hgraphpage{opennmt_.pdf}
\end{frame}

\subsection{Résumé automatique}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Classification d'un résumé automatique}
	\hgraphpage{sum-classif.pdf}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approches du résumé automatique}
	\begin{columns}
		\begin{column}{0.32\textwidth}
			\begin{block}{\scriptsize\bfseries\cite{12-nenkova-mckeown}}
				\begin{itemize}
					\item représentation du sujet 
					:
					mots du sujet,
					fréquences, 
					analyse sémantique latente, 
					modèles de sujets bayésiens,
					clustering
					%			\begin{itemize}
					%				\item mots du sujet 
					%				\item fréquences
					%				\item analyse sémantique latente
					%				\item modèles de sujets bayésiens
					%			\end{itemize}
					\item représentation des indicateurs : 
					par graphes, 
					apprentissage automatique
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.3\textwidth}
			\begin{block}{\scriptsize\bfseries\cite{12-lloret-palomar}}
				\begin{itemize}
					\item statistique 
					\item par graphes
					\item basée discours
					\item par apprentissage automatique
				\end{itemize}
			\end{block}
		\end{column}
		\begin{column}{0.28\textwidth}
			\begin{block}{\scriptsize\bfseries\cite{19-aries-al}}
				\begin{itemize}
					\item statistique 
					\item par graphes
					\item linguistique 
					\item par apprentissage automatique
				\end{itemize}
			\end{block}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche statistique}
	
	\begin{itemize}
		\item \optword{Fréquence des mots} 
		
		\hspace{.5cm}Ex. $Score_\text{TF-IDF}(s_i) = \sqrt{\sum\limits_{w_{ik} \in s_i} (\text{TF-IDF}(w_{ik}))^2}$
		
		\item \optword{Position des phrases (ou des mots)}
		
		\hspace{.5cm}Ex. $ Score_\text{pos}(s_i) = \max (\frac{1}{i}, \frac{1}{|D| - i + 1}) $
		
		\item \optword{Taille (longueur) des phrases}
		
		\hspace{.5cm}Ex. $ Score_\text{taille}(s_i) = \left\lbrace 
		\begin{array}{lll}
		0 & si & (L_i \geq L_{min}) \\
		\frac{L_i - L_{min}}{L_{min}} & sinon & \\
		\end{array}
		\right. $
		
		\item \optword{Mots du titre et des sous-titres}
		
		\hspace{.5cm}Ex. $ Score_{titre}(s_i) = \frac{\sum_{e \in T \bigcap s_i}{\frac{tf(e)}{tf(e)+1}}}
		{\sum_{e \in T}{\frac{tf(e)}{tf(e)+1}}} $
		
		\item \optword{Centroid}, \optword{Frequent itemsets}, \optword{Analyse sémantique latente}, ...
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche statistique (Exemple : \cite{13-aries-al})}
	
	\hgraphpage{tcc-arch.pdf}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche statistique (Exemple: \cite{13-aries-al} - suite)}
	
	\begin{itemize}
		\item Un texte peut contenir plusieurs sujets, et une phrase peut discuter plusieurs sujets
		\begin{itemize}
			\item Regroupement de phrases avec similarité et seuil de regroupement ($Th$)
		\end{itemize}
		\item Une phrase est importante si elle peut représenter le maximum des sujets(cluters)
		\[ Score(s_i , c_j , f_k ) = 1 + \sum_{\phi \in s_i} {P(f_k=\phi | s_i \in c_j)} \]
		\[ Score(s_i , \bigcap_{j} c_j , F) =  %\propto 
		\prod_{j} \prod_{k} Score(s_i , c_j , f_k ) \]
		$ s $ : phrase, $ c $ : cluster, $ f $ : caractéristique, $ F $ : ensemble de caractéristiques, $ \phi $: observation de $ f $.
		\item $f$ : TF (Uni, Bi); Pos (intervalle de 10); Len (réel, pré-traité)
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche statistique (Exemple: \cite{15-oufaida-al})}
	\vspace{-6pt}
	\begin{itemize}
		\item Représentation des mots : embeddings pré-entrainés (Polyglot)
		\item Clustering pour extraire les sous-sujets du texte
		\begin{itemize}
			\item On peut trouver le mot $w_j \in S_2$ le plus similaire à un mot $w_i$
			
			$Match(w_i | S_2) = \arg\max_{w_j \in S_2} sim(Rep(w_i), Rep(w_j))$
			
			\item Lien entre deux phrases $S_2$ et $S_2$
			
			$Sim(S_1, S_2) = \frac{\sum_{w_i \in S_1} Match(w_i | S_2) + \sum_{w_j \in S_2} Match(w_j | S_1)}{|S_1| + |S_2|}$
		\end{itemize}
	
		\item Score des termes en utilisant mRMR
		\begin{itemize}
			\item Information mutuelle  
			$I(X, Y) = \sum\limits_{x \in X} \sum\limits_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}$
			\item Redondance des termes 
			$Redondance(T_i) = \frac{1}{|S|} \sum\limits_{j \in S} I(T_i, T_j)$
			
			\item Score final d'un terme : Mutual Information (Difference/Quotient)
			
			$MID \equiv \max_{t \in T} Pertinence(t) - Redondance(t)$

			$MIQ \equiv \max_{t \in T} Pertinence(t) / Redondance(t)$
		\end{itemize}
	
		\item Score d'un phrase à base des scores des termes mRMR (score pondéré par des poids)
	
		\item Lors de la sélection d'une phrase, on décrémente les poids des termes sélectionnés
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche par graphes}
	
	\begin{itemize}
		\item \optword{Propriétés du graphe}
		\begin{itemize}
			\item Bushy paths 
			
			\hspace{.5cm}$Score_{\#arcs}(s_i) = |\{ s_j : a(s_i, s_j) \in A / s_j \in S, s_i \neq s_j \}|$
			
			\item Aggregate Similarity
			
			\hspace{.5cm}$Score_{aggregate}(s_i) = \sum\limits_{(s_i, s_j) \in E} sim(s_i, s_j)$
		\end{itemize}
		\item \optword{Méthodes itératives}
		\begin{itemize}
			\item Mettre à jour les scores des nœuds à base des voisins 
			\item L'arrêt : un état d'équilibre (on peut plus mettre à jour)
			\item Ex. TextRank \cite{04-mihalcea-tarau}
			
			$WS(V_i) = ( 1 - d) + d * \sum\limits_{V_j \in In(V_i)} \frac{w_{ji}}{\sum\limits_{V_k \in Out(V_j)} w_{jk}} WS(V_j)$
			
			$w_{ij} = \frac{|\{w_k \text{ / } w_k \in S_i \text{ and } w_k \in S_j\}|}{\log(|S_i|) + \log(|S_j|)}$
			
			$ d $ : damping factor (en général, environ $ 0.85 $)
		\end{itemize}
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche par graphes (Exemple: \cite{21-aries-al})}
	
	\begin{center}
		\vgraphpage{gc-archi.pdf}
	\end{center}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche par graphes (Exemple : \cite{21-aries-al} - suite)}
	
	\vspace{-6pt}
	\begin{itemize}
		\item Simplification du graphe
		
		\hspace{.5cm}$noeud\_faible(v_i) = ( \sum_{(v_i, v_j) \in E} w_{ij} < \frac{1}{MImpN(v_i)} )$ 
		
		\hspace{.5cm}$arc\_faible(v_i, v_j) = ( w_{ij} < \frac{Threshold}{MImpN(v_i)})$
		
		\item Score statistique des phrases
		
		\hspace{.5cm}$ Score(s_i/ sim) = sim(s_i, C\backslash s_i) $
		$Score(s_i/ tfisf) = \sqrt{\sum\limits_{w_{ik} \in s_i} (tfisf(w_{ik}))^2}$
		
		\hspace{.5cm}$Score(s_i/ size) = \frac{1}{|s_i|}$
		$Score(s_i/ pos) = \max (\frac{1}{i}, \frac{1}{|D| - i + 1})$
		
		\hspace{.5cm}$SSF(s_i/ F) = \prod_{f_i \in F} score(s_i/f_i)$
		
		\item Score à base de graphe 
		
		\hspace{.5cm}Ex. $GC1(s_i) = SSF(s_i) + \sum\limits_{(s_i, s_j) \in E} sim(s_i, s_j) * SSF(s_j)$
		
		\item Extraction
		
		\hspace{.5cm}Ex. $ suiv_{e4}  =  \arg\min\limits_i (iord\ ssfgc(s_i) + ord\ simil(dernier{e4}, i))$ 
		
		\hspace{3cm}$ \text{ où } (dernier_{e4}, s_i) \in E $
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche linguistique}
	
	\begin{itemize}
		\item \optword{Mots de sujet} : une liste des mots pertinents au sujet, comme ``significant", ``impossible", etc.
		\begin{itemize}
			\item Ex. \cite{69-edmundson} : Bonus (mots positivement pertinents), Stigma (mots négativement pertinents)
			
			$Score_{cue}(s_i) = \sum_{w \in s_i}{cue(w)}
			\text{ où }
			cue(w) = \left\lbrace 
			\begin{array}{ll}
			b > 0 & \text{si } (w \in Bonus) \\
			\delta < 0 & \text{si } (w \in Stigma) \\
			0 & sinon 
			\end{array} 
			\right. $
		\end{itemize}
		\item \optword{Indicateurs} : Des structures qui impliquent que la phrase les contenant a une chose importante à propos du sujet
		\begin{itemize}
			\item Ex. \expword{the principal aim of this paper is to investigate ...}
		\end{itemize}
		\item \optword{Co-référence} : utilisation des anaphores ou des représentations sémantiques (Ex. Wordnet)
		\item \optword{Structure rhétorique} : utilisation de la structure rhétorique pour noter les phrases ou des syntagmes
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche linguistique (Exemple : \cite{81-paice})}
	
	\begin{itemize}
		\item Utilisation des patrons (templates) préparés manuellement
		\item\ [x] : x mots peuvent être entre ce mot et le mot précédent
		\item +y : le score est augment par y
		\item ? : ce mot est optionnel
	\end{itemize}

	\begin{figure}[!ht]
		\begin{center}
			\hgraphpage[.7\textwidth]{paice-template.pdf}
			\caption{Un exemple d'un patron simplifié \cite{81-paice}.}
			\label{fig:paice-template}
		\end{center}
	\end{figure}
	
\end{frame}

%\begin{frame}
%	\frametitle{Quelques applications : Transformation}
%	\framesubtitle{Résumé automatique : Approche linguistique (Exemple : \cite{})}
%	
%	\begin{itemize}
%		\item 
%	\end{itemize}
%	
%\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche par apprentissage automatique (AA)}
	
	\begin{itemize}
		\item \optword{Par caractéristiques}
		\begin{itemize}
			\item Réglage : régler des hyper-paramètres comme les poids des caractéristiques pour le score
			\item Classement : décider si une unité (phrase) appartient au résumé ou non
		\end{itemize}
		\item \optword{Bayesian topic models}
		\begin{itemize}
			\item Identifier les concepts principaux à partir des documents et les liens entre eux pour avoir une hiérarchie
		\end{itemize}
		\item \optword{Deep learning}
		\begin{itemize}
			\item Réglage ou Classement
			\item Génération des mots (une forme du classement)
		\end{itemize}
		\item \optword{Reinforcement learning}
		\begin{itemize}
			\item Utilisation des actions et des récompenses pour entrainer un système à générer des résumés
		\end{itemize}
	\end{itemize}
	
\end{frame}


\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche par AA (Exemple : \cite{2020-aries})}
	
	\begin{center}
		\vgraphpage{ml2es-archi.pdf}
	\end{center}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche par AA (Exemple : \cite{06-daumeiii-marcu})}
	
	\begin{minipage}{.6\textwidth}
		\begin{itemize}
			\item $D$ : un ensemble de $K$ documents
			\item $Q$ : un ensemble de $J$ requêtes
			\item $P^G$ : un modèle de langue général de l'anglais 
			\item $P^Q$ : un modèle de langue des requêtes
			\item $P^D$ : un modèle de langue d'arrière-plan (des documents)
		\end{itemize}
	\end{minipage}
	\begin{minipage}{.38\textwidth}
		\hgraphpage{btm-daumeiii_.pdf}
	\end{minipage}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche par AA (Exemple : \cite{15-rush-al})}
	
	\begin{center}
		\hgraphpage[.35\textwidth]{2015-rush-al_.pdf}
		\hgraphpage[.35\textwidth]{2015-rush-al-exp_.pdf}
	\end{center}
	
	\begin{itemize}
		\item[(a)] Un décodeur qui cherche le prochaine mot du résumé $y_{i+1}$ sachant la phrase en entrée $x$ et des mots déjà générés pour le résumé $y_c \equiv [y_{i-c+1},\ldots, y_i]$
		\item[(b)] Un encodeur avec attention
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Transformation}
	\framesubtitle{Résumé automatique : Approche par AA (Exemple : \cite{18-narayan-al})}
	
	\hgraphpage{narayan-al_.pdf}
	
\end{frame}

%===================================================================================
\section{Interaction}
%===================================================================================

\begin{frame}
	\frametitle{Quelques applications}
	\framesubtitle{Interaction}
	\begin{itemize}
		\item Entrée : une question (texte)
		\item Sortie : une réponse (texte)
		\item Applications 
		\begin{itemize}
			\item Questions/Réponses
			\item Systèmes de dialogue
		\end{itemize}
	\end{itemize}
\end{frame}

\subsection{Questions-Réponses}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Questions-Réponses}
%	\begin{itemize}
%		\item 
%	\end{itemize}
	\hgraphpage{qa-classif.pdf}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Questions-Réponses : Approche par RI}
	
	\begin{figure}
		\hgraphpage{qa-ri_.pdf}
		\caption{Architecture d'un système de questions/réponses \cite{2019-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Questions-Réponses : Approche par RI (traitement de la question)}
	
	\begin{itemize}
		\item Formulation de la requête
		\begin{itemize}
			\item Séparation des mots
			\item Suppression des mots vides
			\item Radicalisation  
		\end{itemize}
	    \item Formulation de la requête
	    \begin{itemize}
	    	\item Détection du type de réponse
	    	\item Personne ? Place ? Organisation ? ...
	    	\item En utilisant des taxonomies comme celle de Wordnet  
	    \end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Questions-Réponses : Approche par RI (recherche)}
	
	\begin{itemize}
		\item Recherche des documents
		\begin{itemize}
			\item En utilisant les mots clés et l'index des documents
			\item Retourner les documents avec plus de score
			\item Diviser ces documents en passages (paragraphes ou phrases)
		\end{itemize}
		\item Recherche des passages
		\begin{itemize}
			\item Appliquer la détection des entités nommées sur les passages
			\item Filtrer les passages qui ne contiennent pas le type de la réponse
			\item On peut appliquer un algorithme d'apprentissage automatique
			\item Cette algorithme peut être utilisé pour noter les passages
			\item Caractéristiques : nombre des entités nommées du type recherché, nombre des termes de la question, la séquence la plus longue similaire à la question, le rang du document, etc.
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Questions-Réponses : Approche par RI (extraction de réponse)}
	
	\begin{itemize}
		\item \optword{Apprentissage par caractéristiques}
		\begin{itemize}
			\item Patrons : \expword{\textless REP\textgreater comme \textless QES\textgreater; }
			\item Type de réponse et celui du syntagme
			\item Les mots clés de la question
			\item Nouveauté : au moins un mot n'existe pas dans la question
			\item Ponctuation
			\item ...
		\end{itemize}
		\item \optword{Réseaux de neurones}
		\begin{itemize}
			\item Tâche de lecture/compréhension
			\item Pour chaque mot, calculer la probabilité d'être le début de la réponse et la fin de la réponse en utilisant la question. 
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Questions-Réponses : Approche par RI (extraction par Bi-LSTM \cite{2017-chen-al})}
	
	\begin{figure}
		\hgraphpage{qa-bilstm-exp_.pdf}
		\caption{Extraction de réponse dans le système DrQA \cite{2019-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Questions-Réponses : Approche par RI (extraction par BERT \cite{2018-devlin-al})}
	
	\begin{figure}
		\centering
		\hgraphpage[.5\textwidth]{qa-bert-exp_.pdf}
		\caption{Extraction de réponse dans le système DrQA \cite{2019-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Questions-Réponses : Approche par connaissance}
	
	\begin{itemize}
		\item \optword{Par graphe}
		\begin{itemize}
			\item Annotation sémantique (Entity linking)
			\item Déterminer la relation recherchée ; Ex. \expword{Place\_naissance}
			\item S'il y a plusieurs relations comme réponse : calculer la similarité entre la question et la réponse
			\item Exemple, \expword{en utilisant les embeddings}
		\end{itemize}
		\item \optword{Par analyse sémantique}
		\begin{itemize}
			\item Analyse sémantique de la question
			\item Génération d'une forme structurée de la question : lambda calcul, SQL, SPARQL, etc.
			\item Cette forme est utilisée comme requête
			\item Récupérer la réponse à partir de la base de connaissance
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Questions-Réponses : Approche par connaissance (Exemple)}
	
	\begin{figure}
		\centering
		\hgraphpage[.9\textwidth]{qa-logfrm-exp_.pdf}
		\caption{An exemple des formes logiques des questions \cite{2020-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Questions-Réponses : Approche par modèles de langues}
	
	\begin{itemize}
		\item Utiliser un modèle de langue pré-entrainé
		\item Régler le modèle pour répondre les questions
	\end{itemize}

	\begin{figure}
		\centering
		\hgraphpage[.6\textwidth]{qa-t5_.pdf}
		\caption{T5 est pré-entraîné pour remplir le texte absent, ensuite réglé pour répondre aux questions sans saisir d'informations ou de contexte supplémentaires. \cite{2020-roberts-al}}
	\end{figure}
	
\end{frame}

\subsection{Systèmes de dialogue}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Systèmes de dialogue}
%	\begin{itemize}
%		\item Systèmes de dialogue ou agents conversationnels
%		\item \textbf{Moyenne} : texte ou parole
%		\item \optword{Systèmes de dialogue orientés tâche}
%		\begin{itemize}
%			\item \textbf{BUT} : aider l'utilisateur à compléter une tâche
%			\item Ex. \expword{Siri, Alexa, Google Now/Home, Cortana, etc.}
%		\end{itemize}
%		\item \optword{Chatbots}
%		\begin{itemize}
%			\item \textbf{BUT} : imiter les conversations entre humains
%			\item \textbf{Méthodes} : à base de règles, à base de RI, à base des encodeurs-décodeurs
%		\end{itemize}
%	\end{itemize}
	\hgraphpage{sd-classif.pdf}
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Systèmes de dialogue : orienté tâche - Frame-based (Frame)}
	
	\begin{itemize}
		\item Frame (Cadre) : une structure contenant des slots à remplir et des questions prédéfinies pour chaque slot
		\item On ne pose que les questions dont le slot est vide
		\item Il y a la possibilité de remplir d'autres slots dans d'autre cadres. 
		Ex. \expword{Le slot RESERVATION\_DATE dans le cadre HOTEL\_RESERVATION à partir du slot ARRIVAL\_DATE du cadre FLIGHT\_RESERVATION}
		\item Appliquer \keyword{Détection d'intention} pour savoir quel cadre à utiliser
	\end{itemize}
	
	\begin{figure}
		\centering
		\hgraphpage[.6\textwidth]{sd-frame-exp_.pdf}
		\caption{Exemple d'un cadre pour programmer un vol. \cite{2020-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Systèmes de dialogue : orienté tâche - Frame-based (Réponses multiples)}
	
	\begin{figure}
		\centering
		\hgraphpage[.6\textwidth]{sd-frame-semgram-exp_.pdf}
		
		\hgraphpage[.8\textwidth]{sd-frame-parse-exp_.pdf}
		\caption{Exemple d'une grammaire sémantique et un arbre sémantique d'une phrase. \cite{2020-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Systèmes de dialogue : orienté tâche - Dialogue-State}
	
	\begin{figure}
		\centering
		\hgraphpage[.7\textwidth]{sd-dialog-arch_.pdf}
		\caption{Architecture d'un système utilisant dialogue-state \cite{2016-williams-al}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Systèmes de dialogue : orienté tâche - Dialogue-State (Actes du dialogue)}
	
	\vspace{-6pt}
	\begin{figure}
		\centering
		\hgraphpage[.5\textwidth]{sd-dialog-act-exp1_.pdf}
		
		\hgraphpage[.5\textwidth]{sd-dialog-act-exp2_.pdf}\vspace{-6pt}
		\caption{Des actes de dialogue ainsi qu'un exemple d'un dialogue d'un système de recommandation des restaurants appelé HIS \cite{2010-young-al}. Il est indiqué ce qui est permété d'être une entrée du système et une sortie de l'utilisateur \cite{2020-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Systèmes de dialogue : orienté tâche - Dialogue-State (Remplissage des slots)}
	
	\begin{itemize}
		\item Classifier la phrase par intention, domaine et slot
		\item Extraire les informations pour remplir les slots. Ex. \expword{En utilisant l'apprentissage automatique pour l'étiquetage des séquences}
	\end{itemize}
	
	\begin{figure}
		\centering
		\hgraphpage[.5\textwidth]{sd-dialog-remp-exp_.pdf}
		\caption{Exemple d'une architecture pour le remplissage des slots en utilisant BERT \cite{2020-jurafsky-martin}}
	\end{figure}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Systèmes de dialogue : orienté tâche - Dialogue-State (Autres composants)}
	
	\begin{itemize}
		\item Traqueur de l'état du dialogue
		\begin{itemize}
			\item Sauvegarder l'état des cadres (slots) et le dernier acte du dialogue
		\end{itemize}
		\item Politique du dialogue
		\begin{itemize}
			\item Déterminer l'action $A_i$ à prendre 
			
			$\hat{A}_i = \arg\max_{A_i \in A} P(A_i | Frame_{i-1}, A_{i-1}, U_{i-1})$
		\end{itemize}
	
		\item Générateur du texte 
		\begin{itemize}
			\item Générer du texte à partir d'un acte de dialogue
			\item On peut entraîner un encodeur/décodeur pour le faire
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Systèmes de dialogue : chatbot - à base des règles (Exemple : ELIZA \cite{1966-Weizenbaum})}
	
	\begin{itemize}
		\item Le plus célèbre chatbot est \keyword{ELIZA} : psychologue
		\item Une liste des patrons/transformations 
		
		\expword{\small [(.*) YOU (.*) ME]\textsubscript{[Patron]} \textrightarrow\ [WHAT MAKES YOU THINK I \$2 YOU?]\textsubscript{[Transformation]}}
		
		\expword{You hate me \textrightarrow\ WHAT MAKES YOU THINK I HATE YOU?}
		
		\item Les patrons sont liés à une liste des mots. Le mot qui score le plus dans la phrase va déclencher plusieurs patrons
		
		\item Parmi les, le patron le plus similaire à la phrase est utilisé
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Systèmes de dialogue : chatbot - à base de la RI}
	
	
\end{frame}

\begin{frame}
	\frametitle{Quelques applications : Interaction}
	\framesubtitle{Systèmes de dialogue : chatbot - à base des encodeurs/décodeurs}
	
	
\end{frame}


%===================================================================================
\section{Classification}
%===================================================================================

\begin{frame}
	\frametitle{Quelques applications}
	\framesubtitle{Classification}
	\begin{itemize}
		\item Filtrage de spams
		\item Identification des langues
		\item Lisibilité 
		\item Analyse des sentiments
		\item Détection de l'humeur
	\end{itemize}
\end{frame}

\subsection{Analyse des sentiments}

\begin{frame}
	\frametitle{Quelques applications : Classification}
	\framesubtitle{Analyse des sentiments}
	\begin{itemize}
		\item 
	\end{itemize}
\end{frame}

\subsection{Lisibilité}

\begin{frame}
	\frametitle{Quelques applications : Classification}
	\framesubtitle{Lisibilité}
	\begin{itemize}
		\item 
	\end{itemize}
\end{frame}


%===================================================================================
\section{Voix}
%===================================================================================

\begin{frame}
	\frametitle{Quelques applications}
	\framesubtitle{Voix}
	\begin{itemize}
		\item 
	\end{itemize}
\end{frame}

\subsection{Reconnaissance de la voix}

\begin{frame}
	\frametitle{Quelques applications : Voix}
	\framesubtitle{Reconnaissance de la voix}
	\begin{itemize}
		\item 
	\end{itemize}
\end{frame}

\subsection{Synthèse de la voix}

\begin{frame}
	\frametitle{Quelques applications : Voix}
	\framesubtitle{Synthèse de la voix}
	\begin{itemize}
		\item 
	\end{itemize}
\end{frame}


\insertbibliography{TALN10}{*}

\end{document}

