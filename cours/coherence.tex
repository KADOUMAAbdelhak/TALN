% !TEX TS-program = xelatex
% !TeX program = xelatex
% !TEX encoding = UTF-8
% !TEX spellcheck = fr

%=====================================================================
\ifx\wholebook\relax\else
	\documentclass{KodeBook}
	\input{calls}
	\begin{document}
		\mainmatter
	
\fi
%=====================================================================
\changegraphpath{../img/coherence/}
\chapter{Cohérence du discours}

\begin{introduction}[LES LANGU\textcolor{white}{E}S]
	\lettrine{E}{n} lisant un texte, nous ne devons pas sentir une interruption dans l'enchaînement des idées.
	On ne doit pas sauter d'une idée à une autre dans le même paragraphe.
	On appelle ça : la cohérence ; un texte cohérent doit garder la même idée ou passer à une autre idée en gardant une sorte de continuité.
	L'analyse de la cohérence nous permet de vérifier qu'un texte est facile à lire et à comprendre. 
	Plusieurs théories ont été proposées pour représenter les relations de cohérence et donc permettre son analyse.
	Dans ce chapitre, on va présenter quelques théories de cohérence et quelques méthodes pour l'analyser.
\end{introduction} 

Prenons le texte ``\expword{L'hiver est un des quatre saisons de l'année. Le rapport était bien exposé. La pluie est une des caractéristiques de cette saison. J'ai lu un rapport sur cette saison.}".
On peut comprendre chaque phrase à part ; mais, il est difficile de comprendre l'idée du texte. 
Il est clair que les phrases sont mal-ordonnées. 
Donc, comprendre les phrases n'est pas suffisant afin de comprendre un discours ; il faut ce dernier soit cohérent.
L'analyse de la cohérence peut aider dans plusieurs tâches :
\begin{itemize}
	\item Génération du texte : lorsqu'on génère un texte automatiquement, il faut que les phrases soient cohérentes. 
	Un exemple plus simple est la génération des résumés automatiques par extraction. 
	Une fois les phrases importantes, on doit réordonner ces phrases pour avoir un résumé cohérent. 
	Un exemple d'une méthode pour ordonner les phrases dans le contexte du résumé automatique est proposé dans \cite{2019-oufaida-al}.
	\item Compréhension du texte : comme l'exemple précédent, on ne peut pas bien comprendre un texte que s'il est cohérent.
	\item Évaluation du texte : on peut utiliser l'analyse de la cohérence afin d'évaluer automatiquement les expressions écrites des enfants. 
	Aussi, on peut l'utiliser comme outil d'aide de rédaction (Personnellement, je n'ai pas vu un tel outil ; mais il va être d'une grande utilité pour les écrivains).
	\item Détection de plagiat : lorsqu'on copie et colle des parties de plusieurs origines, on va avoir un texte incohérent. 
\end{itemize}

%===================================================================================
\section{Relations de cohérence}
%===================================================================================

En général, les phrases proches se partagent quelques mots ou sens. 
Ce phénomène est appelé \optword{Cohésion lexicale} où la relation entre deux phrases consécutives est la similarité lexicale ou sémantique.
Une autre façon de voir la cohérence est en se basant sur le sujet du texte ; les phrases d'un discours forment une seule entité (elles discutent le même sujet). 
On appelle ça : \optword{Cohérence basée sur l'entité} où la relation entre les phrase c'est le sujet. 
Parmi les théories dans cette direction, on peut mentionner : Centering theory et Entity grid model. 
Dans cette section, on va présenter  des relations plus concrètes qui relient les phrases l'une à l'autre. 
Dans ce cas, on parle de la \optword{cohérence basée sur les relations}. 
Deux représentations du discours sont bien connues : \ac{rst} et \ac{pdtb}.

\subsection{Rhetorical Structure Theory (RST)}

\keyword[R]{\ac{rst}} modélise le discours sous formes des relations entre deux unités (des phrases ou parties des phrases) : \optword{Noyau (N)} et \optword{Satellite (S)}. 
Le noyau est une unité indépendante, peut être interprétée indépendamment des autres unités du texte.
Le satellite est une unité dépendante, ne peut être interprétée qu'en utilisant le noyau. 
Chaque relation de cohérence est définie en se basant sur deux acteurs : \optword{Lecteur (L)} ; qui a lit le script et \optword{Scripteur (S)} ;  qui a rédigé le script. 
Elle fournit des restrictions sur le noyau, sur le satellite et/ou sur les deux. 
L'effet de la relation peut prendre lieu dans un ou les deux acteurs. 
Voici quelques définitions des relations \keyword[R]{\ac{rst}} \cite{2006-Cornish} :
\begin{itemize}
	\item \optword{Élaboration} :  S donne des informations additionnelles sur la situation présentée dans N
	\begin{itemize}
		\item \textbf{Contraintes sur N + S :} S présente des informations supplémentaires vis-à-vis de la situation ou de quelqu'aspect du thème présenté(e) dans N 
		\item \textbf{Effet :}  L reconnaît la situation présentée dans S comme fournissant des informations supplémentaires au sujet de N. 
		\item \textbf{Lieu de l'effet :} N + S
		\item Ex. \expword{[\textsubscript{N} L'examen est facile.] [\textsubscript{S} Il ne prend qu'une heur.]}
	\end{itemize}

	\item \optword{Évidence} :  S donne des informations additionnelles sur la situation présentée dans N dans le but de convincre L à accepter les informations de N
	\begin{itemize}
		\item \textbf{Contraintes sur N :} L pourra ne pas croire en N à un degré suffisant pour Sc
		\item \textbf{Contraintes sur S :} L croit S ou le trouve crédible
		\item \textbf{Contraintes sur N + S :} La compréhension de S par L augmente sa croyance en N
		\item \textbf{Effet :}  La croyance de L en N est effectivement augmentée
		\item \textbf{Lieu de l'effet :} N
		\item Ex. \expword{[\textsubscript{N} Kevin doit être ici.] [\textsubscript{S} Sa voiture est garée à l'extérieur.]}
	\end{itemize}

	\item \optword{Contraste} :  Une relation d'opposition entre deux noyaux
	\begin{itemize}
		\item \textbf{Contraintes sur N :} multi-noyaux
		\item \textbf{Contraintes sur N + N :} pas plus de deux noyaux ; les situations présentées dans ces deux noyaux sont (a) comprises comme les mêmes à beaucoup d'égards, (b) comprises comme différant par quelques aspects et (c) comparées par rapport à l'une ou plus d'une de ces différences
		\item \textbf{Effet :}  L reconnaît la comparabilité et les différences fournies par la comparaison effectuée
		\item \textbf{Lieu de l'effet :} noyaux multiples
		\item Ex. \expword{[\textsubscript{N} Il a voté "Non" à la nouvelle constitution.] [\textsubscript{N} Son frère a voté "Oui".]}
	\end{itemize}

	\item \optword{Antithèse} :  Une relation d'opposition entre N et S dans le but que L ait une attitude positive envers N
	\begin{itemize}
		\item \textbf{Contraintes sur N :} Sc a une attitude positive par rapport à la situation présentée dans N
		\item \textbf{Contraintes sur N + N :} es situations présentées dans N et S sont en opposition (cf. CONTRASTE)
		\item \textbf{Effet :}  L'attitude positive de L vis-à-vis de N est augmentée
		\item \textbf{Lieu de l'effet :} N
		\item Ex. \expword{[\textsubscript{N} J'ai défendu la République jeune;] [\textsubscript{S} Je ne l'abandonnerai pas maintenant que je suis vieux.]}
	\end{itemize}
\end{itemize}

Les relations \keyword[R]{\ac{rst}} peuvent être classifiées selon la distinction thématique/présentationnelle (voir le tableau \ref{tab:rst-class-lect}). 
Une relation thématique vise que le lecteur identifie la relation en question.
Elle est utilisée afin d'exprimer certains aspects de la thématique. 
Par contre, une  relation présentationnelle vise à augmenter une disposition chez le lecteur, comme le désir d'agir ou le degré de son attitude positive envers, ou croyance en, ou encore acceptation de la proposition noyau. 

\begin{table}[ht]
	\centering\small
\begin{tabular}{p{.3\textwidth}p{.3\textwidth}p{.3\textwidth}}
	\hline\hline
	\textbf{thématique} && \textbf{présentationnelle}\\
	\hline
	
	Élaboration
	
	Circonstance
	
	Problème-Solution
	
	Cause intentionnelle
	
	Résultat intentionnel
	
	Cause non-intentionnelle
	
	Résultat non-intentionnel
	
	& But
	
	Condition
	
	Interprétation
	
	Évaluation
	
	Reformulation
	
	Résumé
	
	Séquence
	
	Contraste
	&
	Motivation
	
	Antithèse
	
	Arrière-plan
	
	Facilitation
	
	Évidence (« indice »)
	
	Justification
	
	Concession\\
	\hline\hline
\end{tabular}
\caption[Classification des relations RST selon le point de vu lecteur]{Classification des relations RST selon le point de vu lecteur \cite{2006-Cornish}}
\label{tab:rst-class-lect}
\end{table}

Les relations de cohérence peuvent être classifiées selon le niveau de traitement du langage (voir le tableau \ref{tab:rst-niveau}).
La catégorie ``thématique" correspondait à la catégorie ``sémantique" dans cette classification. 
La catégorie ``présentationnelle" peut être divisée sur deux catégories : ``pragmatique" et ``textuelle".  

\begin{table}[ht]
	\centering\small
\begin{tabular}{p{.3\textwidth}p{.2\textwidth}p{.2\textwidth}p{.15\textwidth}}
	\hline\hline
	\textbf{Sémantique} && \textbf{Pragmatique} & \textbf{Textuelle} \\
	\hline
	
	Élaboration
	
	Circonstance
	
	Problème-Solution
	
	Cause intentionnelle
	
	Résultat intentionnel
	
	Cause non-intentionnelle
	
	Résultat non-intentionnel
	&
	But
	
	Condition
	
	Interprétation
	
	Évaluation
	
	Séquence
	
	Contraste
	
	&
	
	Motivation
	
	Antithèse
	
	Facilitation
	
	Évidence (« indice »)
	
	Justification
	
	Concession
	
	&
	
	Arrière-plan
	
	Reformulation
	
	Résumé\\
	\hline\hline
\end{tabular}
\caption[Classification des relations RST selon les niveaux de traitement du langage]{Classification des relations RST selon les niveaux de traitement du langage \cite{2006-Cornish}}
\label{tab:rst-niveau}
\end{table}

\subsection{Penn Discourse TreeBank (PDTB)}

\keyword[P]{\ac{pdtb}} est un dataset annoté avec un autre modèle de cohérence. 
Dans ce modèle, on ne s'intéresse pas par la structure en arbre ; on s'intéresse seulement par les relations binaires. 
Les unités en relation sont annotées comme arguments : ``ARG1" et ``ARG2". 
L'unité marquée par ``ARG2" est annotée par un connective du discours (explicitement ou implicitement). 
Les connectives du discours peuvent être (voir le tableau \ref{tab:pdtb-connect} pour des statistiques sur les connectives et leurs sens) :
\begin{itemize}
	\item \optword{Conjonctions de subordination} :  
	temporelle (Ex., \expword{'when', 'as soon as'}), 
	causale (Ex., \expword{'because'}), 
	concessive (Ex., \expword{'although', 'even though'}), 
	objectif (Ex.,\expword{'so that', 'in order that'}) et 
	conditionnelle (Ex., \expword{'if', 'unless'}).
	
	\item \optword{Conjonctions de coordination} : \expword{'and', 'but', 'or'}
	
	\item \optword{Conjonctives adverbiales} : des adverbes qui expriment une relation de discours entre des évenements ou des états. Ex., \expword{'however', 'therefore', 'then', etc.}
	Des syntagmes prépositionnels sont inclus aussi dans cette classe. Ex. \expword{'as a result',
		'in addition', 'in fact', etc. }
	
	\item \optword{Connectives implicites} :  identifiées entre deux phrases adjacentes qui ne sont pas reliées par des connectives explicites.
\end{itemize}

\begin{table}[ht]
	\centering\small
	\begin{tabular}{p{.1\textwidth}lp{.8\textwidth}}
		\hline\hline
		\textbf{Connective} && \textbf{Sens}\\
		\hline
		
		after && succession (523), succession-reason (50), other (4) \\
		since && reason (94), succession (78), succession-reason (10), other (2) \\
		when && Synchrony (477), succession (157), general (100), succession-reason (65), Synchrony-general (50),
		Synchrony-reason (39), hypothetical (11), implicit assertion (11), Synchrony-hypothetical (10), other
		(69) \\
		while && juxtaposition (182), Synchrony (154), Contrast (120), expectation (79), opposition (78), Conjunction
		(39), Synchrony-juxtaposition (26), Synchrony-Conjunction (21), Synchrony-Contrast(22), COMPARISON (18), Synchrony-opposition (11), other (31) \\
		meanwhile && Synchrony-Conjunction (92), Synchrony (26), Conjunction (25), Synchrony-juxtaposition (15),
		other(35)\\
		but && Contrast (1609), juxtaposition (636), contra-expectation (494), COMPARISTON (260), opposition
		(174), Conjunction (63), Conjunction-Pragmatic contrast (14), Pragmatic-contrast (14), other (32)
		however Contrast (254), juxtaposition (89), contra-expectation (70), COMPARISON (49), opposition (31),
		other (12)\\
		although && expectation (132), Contrast (114) juxtaposition (34), contra-expectation (21), COMPARISON (16),
		opposition (9), other (2)\\
		and && Conjunction (2543), List (210), result-Conjunction (138), result (38), precedence-Conjunction (30),
		juxtaposition (11), other(30)\\
		if && hypothetical (682), general (175), unreal present (122), factual present (73), unreal past (53), expectation (34), implicit assertion (29), relevance (20), other (31)\\
		\hline\hline
	\end{tabular}
	\caption[Quelques connectives et des statistiques sur leurs sens]{Quelques connectives et des statistiques sur leurs sens \cite{2008-prasad-al}}
	\label{tab:pdtb-connect}
\end{table}

Les relations \keyword[P]{\ac{pdtb}} sont divisées en quatre familles : temporelle, comparaison, contingence et extension. 
Chaque relation appartient à une famille d'un façon hiérarchique comme indiqué dans la figure \ref{fig:pdtb-rel}. 
Par exemple, la relation de succession indiquée par des connectives comme ``after" est une relation synchrone qui est à son tour une relation temporelle.

\begin{figure}[!ht]
	\centering
\begin{minipage}{.3\textwidth}
	\scriptsize\bfseries
\begin{itemize}
	\item CONTINGENCY
	\begin{itemize}
		\item Cause
		\begin{itemize}
			\item Reason
			\item Result
		\end{itemize}
		\item Pragmatic Cause
		\begin{itemize}
			\item Justification
		\end{itemize}
		\item Condition
		\begin{itemize}
			\item Hypothetical
			\item General
			\item Unreal Present
			\item Unreal Past
			\item Factual Present
			\item Factual Past
		\end{itemize}
		\item Pragmatic Condition
		\begin{itemize}
			\item Relevance
			\item Implicit Assertion
		\end{itemize}
	\end{itemize}
\end{itemize}
\end{minipage}
\begin{minipage}{.3\textwidth}
	\scriptsize\bfseries
\begin{itemize}
	\item TEMPORAL
	\begin{itemize}
		\item Asynchronous
		\item Synchronous
		\begin{itemize}
			\item Precedence
			\item Succession
		\end{itemize}
	\end{itemize}
	\item COMPARISON
	\begin{itemize}
		\item Contrast
		\begin{itemize}
			\item Juxtaposition 
			\item Opposition
		\end{itemize}
		\item Pragmatic Contrast
		\item Concession
		\begin{itemize}
			\item Expectation
			\item Contra-expectation
		\end{itemize}
		\item Pragmatic Concession
	\end{itemize}
\end{itemize}
\end{minipage}
\begin{minipage}{.3\textwidth}
	\scriptsize\bfseries
	\begin{itemize}
		\item EXPANSION
		\begin{itemize}
			\item Conjunction
			\item Instantiation
			\item Restatement
			\begin{itemize}
				\item Specification
				\item Equivalence
				\item Generalization
			\end{itemize}
			\item Alternative
			\begin{itemize}
				\item Conjunction
				\item Disjunction
				\item Chosen Alternative
			\end{itemize}
			\item Exception
			\item List
		\end{itemize}
	\end{itemize}
\end{minipage}\vspace{-0.5cm}
	\caption[Hiérarchie des sens dans PDTB]{Hiérarchie des sens dans PDTB \cite{2008-prasad-al}}
	\label{fig:pdtb-rel}
\end{figure}



%\begin{figure}[ht]
%	\centering
%	\hgraphpage[.6\textwidth]{pdtb-sens_.pdf}
%	\caption{Hiérarchie des sens dans PDTB \cite{2008-prasad-al}}
%	\label{fig:pdtb-rel}
%\end{figure}



%===================================================================================
\section{Analyse basée structure de discours}
%===================================================================================

Un discours est cohérent s'il existe des relations entre ses parties. 
Analyser un discours veut dire chercher une structure du texte.
Dans le cas de \keyword[R]{\ac{rst}}, cette structure est un arbre indiquant les relations de cohérence entre les différentes parties. 
Contrairement à \keyword[R]{\ac{rst}}, \keyword[P]{\ac{pdtb}} ne génère pas un arbre.
Plutôt, c'est un ensemble des relations binaires entre les parties du texte ; surtout les phrases consécutives. 


\subsection{Analyse RST}

L'analyse \keyword[R]{\ac{rst}} consiste à construire un arbre de relations de cohérence entre les parties d'un texte. 
Cette tâche passe par deux étapes : détection des unités élémentaires de discours et classification des relations. 
La figure \ref{fig:rst-exp} représente une analyse \keyword[R]{\ac{rst}} d'un texte.
On remarque que l'unité n'est pas toujours une phrase, mais elle peut être une partie d'une phrase (une clause).

\begin{figure}[!ht]
	\centering
	\hgraphpage[0.5\textwidth]{RST-arbre.pdf}
	\caption{Exemple d'un arbre RST}
	\label{fig:rst-exp}
\end{figure}

\subsubsection{Détection des unités élémentaires}

Une unité élémentaire de discours, en anglais \ac{edu}, est une phrase ou une clause de la phrase qui représente un sens. 
Exemple d'un texte divisé en \ac{edu} : \expword{[Mr. Rambo says]\textsubscript{e1} [that a 3.2-acre property]\textsubscript{e2} [overlooking the San Fernando Valley]\textsubscript{e3} [is priced at \$4 million]\textsubscript{e4} [because the late actor Erroll Flynn once lived there.]\textsubscript{e5}}. 
La détection des \ac{edu} consiste à trouver ces unités qui sont en général des clauses. 
Une des méthodes les plus anciennes utilise l'analyse syntaxique afin de trouver ces clauses. 
Des méthodes statistiques peuvent être utilisées afin de détecter les limites des \ac{edu}s. 
Comme caractéristiques, on peut utiliser les informations syntaxiques et des indices de surface. 
Ce problème peut être vu comme annotation des séquences. 
Dans \cite{2018-wang-al}, les auteurs ont proposé un système neuronal\footnote{EDU avec réseau de neurones : \url{https://github.com/PKU-TANGENT/NeuralEDUSeg}} qui utilise le \keyword[E]{embedding} des mots. 
La figure \ref{fig:edu-embedding} représente leur architecture où l'entrée est une concaténation entre deux \keyword[E]{embedding}s : \keyword[G]{GloVe} et \keyword[B]{BERT}. 
On utilise un réseau \keyword[B]{Bi-LSTM} afin d'avoir le contexte en avant et en arrière pour chaque mot. 
Ensuite, chaque mot est classé comme : début d'un \ac{edu} (1) ou continuation d'un \ac{edu} (0). 

\begin{figure}[!ht]
	\centering
	\hgraphpage[.6\textwidth]{EDU_seg_.pdf}
	\caption[Segmentation en EDUs par embeddings]{Segmentation en EDUs proposée par \cite{2018-wang-al} ; figure prise de \cite{2019-jurafsky-martin}}
	\label{fig:edu-embedding}
\end{figure}

\subsubsection{Classification des relations}

Un fois les \ac{edu} sont extraits, on doit les lier en utilisant les relations de cohérence. 
La méthode la plus utilisée pour trouver la structure \keyword[R]{\ac{rst}} est SHIFT-REDUCE.  
Celle-ci se base sur une machine abstraite ayant la configuration $C = (\sigma, \beta, A)$ où $\sigma$ est une pile, $\beta$ est le tampon (buffer) d'entrée et $A$ est la liste des arcs créés (relations). 
La figure \ref{fig:rst-shift-reduce} représente l'architecture de cette machine qui est similaire à celle utilisée dans l'analyse syntaxique des dépendances par transition. 
La différence est que cette machine utilise les \ac{edu}s comme éléments et pas les mots.
Au début, la pile est vide, la liste des relations est vide et le tampon d'entrée contient tous les \ac{edu} ordonnés d'où  $C_{initiale} = (\varnothing, w, \emptyset)$. 
A la fin, la pile et le tampon d'entrée doivent être vides d'où $C_{finale} = (\varnothing, \varnothing, A)$.

\begin{figure}[!ht]
	\centering
	\hgraphpage[.38\textwidth]{RST-transitions.pdf}
	\caption{Architecture SHIFT-REDUCE pour la résolution de la structure RST}
	\label{fig:rst-shift-reduce}
\end{figure}

Les opérations permises par cette machine sont :
\begin{itemize}
	\item \optword{Shift} : mettre le premier élément du buffer dans la pile
	\item \optword{Reduce}\textbf{(l, d)} : fusionne les deux sous-arbres supérieurs de la pile, où \textbf{l} est l'étiquette de relation de cohérence, et \textbf{d} est la direction de nucléarité : \textbf{d $ \in $ \{NN, NS, SN\}}.
	\item \optword{Pop Root} : enlever la racine de l'arbre final de la pile.
\end{itemize}
Un exemple d'une analyse \keyword[R]{\ac{rst}} en utilisant la méthode SHIFT-REDUCE est illustré das la figure \ref{fig:rst-shred-yu-al}.

\begin{figure}[!ht]
	\centering
	\hgraphpage{RST_exp_.pdf}
	
	\hgraphpage[.7\textwidth]{RST_SR_exp_.pdf}
	\caption[Exemple d'analyse RST en utilisant Shif-Reduce]{Exemple d'analyse RST en utilisant Shif-Reduce \cite{2018-yu-al}}
	\label{fig:rst-shred-yu-al}
\end{figure}

Le composant ``Oracle" doit être entraîné afin de décider l'opération suivante. 
On peut utiliser des caractéristiques afin d'entraîner un algorithme d'apprentissage comme on a vu dans l'analyse des dépendances (chapitre 5). 
On peut aussi utiliser les \keyword[E]{embedding}s avec un réseau de neurone comme la méthode proposée dans \cite{2018-yu-al}\footnote{Embedding pour RST : \url{https://github.com/yunan4nlp/NNDisParser}}. 
Les auteurs proposent d'utiliser une architecture encodeur-décodeur afin de décider l'opération suivante. 
L'encodeur a comme but de représenter tous les \ac{edu} comme des vecteurs.
Le décodeur prend les représentations de quelques \ac{edu} pour décider l'opération suivante. 

On commence par l'encodeur qui prend en entrée une représentation $x_i^w$ de chaque mot $w_i$. 
Cette représentation et la concaténation du \keyword[E]{embedding} du mot $w_i$ et de sa catégorie grammaticale $t_i$ comme indiqué dans l'équation \ref{eq:rst-embedding-entree}
\begin{equation}\label{eq:rst-embedding-entree}
x_i^w = embedding(w_i) \oplus embedding(t_i)
\end{equation}
Les mots d'une phrase $w_1 w_2 \ldots w_m$ sont passés par un réseau récurrent \keyword[B]{Bi-LSTM} pour avoir une représentation séquentielle comme indiqué dans l'équation \ref{eq:rst-embedding-seqrep}.
\begin{equation}\label{eq:rst-embedding-seqrep}
\{h_1^w, h_2^w, \ldots, h_m^w \} = biLSTM(\{x_1^w, x_2^w, \ldots, x_m^w \})
\end{equation}
Nous avons déjà sélectionné les \ac{edu}s ; donc on sais chaque début et fin d'un \ac{edu}. 
Afin de représenter un \ac{edu} $\{w_s, w_{s+1}, \ldots, w_t \}$, on cherche le vecteur central des vecteurs représentant les mots qui lui composent comme indiqué dans l'équation \ref{eq:rst-embedding-edurep}
\begin{equation}\label{eq:rst-embedding-edurep}
x^e = \frac{1}{t-s+1} \sum_{k=s}^{t} h_k^w
\end{equation}
Une fois la représentation individuelle de chaque \ac{edu} $x_i^e$ est trouvée, on cherche leurs représentations séquentielles. 
Pour ce faire, on utilise un autre réseau récurrent \keyword[B]{Bi-LSTM} comme indiqué dans l'équation \ref{eq:rst-embedding-eduseqrep}
\begin{equation}\label{eq:rst-embedding-eduseqrep}
\{h_1^e, h_2^e, \ldots, h_n^e \} = biLSTM(\{x_1^e, x_2^e, \ldots, x_n^e \})
\end{equation}

Le décodeur est une couche neuronale feed-forward $W$ qui vise à inférer l'action suivante $o$. 
En entrée, il prend la représentation séquentielle du premier UED dans le buffer $ h_{e}^{q0} $ et les représentations séquentielles des trois sous-arbres $i$ au sommet de la pile $h_{si}^{sbt}$. 
Le décodeur est représenté par l'équation \ref{eq:rst-embedding-dec}. 
\begin{equation}\label{eq:rst-embedding-dec}
o = W(h_{s0}^{sbt} \oplus h_{s1}^{sbt} \oplus h_{s2}^{sbt} \oplus h_{q0}^{e})
\end{equation}
Un sous arbre peut être représenté par plusieurs \ac{edu}s $ s= \{e_i, \ldots, e_j\}$. 
Sa représentation peut être calculée par la moyenne des représentations des UEDs couverts, comme indiqué dans l'équation \ref{eq:rst-embedding-sousarbre}. 
\begin{equation}\label{eq:rst-embedding-sousarbre}
h_{s}^{sbt} = \frac{1}{j-i+1} \sum_{k=i}^{j} h_k^e
\end{equation}

\subsection{Analyse PDTB}

Dans l'analyse \ac{pdtb}, on essaye de séparer les parties du texte et les annoter deux à deux par : ``ARG1" et ``ARG2". 
Dans la partie annotée par ``ARG2", on essaye de l'annoter par une connective. 
Nous avons vu qu'il y a deux types de connectives : explicites (existantes dans le texte) et implicites (qu'on peut inférer). 
Dans le cas des connectives explicites, on peut facilement les chercher et les trouver en utilisant leur liste. 
Mais, il existe des connectives qui ne représentent pas des relations de cohérence. 
Pour résoudre ce problème, on peut utiliser un algorithme de désambigüisation qui classe un connective donnée comme : discours ou non. 
Une fois une connective est marquée comme discours, on marque la partie le contenant comme ``ARG2". 
On cherche la partie en relation ``ARG1" en utilisant un algorithme d'apprentissage automatique. 
L'algorithme prend ``ARG2" et une autre partie adjacente comme entrée et comme sortie il estime si elles sont en relation ou non. 
Ensuite, on marque la relation entre les deux en utilisant la connective. 

Dans le cas où le connective est implicite, on doit inférer la connective étant donnée deux parties sans connectives. 
Une méthode est d'utiliser \keyword[B]{BERT} avec les parties séparées par ``[SEP]" comme entrée. 
Dans la sortie ``[CLS]", on attache un réseau de neurones à propagation avant afin d'inférer la connective.
Une autre méthode similaire est l'utilisation de deux réseaux \keyword[L]{LSTM} pour représenté chacune des deux parties. 
Cette méthode a été proposé dans \cite{2020-liang-al} où l'architecture est illustrée dans la figure \ref{fig:pdtb-liang}.
Le ``Max-Pool" est utilisé sur les représentations récurrentes des mots afin de composer le sens d'une partie et aussi pour réduire les paramètres du modèle.
Dans chaque position dans les vecteurs des mots, il prend le nombre max pour avoir un autre vecteur de même taille.

\begin{figure}[!ht]
	\centering
	\hgraphpage[.6\textwidth]{PDTB_exp_.pdf}
	\caption[Architecture pour la détection de relations PDTB implicites]{Architecture pour la détection de relations PDTB implicites \cite{2020-liang-al}}
	\label{fig:pdtb-liang}
\end{figure}

%===================================================================================
\section{Analyse basée sur l'entité de discours}
%===================================================================================

Un discours est cohérent s'il discute le même sujet.
Ce dernier peut être représenté par une entité. 
Quelque soit la position dans le discours, cette entité doit rester la plus importante.
Prenons l'exemple ``\expword{John went to his favorite music store to buy a piano [John]. He had frequented the store for many years [John]. He was excited that he could finally buy a piano [John]. He arrived just as the store was closing for the day [John].}. 
Dans cet exemple, l'entité centrale de chaque phrase est ``John". 
Maintenant, prenons l'exemple : ``\expword{John went to his favorite music store to buy a piano [John]. It was a store John had frequented for many years [The store]. He was excited that he could finally buy a piano [John]. It was closing just as John arrived [The store].}
On peut sentir que le texte est non naturel ; il est un peut difficile à comprendre. 
Cela est dû au fait que l'entité centrale se bascule entre les phrases ; des fois ``John", d'autres ``The store". 
Dans cette section, on va discuter deux théories/méthodes pour analyser un texte en se basant sur l'entité et pas la structure.

\subsection{Centering theory}

Nous avons vu que les phrases (énoncé) doivent maintenir la même entité centrale. 
Cette intuition est réalisée dans cette théorie en maintenant deux représentations pour chaque énoncé $\mathbf{U_n}$.
La première est l'entité saillante actuelle ; celle sur laquelle se concentre le discours dans l'énoncé $ U_{n-1} $.
Elle s'appelle \optword{Backward-looking center} (centre rétrospectif), et elle est dénotée par $\mathbf{C_b(U_n)}$. 
La deuxième est un ensemble des entités potentiellement saillantes dans le futur ; celles candidates pour être $C_b(U_{n+1})$.
Il s'appelle \optword{Forward-looking center} (centres prospectifs), et il est dénoté par $\mathbf{C_f(U_n)}$. 
On score les entités de cet ensemble en se basant sur leurs rôles grammaticaux (sujet plus important que l'objet qui est plus important que le reste), l'ordre (Ex. \expword{En Arabe, ce qui est en premier est plus important}), etc.
L'entité avec le plus grand score est choisie comme candidate pour être $C_b(U_{n+1})$.
Elle s'appelle \optword{Prefered center} (centre préféré), et elle est dénotée par $\mathbf{C_p(U_n)}$. 

Cette théorie se base sur l'hypothèse que le discours est plus facile à traiter lorsque les énoncés successifs parlent de la même entité. 
Cette hypothèse est formalisée comme une classification des énoncés selon la transition qu'ils induisent dans le centre local.
Il existe trois types de transitions \cite{2004-poesio-al} selon l'ordre de la plus cohérente : 
\begin{itemize}
	\item Ordre des transitions selon la plus cohérente
	\item \optword{CONTINUE} : le locuteur parle d'une entité et il a l'intention d'en parler en futur
	\item \optword{RETAIN} : le locuteur parle d'une entité et il a l'intention d'en changer en futur
	\item \optword{SHIFT} : le locuteur a changé l'entité centrale
	\begin{itemize}
		\item \textbf{Smooth-SHIFT} : après le changement, il a l'intention d'en parler en futur
		\item \textbf{Rough-SHIFT} : après le changement, il a l'intention d'en changer en futur
	\end{itemize}
\end{itemize}

La théorie de centralité (Centering theory) se base sur deux règles. 
La première règle décrit que si un élément de $C_f(U_n)$ est réalisé par un pronom dans l'énoncé suivant ($U_{n+1}$) alors $C_b(U_{n+1})$ doit être réalisée par un pronom. 
L'intuition, ici, est que la pronominalisation est un moyen courant de marquer la saillance du discours. 
S'il y a plusieurs pronoms dans un énoncé réalisant des entités de l'énoncé précédent, l'un de ces pronoms doit
réaliser le centre arrière $C_b$.
La deuxième règle concerne les transitions mentionnées précédemment, où ``CONTINU" est plus cohérente que ``RETAIN" que ``Smooth-SHIFT" que ``Rough-SHIFT".
Ces transitions sont calculée selon le tableau \ref{tab:center-trans}.
L'intuition de cette règle est que les discours qui continuent à center sur la même entité sont plus cohérents que ceux qui basculent vers d'autres centres.

\begin{table}[!ht]
	\centering
	\begin{tabular}{p{.2\textwidth}p{.2\textwidth}p{.2\textwidth}}
		\hline\hline
		& \bfseries$\mathbf{C_b(U_n) = C_b(U_{n-1})}$
		
		OU $\mathbf{C_b(U_n) = NULL}$
		& \bfseries$\mathbf{C_b(U_n) \ne C_b(U_{n-1})}$\\
		\hline
		
		$\mathbf{C_b(U_n) = C_p(U_n)}$ &
		CONTINUE & Smooth-SHIFT\\
		
		$\mathbf{C_b(U_n) \ne C_p(U_n)}$ &
		RETAIN & Rough-SHIFT\\
		\hline\hline
	\end{tabular}
	\caption{Transitions de la théorie de centralité}
	\label{tab:center-trans}
\end{table}

Reprenons deux phrases de l'exemple précédent, et essayons de calculer la centralité
\begin{itemize}
	\item John went to his favorite music store to buy a piano. $U_1$
	\begin{itemize}
		\item $C_b(U_1)$ = NULL
		\item $C_f(U_1)$ = \{John, music store, piano\}
		\item $C_p(U_1)$ = John (le sujet)
	\end{itemize}
	\item It was a store John had frequented for many years. $U_2$
	\begin{itemize}
		\item $C_b(U_2)$ = John
		\item $C_f(U_2)$ = \{(music) store, John, years\}
		\item $C_p(U_2)$ =  music store (sujet)
		\item $C_b(U_2) \ne C_p(U_2) \wedge C_b(U_2) \ne C_b(U_1) \Rightarrow$ Rough-SHIFT
	\end{itemize}
	\item He was excited that he could finally buy a piano. $U_3$
	\begin{itemize}
		\item $C_b(U_3)$ = music store
		\item $C_f(U_3)$ = \{John, piano\}
		\item $C_p(U_3)$ =  John (sujet)
		\item $C_b(U_3) \ne C_p(U_3) \wedge C_b(U_3) \ne C_b(U_2) \Rightarrow$ Rough-SHIFT
	\end{itemize}
\end{itemize}


\subsection{Entity Grid model}

Dans le modèle ``entity grid", un document est représenté par une matrice où les phrases représentent les lignes et les entités représentent les colonnes. 
Chaque cellule de cette matrice contient la fonction grammaticale de l'entité dans la phrase : sujet (S), Objet (O), Autre (X) ou l'entité n'existe pas (\_). 
La figure \ref{fig:entity-grid-rep} représente un texte et sa représentation phrases/entités selon le modèle ``entity grid". 

\begin{figure}[!ht]
	\centering
	\hgraphpage[.7\textwidth]{EGM_doc_exp_.pdf}
	
	\hgraphpage[.4\textwidth]{EGM_doc_rep_exp_.pdf}
	\caption[Exemple de la représentation d'un document par entités]{Exemple de la représentation d'un document par entités \cite{2008-barzilay-lapata}}
	\label{fig:entity-grid-rep}
\end{figure}

Un document est considéré comme cohérent s'il suit une certaine forme de pattern.
Ce pattern peut-être représenté par les fréquences de transitions grammaticales. Ex. \expword{SS, SO, SX, S\_, OS, OO, OX, O\_, ...}
Dans l'exemple précédent, la transition ``S\_" a une fréquence de 6.
Un document peut être représenté par les probabilités des transitions grammaticales.
Une probabilité est le ration entre le nombre d'une transition et le nombre de toutes les transitions.
Ex. Dans l'exemple précédent : \expword{P(S\_) = 6/75}.
La figure \ref{fig:entity-grid-prob} représente les probabilités des transitions de longueur 2 du document précédent.

\begin{figure}[!ht]
	\centering
	\hgraphpage[.8\textwidth]{EGM_doc_vec_exp_.pdf}
	\caption[Exemple de la représentation des documents par transitions grammaticales]{Exemple de la représentation des documents par transitions grammaticales \cite{2008-barzilay-lapata}}
	\label{fig:entity-grid-prob}
\end{figure}

Afin de juger qu'un texte soit cohérent, on utilise un algorithme d'apprentissage automatique où les probabilités des transitions grammaticales sont utilisées comme caractéristiques. 
Le modèle d'apprentissage peut être entraîné à attribuer des scores selon le niveau de cohérence.
Dans ce cas, un texte non cohérent doit avoir un score inférieur à un autre cohérent.
Le problème qui se pose : il est difficile d'avoir beaucoup de textes annotés pour entraîner le système.
Une solution est d'utiliser une méthode auto-supervisée ; créer un texte non cohérent à partir d'un texte cohérent. 
Ceci peut être accompli en appliquant un ordre aléatoire des phrases d'un texte cohérent. 

Cette idée peut être utilisée pour tester les systèmes d'analyse de la cohérence : un texte cohérent doit avoir un score plus qu'un autre non cohérent. 
Donc, on peut prendre un texte cohérent (les romans, etc.) et crée un autre non cohérent selon une de ces techniques :
\begin{itemize}
	\item Discrimination de l'ordre des phrases : ordre aléatoire des phrases et comparaison du score de cohérence avec celui de l'ordre original.
	\item Insertion de la phrase : modifier l'ordre d'une seule phrase et comparaison du score de cohérence avec celui de l'ordre original.
	\item Reconstruction de l'ordre des phrases : apprendre à ordonner les phrases
\end{itemize}


\begin{discussion}
Une phrase doit être bien formée ; ceci est garanti par la grammaire. 
Elle doit, aussi, avoir un sens ; ceci est garanti par la sémantique. 
Lorsqu'on fusionne plusieurs phrases qui ont un sens, ce n'est pas une garantie que ce texte soit compréhensible ou au moins naturel. 
Un texte doit être cohérent. 
Mais, c'est quoi la cohérence ? Comment elle peut être mesurée ?
Il existe plusieurs théories pour représenter la cohérence qui peuvent être classées en deux approches : basées sur la structure ou basées sur l'entité. 
Les phrases (ou les clauses) d'un texte forment une structure qui se base sur des relations de cohérence. 
Aussi, elles doivent discuter la même entité.
Les deux approches ont deux visions différentes ; qui sont complémentaires à mon avis. 
Afin de juger la cohérence d'un texte, il faut mieux utiliser les deux afin de vérifier deux aspects différents.

\end{discussion}

%=====================================================================
\ifx\wholebook\relax\else
% \cleardoublepage
% \bibliographystyle{../use/ESIbib}
% \bibliography{../bib/RATstat}
	\end{document}
\fi
%=====================================================================
