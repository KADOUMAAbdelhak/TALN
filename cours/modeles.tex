% !TEX TS-program = xelatex
% !TeX program = xelatex
% !TEX encoding = UTF-8
% !TEX spellcheck = fr

%=====================================================================
\ifx\wholebook\relax\else
	\documentclass{KodeBook}
	\input{calls}
	\begin{document}
		\mainmatter
	
\fi
%=====================================================================
\changegraphpath{../img/modeles/}
\chapter{Modèles de langue}

\begin{introduction}[LE\textcolor{white}{S} LANGUES]
	\lettrine{S}{i} on veut tester qu'un langage est bien écrit, on doit revenir aux règles de composition de ces phrases.
	Un langage se compose d'un vocabulaire et une grammaire pour construire les phrases.
	Par exemple, on peut trouver un syntagme nominale suivi par un verbe suivi par un syntagme nominale si le verbe est transitif.
	En statistique, ces règles peuvent être vu comme la probabilité d'apparition d'un mot sachant qu'un ou plusieurs mots ont apparus avant. 
	Les probabilités estimées à partir d'un corpus d'entrainement sont appelées un modèle de langage. 
	Ce dernier est utile pour estimer le mot suivant sachant une liste de mots et aussi calculer la probabilité qu'une phrase soit juste.
	Dans ce chapitre, on va présenter les modèles de langage traductionnels (NGrams), ainsi que ceux basés sur les réseaux de neurones.
\end{introduction} 


Une phrase est bien définie si sa probabilité $P(S) = P(w_1, w_2, ..., w_n) $ égale à $1$. 
Une phrase avec une probabilité plus grande qu'une autre a plus de chance de ce produire étant donnée le contexte qu'on a appris. 
Cette hypothèse a plusieurs applications : 
\begin{itemize}
	\item Traduction automatique : En traduisant un texte d'une langue vers une autres, on peut trouver des mots avec plusieurs traductions/sens. 
	En plus, l'ordre des mots n'est pas toujours symétrique. 
	En utilisant un modèle de langage de la langue destinatrice, on peut vérifier le choix le plus probable. 
	Exemple, 
	\expword{My tall brother \textrightarrow\ P(Mon grand frère) \textgreater\ P(Mon haut frère)}.
	
	\item Correction des fautes grammaticales : Du même, en utilisant un modèle de langue, on peut vérifier qu'un mot n'a pas de chance de se produire après une séquence. 
	En calculant, par exemple, les différentes probabilités des phrases avec les mots similaires (en terme de distance d'édition) avec le mot erroné, on peut suggérer les corrections.
	Exemple, \expword{P(Un objet qu'on puisse emporter) \textgreater\ P(Un objet qu'ont puisse emporter)}.
	
	\item Reconnaissance de paroles : Lorsqu'on transforme les paroles en texte, le programme peut mélanger entre les mots ou les expressions proches en prononciation. 
	On peut aider le choix des mots en utilisant un modèle de langue.
	Exemple, \expword{P(Jeudi matin) \textgreater\ P(Je dis matin)}
\end{itemize}
%
En plus de la probabilité d'une phrase, on peut aussi estimer la probabilité d'occurrence d'un mot en se basant sur les mots précédents $P(w_n | w_1, \ldots, w_{n-1}) $. 
Estimer cette probabilité a plusieurs applications :
\begin{itemize}
	\item Auto-complétion : En se basant sur les mots déjà introduits par l'utilisateur, le programme estime la probabilité de chaque mot du vocabulaire et affiche sous avec une grande probabilité conditionnelle.
	Exemple, \expword{P(traitement automatique de l'information) \textgreater\ P(traitement automatique de l'eau)}.
	
	\item Génération automatique de textes : En utilisant une représentation interne, on essaye de générer le texte mot par mot. 
	Le programme utilise cette représentation et un modèle de langue pour apprendre la génération.
\end{itemize}


\section{Modèle N-gramme}

La probabilité d'occurrence d'une phrase est calculée en utilisant la formule des probabilités composées exprimée dans l'équation \ref{eq:ph-prob}.
Par exemple, \expword{P(\text{\textit{je travaille à l'ESI}}) =  P(je) P(travaille | je) P(à | je travaille) $ \ldots $ P(ESI | je travaille à l')}.
Afin d'estimer les probabilités conditionnelles, on utilise le maximum de vraisemblance (qui va être présenté juste après).
\begin{equation}\label{eq:ph-prob}
	P(w_1 \ldots w_m) =  P(w_1) P(w_2 | w_1) P(w_3 | w_1, w_2) \ldots P(w_m | w_1, \ldots, w_{m-1})
\end{equation}

Dans un langage naturel, on peut trouver plusieurs phrases possibles ; On peut même générer une infinité de phrases.
Pour estimer une probabilité conditionnelle avec un grand historique (mots précédents), on doit utiliser un corpus (dataset textuel) très grand. 
Vu que les phrases sont infinies, on ne peut pas représenter toutes les combinaisons possibles. 
Dans ce cas, une solution est de limiter la taille de l'historique. 
Donc, on essaye d'estimer $P(w_k|w_{k-n+1},\ldots,w_{k-1})$ avec un historique de $n-1$ mots ; 
Ce modèle est appelé NGram.

\subsection{Formulation}

\begin{block}{Propriété de Markov}
	Un état futur ne dépend que de l'état présent. 
	\[%
	P(x_n | x_1,\ldots, x_{n-1}) \approx P(x_n | x_{n-1})
	\]
	Cas général avec $k$ états passés 
	\[%
	P(x_n | x_1,\ldots, x_{n-1}) \approx P(x_n | x_{n-k+1}, \ldots, x_{n-1})
	\]
\end{block}

\begin{block}{Estimation de probabilité en utilisant les N-grammes (k grammes)}
	\[
	P(x_1,\ldots, x_{n}) \approx \prod_i P(x_i | x_{i-k+1}, \ldots, x_{i-1})
	\]
\end{block}

\begin{itemize}
	\item \optword{Modèle uni-gramme} : $P(w_n | w_1,\ldots, w_{n-1}) \approx P(w_n)$
	\item \optword{Modèle bi-gramme} : $P(w_n | w_1,\ldots, w_{n-1}) \approx P(w_n | w_{n-1})$
	\item \optword{Modèle tri-gramme} :  $P(w_n | w_1,\ldots, w_{n-1}) \approx P(w_n | w_{n-2}, w_{n-1})$
	\item Google Books Ngram Viewer
	\begin{itemize}
		\item \url{https://books.google.com/ngrams}
		\item Modèles pré-traités à partir des livres 
		\item Téléchargement gratuit : \url{https://storage.googleapis.com/books/ngrams/books/datasetsv3.html}
	\end{itemize}
\end{itemize}

Estimation
\begin{itemize}
	\item En utilisant un corpus d'entraînement avec suffisamment de données
	\item On marque le début et la fin des phrases avec \keyword{\textless s\textgreater} et \keyword{\textless/s\textgreater} (une fois pour les bi-grammes, 2 fois pour les tri-grammes, etc.)
	\item \keyword{Estimateur du maximum de vraisemblance}
\end{itemize}

\begin{block}{Estimation des probabilités en utilisant le maximum de vraisemblance}
	{\small \[%
		P(w_n | w_{n-k+1},\ldots, w_{n-1}) = \frac{C(w_{n-k+1} \ldots w_{n-1} w_n)}{\sum_i C(w_{n-k+1} \ldots w_{n-1} w_i)}
		= \frac{C(w_{n-k} \ldots w_{n-1+1} w_n)}{C(w_{n-k+1} \ldots w_{n-1})}
		\]}
	Où $C$ est le nombre d'occurrences des N-grammes dans le corpus
	\[%
	\text{Bi-grammes : } P(w_n | w_{n-1}) = \frac{C(w_{n-1} w_n)}{C(w_{n-1})}
	\]
\end{block}

\begin{exampleblock}{Exemple d'un corpus d'entraînement}
	\begin{itemize}
		\item \textless s\textgreater un ordianteur peut vous aider \textless/s\textgreater
		\item \textless s\textgreater il veut vous aider \textless/s\textgreater
		\item \textless s\textgreater il veut un ordinateur \textless/s\textgreater
		\item \textless s\textgreater il peut nager \textless/s\textgreater
	\end{itemize}
\end{exampleblock}

\begin{itemize}
	\item $P(peut | il) = \frac{C(il\ peut)}{C(il)} = \frac{1}{3}$
	\item $P(\text{\textit{\textless s\textgreater il peut vous aider \textless/s\textgreater}}) = 
	\underbrace{P(il|\text{\textit{\textless s\textgreater}})}_{\frac{3}{4}}
	\underbrace{P(peut|il)}_{\frac{1}{3}} 
	\underbrace{P(vous|peut)}_{\frac{1}{2}} 
	\underbrace{P(aider|vous)}_{\frac{2}{2}}
	\underbrace{P(\text{\textit{\textless/s\textgreater}}|aider)}_{\frac{2}{2}} = 
	%	\frac{3}{4} \frac{1}{3} \frac{1}{2} \frac{2}{2} \frac{2}{2} = \frac{1}{8}
	\frac{1}{8}
	$
\end{itemize}

\subsection{Lissage (Smoothing)}

\begin{itemize}
	
	\item Si on utilise des petits N-grammes $ \Longrightarrow $ Perte de l'information
	\begin{itemize}
		\item Les langues permettent des dépendances à long terme
		\item \expword{\underline{L'ordinateur} que j'ai utilisé hier à l'ESI pendant la  séance du cours \underline{a planté}}
	\end{itemize}
	
	\item Si on utilise des grands N-grammes $ \Longrightarrow $ Complexité élevée du modèle
	\begin{itemize}
		\item Il faut un corpus plus grand
		\item Représentation des N-grammes : $V^N$ où $V$ est la taille du vocabulaire et $N$ est le nombre de grammes
	\end{itemize}
	
	\item Problème des N-grammes absents dans le corpus d'entraînement
	\begin{itemize}
		\item $P(\text{\textit{\textless s\textgreater il veut nager \textless/s\textgreater}}) = 
		P(il|\text{\textit{\textless s\textgreater}}) P(veut|il) P(nager|veut)  P(\text{\textit{\textless/s\textgreater}}|nager) = 
		\frac{3}{4} \frac{2}{3} \frac{0}{1} \frac{1}{1} = 0
		$
	\end{itemize}
	\item L'intuition est d'emprunter une petite portion des probabilités des N-grammes existants pour former une probabilité aux N-grammes absents 
\end{itemize}

\begin{block}{Lissage de Lidstone (Bi-grammes comme exemple)}
	\[%
	P(w_n | w_{n-1}) = \frac{C(w_{n-1} w_n) + \alpha}{C(w_{n-1}) + \alpha V}
	\]
	Où $V$ est la taille du vocabulaire du modèle
	
	$\alpha = 1$ : \keyword{Lissage de Laplace} 
	
	$\alpha = 0.5$ : \keyword{Loi de Jeffreys-Perks}
\end{block}

\begin{exampleblock}{Exemple : lissage de Laplace}
	\begin{itemize}
		\item Le corpus contient 8 mots différents
		\item Il y a  $8^2 = 64$ bi-grammes possibles
		\item $P(\text{\textit{\textless s\textgreater il veut nager \textless/s\textgreater}}) = 
		P(il|\text{\textit{\textless s\textgreater}}) P(veut|il) P(nager|veut)  P(\text{\textit{\textless/s\textgreater}}|nager) = 
		\frac{3 + 1}{4 + 64} \frac{2 + 1}{3 + 64} \frac{0 + 1}{1 + 64} \frac{1 + 1}{1 + 64} $
	\end{itemize}
\end{exampleblock}

\begin{block}{Interpolation (Tri-grammes comme exemple)}
	\[%
	P_{I}(w_n | w_{n-2} w_{n-1}) = 
	\lambda_3 P(w_n | w_{n-2} w_{n-1}) 
	+ \lambda_2 P(w_n | w_{n-1}) 
	+ \lambda_1 P(w_n) 
	\]
	
	Où $\sum_i \lambda_i = 1$
	
	$\lambda_3$, $\lambda_2$ et $\lambda_1$ sont estimés en utilisant un autre corpus de réglage
\end{block}

\begin{block}{Back-off de Katz (Tri-grammes comme exemple)}
	\[%
	P_{BO}(w_n | w_{n-2} w_{n-1}) = 
	\begin{cases}
	P^*(w_n | w_{n-2} w_{n-1}) & \text{si } C(w_{n-2} w_{n-1} w_n) > 0 \\
	\alpha(w_{n-2} w_{n-1}) P_{BO}(w_n | w_{n-1}) & \text{sinon}
	\end{cases}
	\]
	
	Où : 
	
	$P^*$ est la probabilité réduite (la réduction sera distribuée sur les probabilités des N-grammes de l'ordre inférieur)
	
	$\alpha$ est une fonction qui distribue la réduction selon le contexte
\end{block}

%===================================================================================
\section{Modèles neuronaux}
%===================================================================================

\subsection{Réseau de neurones à propagation avant}

\begin{itemize}
	\item On va suivre le modèle de \cite{2003-bengio-al}
	\item On choisit le nombre des n-grammes $n$ ; donc, le nombre des mots en entrée c'est $n-1$
	\item Le vecteur $m_j$ est le embedding de position $j$
	\item On choisit la taille de vecteur de représentation (embedding) $d$
	\item Les mots sont encodés sous forme One-Hot (un vecteur avec une taille $V$ du vocabulaire)
\end{itemize}

\begin{block}{Modèle neuronal à propagation avant}
	$
	P(.|h_1,\ldots, h_{n-1}) = 
	Softmax \left(
	(b + \sum\limits_{j=1}^{n-1} m_j A_j) 
	+ 
	W\ Tanh(u + \sum\limits_{j=1}^{n-1} m_j T_j)
	\right)
	$
	
	Où $b \in \mathbb{R}^{V},\, A \in \mathbb{R}^{(n-1) \times d \times V},\, u \in \mathbb{R}^{H},\, T \in \mathbb{R}^{(n-1) \times d \times H},\, W \in \mathbb{R}^{V \times H}$
\end{block}

\begin{figure}[ht]
	\centering
	\hgraphpage[0.75\textwidth]{fw-model.pdf}
\end{figure}

\subsection{Réseau de neurones récurrents}

\begin{itemize}
	\item On va suivre le modèle de \cite{2010-mokolov-al}
	\item Dans un instant $t$, on calcule l'état $s_t$ en se basant sur l'état précédent $s_{t-1}$ et le mot actuel $m_t$
	\item L'état $s_t$ est utilisé pour estimer les probabilités $y_t$ de chaque mot du vocabulaire
\end{itemize}

\begin{block}{Modèle neuronal récurrent}
	$x_t = s_{t-1} \bullet m_t$
	
	$s_t = \sigma(x_t W)$
	
	$y_t = softmax(s_t U)$
	
	Où $m_t \in \mathbb{R}^{V},\, s_t \in \mathbb{R}^{H},\, W \in \mathbb{R}^{(H+V)\times H},\, U \in \mathbb{R}^{H\times V}$
\end{block}

\begin{figure}[ht]
	\centering
	\hgraphpage[0.75\textwidth]{rnn-model.pdf}
\end{figure}


\subsection{Quelques améliorations}

\begin{itemize}
	\item Taille limitée du contexte dans les réseaux à propagation avant
	\begin{itemize}
		\item Utiliser les réseaux récurrents
	\end{itemize}
	\item Problème de disparition du gradient dans les réseaux récurrents
	\begin{itemize}
		\item Utiliser des réseaux plus avancés : \keyword{LSTM} et \keyword{GRU}
		\item Limiter la taille du contexte 
	\end{itemize}
	\item Mots hors vocabulaire
	\begin{itemize}
		\item Limiter le vocabulaire  et marquer le reste des mots par \optword{\textlangle UNK\textrangle}
	\end{itemize}
\end{itemize}

%===================================================================================
\section{Évaluation}
%===================================================================================

	
	\begin{itemize}
		\item \optword{Évaluation extrinsèque}
		\begin{itemize}
			\item Évaluer le modèle par rapport à une autre tâche : son effet
			\item Exemple, \expword{La qualité de traduction automatique en utilisant ce modèle} 
			\item Évaluation très couteuse
		\end{itemize}
		\item \optword{Évaluation intrinsèque}
		\begin{itemize}
			\item Évaluer le modèle par rapport à sa représentation du langage
			\item Exemple, \expword{Comparer deux modèles en se basant sur leurs capacités de représenter un dataset de test} 
			\item Ne garantit pas une bonne performance du modèle pour une tâche donnée
		\end{itemize}
	\end{itemize}

\begin{itemize}
	\item Mesurer la qualité de prédiction d'un modèle sur un corpus de test
	\item Utiliser la probabilité estimée sur un corpus de test de taille $N$
	\item Le modèle avec une perplexité minimale est le meilleur
	\item Il faut inclure la fin d'une phrase et le début de la suivante dans l'entraînement du modèle (puisque la perplexité traite tout le corpus comme une seule chaîne)
\end{itemize}

\begin{block}{Perplexité}
	\begin{center}
		$PP(w) = \sqrt[N]{\frac{1}{P(w_1 w_2 \ldots w_N)}}$
		
		$PP(w) = \sqrt[N]{\prod\limits_{i=1}^{N}\frac{1}{P(w_i | w_1 \ldots w_{i-1})}}$
	\end{center}
\end{block}






\begin{discussion}



\end{discussion}

%=====================================================================
\ifx\wholebook\relax\else
% \cleardoublepage
% \bibliographystyle{../use/ESIbib}
% \bibliography{../bib/RATstat}
	\end{document}
\fi
%=====================================================================
